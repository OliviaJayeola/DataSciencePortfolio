abstract,full_text
 Existing Transformers for monocular 3D human shape and pose estimation typically have a quadratic computation and memory complexity with respect to the feature length which hinders the exploitation of fine grained information in high resolution features that is beneficial for accurate reconstruction In this work we propose an SMPL based Transform erframework SMPLer to address this issue SMPLer incorporates two key ingredients a decoupled attention operation and an SMPL based target representation which allow effective utilization of high resolution features in the Transformer In addition based on these two designs we also introduce several novel modules including a multi scale attention and a joint aware attention to further boost the reconstruction performance Extensive experiments demonstrate the effectiveness of SMPLer against existing 3D human shape and pose estimation methods both quantitatively and qualitatively Notably the proposed algorithm achieves an MPJPE of 45 2 mm on the Human3 6M dataset improving upon Mesh Graphormer by more than 10 with fewer than one third of the parameters Code and pretrained models are available at https github com xuxy09 SMPLer Index Terms 3D human shape and pose Transformer attention multi scale SMPL joint aware 1 I ,IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 1 SMPLer Taming Transformers for Monocular 3D Human Shape and Pose Estimation Xiangyu Xu Lijuan Liu Shuicheng Y an AbstractNTRODUCTION MONOCULAR 3D human shape and pose estimation is a fundamental task in computer vision which aims to recover the unclothed human body shape as well as its 3D pose from a single input image 3 4 5 6 7 It has been widely used in many applications including visual track ing 8 9 virtual augmented reality 10 11 12 13 14 15 16 17 18 motion generation 19 20 image manipulation 21 22 and neural radiance field 23 24 25 Different from multi view 3D reconstruction where the solution is well restricted by geometrical constraints 26 this task is particularly challenging due to the inherent depth ambiguity of a single 2D image and usually requires strong prior knowledge learned from large amounts of data to generate plausible results Thus the state of the art algorithms 1 2 use Transformers 27 for this task due to their powerful capabilities of grasping knowledge and learning representations from data 28 29 Existing Transformers for monocular 3D human shape and pose estimation 1 2 generally follow the ViT style 29 to design the network As shown in Figure 1 a the target embeddings are first concatenated with the input features and then processed by a full attention layer that models all pairwise dependencies including target target target feature feature target and feature feature While this design has achieved impressive results it leads to quadratic computation and memory complexity with respect to the length of the image feature i e O lF lT 2 where lFand lTare the numbers of feature and target tokens respectively or simply the lengths of the image feature Fand the target embedding T Such complexity is prohibitive for a large lF hindering the existing methods from employing high X Xu is with Xi an Jiaotong University Xi an China E mail xuxi angyu2014 gmail com L Liu is with Sea AI Lab Singapore E mail liulj sea com S Yan is with Skywork AI Singapore E mail shuicheng yan gmail com resolution image features in Transformers which possess abundant fine grained features 30 31 32 that are bene ficial for accurate 3D human shape and pose recovery In this work we propose two strategies to improve the Transformer framework to better exploit higher resolution image features for high quality 3D body shape and pose reconstruction One of them is attention decoupling We notice that different from the original ViT 29 where the image features are learned by attention operations the 3D hu man Transformers 1 2 usually rely on Convolutional Neural Networks CNNs to extract these features and the attention operations are mainly used to aggregate the image features to improve the target embeddings Thus it is less important to model the feature feature and feature target correlations in Figure 1 a as they do not have direct effects on the target We therefore propose to decouple the full attention operation into a target feature attention and a target target attention which are cascaded together to avoid the quadratic complexity As shown in Figure 1 b the decoupled attention only has a computation and memory complexity of O lFlT l2 T which is linear with respect to the feature length lF The other strategy we propose for improving the previ ous 3D human Transformer is SMPL based target representa tion Existing Transformers for 3D human shape and pose estimation mostly adopt a vertex based target representa tion 1 2 where the embedding length lTis often quite large equaling the number of vertices on a body mesh Even with the proposed attention decoupling strategy a large lT can still bring a considerable cost of computation and mem ory which hinders the usage of high resolution features To address this issue we introduce a new target representation based on the parametric body model SMPL 33 with which we only need to learn a small number of target embeddings that account for the human body shape as well as 3D body part rotations As illustrated in Figure 1 c the SMPL basedarXiv 2404 15276v1 cs CV 23 Apr 2024 IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 2 Target ùëáTarget ùëáFeature ùêπFeature ùêπ Target ùëáFeature ùêπTarget ùëá Target Feature ùêπTarget a Full Vertex b Decoupled Vertex c Decoupled SMPL Fig 1 Two key designs of the proposed Transformer The sub caption A B denotes the attention form A and the target representation B respectively The vertical and horizontal lines around the rectangles represent query and key in the attention operation Red indicates source image features blue indicates target output representation and the colors within the rectangles represent the interactions between them a Existing Transformers for 3D human reconstruction 1 2 typically adopt a ViT style full attention operation and a vertex based target representation hindering the utilization of high resolution image features In contrast we propose a decoupled attention b and an SMPL based target representation c which effectively address the above problem and improve reconstruction performance lT lT and lFare the lengths of the vertex based embedding SMPL based embedding and image features respectively The area of each rectangle denotes the computation and memory complexity of the attention operation Please refer to Section 3 1 for mathematical explanations representation Tfurther lessens the computation and mem ory burden compared to the vertex based representation with lT lT Combining the above two strategies leads to a much more concise attention learning process which not only allows the utilization of high resolution features in the Transformer but also motivates us to explore more new designs for better 3D human shape and pose estimation In particular enabled by the lifted efficiency of our model we introduce a multi scale attention operation that effectively exploits the multi scale information in a simple and unified framework Further as the proposed target representation explicitly describes 3D relative rotations between body parts which are mostly local we further propose a joint aware at tention module that emphasizes local regions around body joints to better infer the articulation status of the 3D human To summarize we make the following contributions By introducing the attention decoupling and SMPL based target representation we propose a new Trans former framework SMPLer that can exploit a large number of feature tokens for accurate and efficient 3D human shape and pose estimation Based on the two key designs above we further de velop multi scale attention and joint aware attention modules which significantly improve the reconstruc tion results Extensive experiments demonstrate that SMPLer performs favorably against the baseline methods with better efficiency In particular compared to the state of the art method 1 the proposed algorithm lowers the MPJPE error by more than 10 on Hu man3 6M 34 with fewer than one third of its pa rameters showing the effectiveness of the proposed algorithm 2 R ELATED WORK We first review the previous methods for 3D human pose and shape estimation and then discuss recent advances in vision Transformers 2 1 3D Human Shape and Pose Estimation Recent years have witnessed significant progress in the field of monocular 3D human shape and pose estimation 1 2 3 5 6 7 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 Due to the intrinsic ambiguity of 2D to 3D mapping this problem is highly ill posed and requires strong prior knowledge learned from large datasets to regularize the solutions Indeed the progress of 3D human shape and pose estimation largely relies on the development of powerful data driven models As a pioneer work SMPLify 3 learns a Gaussian Mixture model from CMU marker data 77 to encourage plausible 3D poses which however needs to conduct in ference in a time consuming optimization process To ad dress this low efficiency issue more recent methods use deep learning models to directly regress the 3D human mesh in an end to end fashion which have shown powerful capabilities in absorbing prior knowledge and discovering informative patterns from a large amount of data Typically GraphCMR 7 trains Graph Neural Networks GNNs for 3D human shape and pose estimation which directly learns the vertex location on human mesh and is prone to outliers and large errors under challenging poses and cluttered back grounds In contrast some other methods such as SPIN 6 and RSC Net 72 circumvent this issue by training deep CNNs to estimate the underlying SMPL parameters which have demonstrated robust performance for in the wild data inspiring numerous new applications 8 10 11 19 78 Nevertheless recent research in the machine learning community 29 79 reveals shortcomings of CNNs that they have difficulties in modeling long range dependencies which limits their capabilities for higher quality represen tation learning To overcome the above issue METRO 2 and Mesh Graphormer 1 introduce Transformers to this task significantly improving the reconstruction accuracy However these Transformer based methods generally fol low the architecture of ViT 29 without considering the characteristics of 3D human shape and pose and adopt IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 3 a straightforward vertex representation which hinders the network from exploiting high resolution features for better performance In contrast we propose a decoupled attention formulation that is more suitable for accurate 3D human reconstruction with a large number of feature tokens To better realize the concept of attention decoupling we de velop a new Transformer framework that can effectively exploit multi scale high and low and multi scope global and local information for better results Moreover we show that the parametric SMPL model can be combined with Transformers by introducing an SMPL based target representation which naturally addresses issues of vertex regression 2 2 Transformers With its remarkable capability of representation learning the Transformer has become a dominant architecture in natural language processing 27 28 Recently it has been introduced in computer vision and the applications include object detection 80 image classification 29 image restora tion 81 video processing 82 and generative models 83 84 Most of these works employ an attention operation that has quadratic complexity regarding the feature length and thus is not suitable for our goal of exploiting higher resolution features for more accurate 3D human shape and pose estimation In this work we demonstrate the effectiveness of a decoupled attention mechanism that has linear complexity with respect to the feature length We also introduce a compact target representation based on a parametric human model Furthermore we provide new insights in design ing multi scale and joint aware attention operations for 3D human reconstruction and the proposed network achieves better performance than the state of the art Transformers with improved efficiency 3 M ETHODOLOGY In this work we propose a new SMPL based Transformer framework SMPLer for 3D human shape and pose esti mation which can exploit a large number of image feature tokens based on an efficient decoupled attention and a compact target representation An overview is shown in Figure 2 3 1 Efficient Attention Formulation The attention operation 27 is the central component of Transformers which can be formulated as h Q K V fsoft QWq KW k d V Wv 1 where fsoftis the softmax function along the row dimension Q RlQ d and K V RlK dare the input of this oper ation representing query key and value respectively lQis the length of the query Q and lKis the length of KandV d corresponds to the channel dimension Wq Wk Wv Rd d represent learnable linear projection weights We use a multi head attention 27 in our implementation where each head follows the formulation in Eq 1 and different heads employ different linear projection weights Similar to priorwork 27 we also use layer normalization 85 and MLP in the attention operation which are omitted in Eq 1 for brevity Essentially Eq 1 models dependencies between pairs of tokens from QandKwith dot product The output h Q K V RlQ dcan be seen as a new query embed ding enhanced by aggregating information from V and the aggregation weights are decided by the dependencies between QandK Noticeably when query key and value are the same Eq 1 is called self attention which we denote ashself Q h Q Q Q When only key and value are the same while query is different the operation becomes cross attention denoted as hcross Q K h Q K K 3 1 1 Full Attention Existing Transformers for 3D human reconstruction 1 2 basically follow a ViT style structure 29 which adopts the full attention formulation as shown in Figure 1 a Mathe matically the full attention can be written as hself T F 2 where the self attention hself Q is used and the Qis a concatenation of the target embedding Tand the image feature Falong the token dimension denoted by T F R lT lF d The target embedding represents the variable of interest which corresponds to the class token in ViT and the 3D human representation in this work see more explanations in Section 3 2 As the image feature Fin Eq 2 is involved in both query and key1 the full attention leads to a quadratic computation and memory cost with respect to the length of the image feature lF i e O lF lT 2 Consequently previous works 1 2 only use low resolution features in the Transformer where lFis small In particular suppose different resolution features from a CNN backbone are denoted as F F1 FS see Figure 2 where Sis the number of scales2 Mesh Graphormer 1 only uses FS in the attention operation and straightforwardly including higher resolution features in Eq 2 e g F1 would be com putationally prohibitive 3 1 2 Decoupled Attention We notice that the attention operation serves different roles in ViT 29 and the Transformers for 3D human shape and pose estimation 1 2 the original ViT relies solely on attention to learn expressive image features while the 3D human Transformer uses a deep CNN for feature extraction Figure 2 and the attention is mainly used to aggregate the image features for improving the target embeddings Therefore modeling feature feature dependencies is less important here as it does not have a direct effect on the target implying that it is possible to avoid the quadratic complexity by pruning the full attention Motivated by this observation we propose a decoupled attention that bypasses the feature feature computations as shown in Figure 1 b It is composed of a target feature 1 This can be seen by replacing Q K V in Eq 1 with T F 2 The resolution decreases from scale 1toS  IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 4 CNN Features Target Transformer Input Output Reconstructed mesh Fig 2 Overview of the proposed framework Given a monocular input image we first use a CNN backbone 31 to extract image features F which are fed into the Transformer to reconstruct the 3D human body The main ingredients of this framework are 1 an efficient decoupled attention module in the Transformer Section 3 1 and 2 a compact target representation Tbased on parametric human model Section 3 2 More detailed descriptions of the Transformer architecture are provided in Figure 3 Transformer Block Global Pooling MLPTransformer Block Transformer Unit Transformer Unit LinearA Linear Features Linear Features a Hierarchical T ransformer b The th T ransformer BlockPE PEFusionFusion LinearRefined output Current output MatMul Add Reconstructed mesh PE Positional encoding A Element wise addResidual Fusion Reg Reg 2D joint regression Fig 3 Hierarchical architecture of our Transformer a shows an overview of the hierarchical architecture which corresponds to the Transformer in Figure 2 With the image features F we progressively refine the initial estimation P0withBTransformer Blocks Eq 13 In b each Transformer Block consists of UTransformer Units and each Unit is formulated as hfinalin Eq 12 The module J Reg represents 2D joint regression from the 3D estimation results corresponding to Eq 4 6 cross attention and a target target self attention which can be written as hself hcross T F 3 Notably Eq 3 has a computation and memory cost of O lFlT l2 T which is linear with respect to the feature length lF 3 2 Compact Target Representation While the attention decoupling strategy effectively relaxes the computation burden a large lTmay still hinder the utilization of high resolution features in Eq 3 Existing 3D human Transformers 1 2 usually recover the 3D body mesh by regressing the vertex locations Y RN 3 which leads to a redundant target representation T RN d where thei th row of Tis the embedding of the i th vertex The length of this target embedding is often quite large N 6890 by default for the SMPL mesh resulting inheavy computation and memory cost in attention operations even after mesh downsampling To address this issue we devise a more compact target representation based on a parametric human body model SMPL 3 2 1 Parametric Human Model SMPL 33 is a flexible and expressive human body model that has been widely used for 3D human shape and pose modeling It is parameterized by a set of pose parameters Œ∏ RH 3and a compact shape vector Œ≤ R1 10 The body pose Œ∏is defined by a skeleton rig with H 24 joints including the body root The i th row of Œ∏ denoted byŒ∏i is the rotation of the i th body part in the axis angle form whose skew symmetric matrix lies in the Lie algebra space so 3 86 The body shape is represented by a low dimensional space learned with principal component analysis 87 from a training set of 3D human scans and Œ≤ is the coefficients of the principal basis vectors  IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 5 With Œ∏andŒ≤ we can obtain the 3D body mesh Y RN 3 fSMPL Œ∏ Œ≤ 4 where fSMPL is the SMPL function 33 that gives the vertices Yon a pre defined triangle mesh The 3D body joints Jcan be obtained from the vertices via linear mapping using a pretrained matrix M RH N J RH 3 MY 5 With the 3D human joints we can further obtain the 2D joints using weak perspective projection Denoting the camera parameters as C R3that represents the scaling factor and the 2D principal point translation in the projec tion process the 2D joints can be obtained by J RH 2 Œ† C J 6 where Œ†Cis the weak perspective projection function 26 3 2 2 SMPL Based Target Representation Inspired by the compactness and expressiveness of SMPL we replace the original vertex based representation with an SMPL based representation Since the variables of interest are Œ∏i H i 1 Œ≤ C as introduced in Eq 4 and 6 we design the new target representation as T R H 2 d where the first Hrows of Tcorrespond to the Hbody part rotations of Œ∏ and the remaining two rows describe the body shape Œ≤and camera parameter C This new target representation conveys several advan tages First the length of this representation is much shorter than the vertex based one H 2 N thereby facilitating the efficient design of our model as shown in Figure 1 c Second our target representation is able to restrict the solution to the SMPL body space which naturally ensures smooth meshes whereas the vertex based representation is prone to outliers and may lead to spiky body surfaces as shown in Figure 8 Third the SMPL based representation explicitly models the 3D rotation of body parts and thus is more readily usable in many applications e g driving a virtual avatar In contrast the vertices cannot be directly used and have to be converted into rotations first often in an iterative optimization manner which is sub optimal in terms of both efficiency and accuracy see more details in Section 4 2 3 3 Multi Scale Attention The above strategies i e attention decoupling and SMPL based target representation enable us to explore different resolution features in the Transformer framework which inspires a multi scale attention design for high quality 3D human shape and pose estimation 3 3 1 Combining Multi Scale Features Our insight is that different resolution features are comple mentary to each other and should be collaboratively used for 3D human shape and pose estimation A straightforward way to combine those features is to take each scale as a subset of tokens and concatenate all the tokens into a single feature embedding Then the concatenated feature can be Features Scale 1 Scale S Average Target Scale 2 Fig 4 Jointly exploiting multi scale features in the attention operation see Eq 8 for more explanations A AA Features Positional encoding Encoded featuresA Element wise add Average pooling stride 2 Learnable parametersA Fig 5 Pooling based multi scale positional encoding We learn the positional encoding only for the highest resolution feature and the en codings for other scales are generated by average pooling such that similar spatial locations across different scales have similar positional embeddings used as the Kof the cross attention hcross Q K in Eq 3 The resulting multi scale attention can be written as3 hms T F hcross T F1 F2 FS 7 However this strategy uses the same projection weights for different scale features and thereby models target feature dependencies in the same subspace 27 which ignores the characteristics of different scales and is less flexible for fully exploiting the abundant multi scale information To address this issue we introduce an improved multi scale strategy that can be written as hms T F 1 SSX i 1hcross T Fi 8 where we employ different projection weights for each scale and the output is an average of all scales as illustrated in Figure 4 Whereas conceptually simple Eq 8 effectively incorporates multi scale image features into the attention computation which differs sharply from existing works 1 2 that only rely on single scale low resolution features 3 3 2 Multi Scale Feature Positional Encoding As the attention operation itself in Eq 1 is position agnostic positional encoding is often used in Transformers to inject the position information of the input Similar to ViT 29 we use the learnable positional encoding for the target and feature which generally takes the form of x œï where œï is a set of learnable parameters representing the position information of a token x In particular the positional encoding of image features can be written as œïi RlFi d i 1 2 S which is directly added to feature Fi Straightforwardly we can learn 3 We focus on improving the cross attention part of the decoupled attention Eq 3 The self attention part is kept unchanged and omitted in the following sections for brevity  IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 6 Fig 6 Illustration of the joint aware attention that aggregates local features around human joints See more details in Sec 3 4 positional encoding œïifor all scales which not only results in excessive parameters but also ignores the relationship between the locations across different scales e g similar spatial locations at different scales should have similar positional embeddings To address this issue we adjust our strategy to only learn the positional embedding for the highest scale i e œï1 and the embeddings for the other scales are produced by aggregating œï1 œïi f 2i 1 pool œï1 9 where f 2i 1 poolis the average pooling with a stride and window size of 2i 1 In real implementation we iteratively apply a stride 2 pooling layer to œï1 see Figure 5 œïi f 2 pool œïi 1 i 2 S which is equivalent to Eq 9 but requires slightly fewer computations 3 4 Joint Aware Attention In addition to the multi scale approach the SMPL based target representation Talso motivates the design of a joint aware attention Recall that the first Hrows of Tdescribe the relative rotations of the Hbody parts Section 3 2 2 As shown in Figure 6 the local articulation status around human joints strongly implies the relative rotation between neighboring body parts Thus it can be beneficial to ade quately focus on the local image features around joints in the attention operation to improve the first Hrows of Tfor better estimation of human pose To this end we devise a joint aware attention operation which modifies the cross attention hcross Q K by restrict ingKto a local neighborhood of the human joints This operation can be written as hja Ti F fsoft TiWq FN Ji 1 Wk d Œ∑ FN Ji 1 Wv 10 where Tiis the i th row of T i 1 H N Ji denotes an image patch around the i th joint Jiwith size r ras shown in Figure 6 and FN Ji 1 Rr2 drepresents the local features sampled at N Ji from F1 Eq 10 has a similar form to Eq 1 where TiandFN Ji 1 serve as the Qand K respectively It is noted that we only use the highest resolution feature F1for the local attention here as N Ji can cover a large area on smaller feature maps such that the Joint aware attention Multi scale attention Features AverageSelf attention Joints Fig 7 Combining the joint aware and multi scale attention Note that only the first Hrows of Tare averaged in the Average operation see Eq 11 for more details joint aware attention on lower resolution features becomes similar to the global attention in Eq 8 Similar to the Swin Transformer 79 we incorporate a relative positional encod ingŒ∑ R1 r2in the softmax function which is bilinearly sampled from a learnable tensor Œ∑ R r 1 r 1 according to the distance between Jiand pixels in N Ji The local attention module has a computation and memory cost of O Hr2 which is almost negligible compared to the normal attention that attends to the image features globally Eventually we combine the multi scale attention Eq 8 and the joint aware attention Eq 10 by simply taking the average as shown in Figure 7 Denoting the combined attention as hco T F R H 2 d the i th row of the output can be written as hco Ti F 1 2 hja Ti F hms Ti F i H hms Ti F i H 11 Note that hja Ti F is only defined for the Hbody parts i e i 1 H and thereby we directly use the multi scale attention for the body shape Œ≤and camera Cwithout averaging which correspond to i H 1 H 2in Eq 11 Following the attention decoupling in Eq 3 the final formulation of our attention module can be written as hfinal T F hself hco T F 12 which is briefly illustrated in Figure 7 3 5 Hierarchical Architecture As shown in Eq 10 an important issue of our current design is that the joint aware attention relies on the 2D joints J which is supposed to be an output of our algorithm see Eq 6 In other words we need Jto reconstruct the 3D human and meanwhile need the 3D human to regress J which essentially leads to a chicken and egg problem To circumvent this problem we propose a hierarchical archi tecture Figure 3 to iteratively refine the 2D joint estimation and 3D reconstruction results We denote the output of the b th stage in Figure 3 a asPb Rb Œ∏1 Rb Œ∏H Œ≤b Cb where Rb Œ∏i SO 3 is the rotation matrix of the i th body part corresponding to Œ∏i thei th row of Œ∏ Then the refinement process can be written as Tb fb TB Tb 1 Pb 1 F Pb ffusion Tb Pb 1 b 1 2 B 13  IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 7 TABLE 1 Quantitative comparison against the state of the art methods on Human3 6M and 3DPW datasets MPVE represents mean per vertex error Numbers in bold indicate the best in each column Method Parameters M Human3 6M 3DPW MPJPE PA MPJPE MPVE MPJPE PA MPJPE HMR 4 88 0 56 8 81 3 GraphCMR 7 40 7 50 1 70 2 SPIN 6 41 1 116 4 59 2 RSC Net 72 28 0 67 2 45 7 112 1 96 6 59 1 FrankMocap 74 94 3 60 0 VIBE 5 42 7 65 6 41 4 99 1 82 9 51 9 Pose2Mesh 46 72 8 64 9 47 0 89 2 58 9 I2LMeshNet 48 135 7 55 7 41 1 93 2 57 7 PARE 62 88 6 74 5 46 5 METRO 2 231 8 54 0 36 7 88 2 77 1 47 9 Mesh Graphormer 1 215 7 51 2 34 5 87 7 74 7 45 6 SMPLer 35 6 47 0 32 8 84 7 75 7 45 2 SMPLer L 70 2 45 2 32 4 82 0 73 7 43 4 where fb TBis the b th Transformer Block which improves the target embedding Tb 1with image features Fand the current 3D estimation Pb 1 ffusion is a fusion layer that generates the refined estimation Pbbased on the improved target embedding Tb As illustrated on the rightmost of Figure 3 in the fusion layer we first use a linear layer to mapTbinto a set of residuals Pb Rb Œ∏1 Rb Œ∏H Œ≤b Cb and then add the residuals to the current estimation Pb 1 Similar to 88 we apply the Gram Schmidt orthogonaliza tion to the rotation residuals such that Rb Œ∏i SO 3 We use matrix multiplication MatMul in Figure 3 instead of addition to adjust the rotation parameters As shown in Figure 3 a the hierarchical architecture is composed of BTransformer Blocks and each Transformer Block is composed of UTransformer Units in Figure 3 b The Transformer Unit represents the aforementioned decou pled attention hfinalillustrated in Figure 7 To bootstrap this refinement process we apply a global pooling layer and an MLP to the CNN features to obtain an initial coarse estimation P0similar to HMR 4 For the initial target embedding T0 a straightforward choice is to use a learned feature embedding as in DETR 80 Nevertheless we em pirically find this choice makes the training less stable Instead we employ a heuristic strategy by combining the globally pooled image feature and linearly transformed P0 i e T0 fglobal FS flinear P0 where fglobal is the global average pooling function 3 6 Loss Function For training the proposed Transformer we adopt the loss functions of Mesh Graphormer 1 which restrict the recon structed human in terms of vertices and body joints ‚Ñìbasic wY Y ÀÜY 1 wJ J ÀÜJ 2 2 wJ J ÀÜJ 2 2 where Y J Jare the predicted vertices 3D joints and 2D joints that are computed with Eq 4 6 using the final output of our Transformer i e PBin Figure 3 a ÀÜY ÀÜJ ÀÜJindicate the corresponding ground truth wY wJ wJare hyper parameters for balancing different terms Following 1 we useL1loss for vertices and MSE loss for human joints In addition we include a rotation regularization term ‚Ñìrotation wR 1 HHX i 1 RŒ∏i ÀÜRŒ∏i 1 14 where we encourage the predicted rotation RŒ∏ito be close to the ground truth rotation matrix ÀÜRŒ∏i wRis the weight of the loss Eventually our training loss is a combination of‚Ñìbasic and‚Ñìrotation We do not restrict the body shape Œ≤as we empirically find no benefits from it Note that ‚Ñìrotation can only be used in our Transformer which uses an SMPL based target representation it is in contrast to existing 3D human reconstruction Transformers that do not directly consider rotations 3 7 Discussions Decoupled attention While a similar idea to our atten tion decoupling has been touched upon in 90 for video classification our work marks the pioneering exploration of it in monocular 3D human shape and pose estimation addressing the intrincs limitations of the full attention for mation of the existing works While we present the idea in a simplified manner in Figure 1 for clarity the innovation of our decoupled attention goes beyond a basic conceptual introduction Unlike the approach in 90 that relies solely on single scale low resolution features we propose a multi scale decoupled attention framework Section 3 3 This unique design allows the model to exploit both coarse and granular visual information substantially improving the performance of 3D human shape and pose estimation No tably this multi scale approach is enabled by the enhanced efficiency of our attention decoupling strategy which allows incorporating higher resolution features without prohibitive computational costs In addition we emphasize that effectively combining multi scale features is not a straightforward endeavor which requires dedicated algorithmic design and meticu lous engineering efforts Particularly instead of directly con catenating all scale features our approach assigns different projection weights for each scale to model target feature de pendency in a scale aware manner Section 3 3 1 Moreover we devise a pooling based multi scale positional encoding IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 8 Input METRO Mesh Graphormer SMPLer SMPLer L Fig 8 Visual comparisons against the SOTA methods The top two rows are from the Human3 6M dataset 34 and the bottom two rows are from the 3DPW dataset 89 to better represent spatial information across varying scales as detailed in Section 3 3 2 SMPL based target representation While SMPL has been used as output in prior methods 4 41 60 71 our work marks the first time that the SMPL based target representa tion is used in a Transformer framework We emphasize the contribution of our SMPL based tar get representation is rooted in its distinct advantages First it considerably reduces the computational cost as illustrated in Figure 1 Second it ensures consistent smooth SMPL mesh avoiding the spiky outliers on human surfaces pro duced by vertex based representations Figure 8 Third it explicitly models body part rotations facilitating its applica tions in driving virtual avatars Figure 11 Furthermore the SPML based target representation allows the development of the joint aware attention and motivates the hierarchical architecture of our Transformer both of which are novel designs absent from previous works 4 41 60 71 and the concurrent work 91 significantly improving the reconstruction results 4 E XPERIMENTS 4 1 Implementation Details For the network structure we set the number of Transformer Blocks as B 3 and the number of Transformer Units in each block as U 2 by default We use four attention headsfor the Transformer and set the feature sampling range of the joint aware attention as r 8 It corresponds to a 32 32region in the input image which is adequately large to encompass the vicinity of the joints We set the number of feature scales S 4 We use HRNet 31 as the CNN backbone We present two variants of the proposed Trans former a base model SMPLer and a larger one SMPLer L The two models use the same architecture and the only difference is the channel dimension of the backbone where we increase the channels by half for SMPLer L We use a batch size of 200 and train the model for 160 epochs The loss weights in ‚Ñìbasic and‚Ñìrotation are empirically set as wY 100 wJ 1000 wJ 100 wR 50 Dataset Similar to 1 2 we extensively train our model by combining several human datasets including Hu man3 6M 34 MuCo 3DHP 92 UP 3D 37 COCO 93 MPII 94 Similar to 1 2 we use the pseudo 3D mesh training data from 46 48 for Human3 6M We follow the general setting where subjects S1 S5 S6 S7 and S8 are used for training and subjects S9 and S11 for testing We present all results using the P2 protocol 4 6 We fine tune the models with the 3DPW 89 training data for 3DPW experiments Metrics We mainly use mean per joint position error MPJPE and Procrustes aligned mean per joint position IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 9 Input Output Alternate views Input Output Alternate views Fig 9 Qualitative results of SMPLer For each example the first column shows the input image the second column shows the output in camera view and the remaining columns show the predicted mesh from alternate viewpoints error PA MPJPE for evaluation MPJPE is defined as 1 HHX i 1 Ji ÀÜJi 2 15 where Jiis the i th row of J i e the coordinate of the i th human joint and ÀÜJrepresents the ground truth Eq 15 directly measures the joint to joint error which can be in fluenced by global factors including scaling global rotation and translation PA MPJPE is defined as min s R t1 HHX i 1 sJiR t ÀÜJi 2 16 s t s R R SO 3 t R1 3 where s R t represent scaling global rotation and transla tion respectively Instead of directly computing the joint to joint error Eq 16 first aligns the prediction Jto the ground truth ÀÜJwith a scaled rigid transformation a closed form solution for the alignment is given by Procrustes analysis 95 Thus PA MPJPE is able to exclude the global factors and focus on the intrinsic human body shape and pose emphasizing the measurement of relative position between adjacent body parts 4 2 Comparison with the State of the Arts Quantitative evaluation We compare against the state of the art 3D human shape and pose estimation methods including CNN based 4 5 6 48 62 72 74 GNN based 7 46 and Transformer based 1 2 As shown in Table 1 the proposed Transformer performs fa vorably against the baseline approaches on both the Hu man3 6M and 3DPW datasets Notably compared to Mesh Graphormer 1 the proposed SMPLer and SMPLer L low ers the MPJPE error on Human3 6M by 8 2 and 11 7 GT OursProjected V ertex basedFig 10 Illustrating the manifold of the desired SMPL human meshes The proposed target representation guarantees our method always moves along this manifold while the existing Transformers may produce results off the desired space resulting in less accurate estimation and inconvenience in many applications e g controlling virtual avatars As a straightforward remedy for avatar control one can project the undesired results to the SMPL manifold with iterative optimization which however is often time consuming and prone to accumulative errors Note that the real SMPL manifold is embedded in a 20670 dimensional space 6890 3D vertices and here we simply show the concept in 3D for ease of visualization respectively while using only 16 5 and 32 5 of its pa rameters clearly demonstrating the effectiveness of our algorithm Qualitative evaluation For a more intuitive understand ing of the results we also provide visual comparisons in Figure 8 As a typical example in the first row of Figure 8 existing approaches cannot well pose the human legs as they only use low resolution features in the Transformer and thus are prone to errors under self occlusions and challenging poses In contrast the proposed SMPLer is able to better exploit the abundant image features in a multi scale coarse and fine and multi scope global and local manner which effectively improves the estimation performance in complex scenarios On the other hand since existing Transformers mostly rely on a vertex based target representation their results do not always lie on the SMPL mesh manifold as IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 10 illustrated in Figure 10 This leads to spiky outliers on the human surface as shown in the second row of Figure 8 or even distorted meshes in the fourth row of Figure 8 By contrast our method introduces an SMPL based target representation which naturally guarantees the solutions to lie on the smooth human mesh space Figure 10 and thereby achieves higher quality results as shown in Figure 8 In addition we show more qualitative results of SMPLer with alternative views in Figure 9 where the proposed net work performs robustly under diverse poses and complex backgrounds In particular the results show that our model is able to accurately recover global rotation relative to the camera coordinate frame which is also validated by the improvements over MPJPE and MPVE metrics Controlling virtual avatars An important application of 3D human shape and pose estimation is to control virtual avatars e g in Metaverse As our SMPL based target repre sentation explicitly models the 3D rotations of body parts the proposed SMPLer can be easily used to drive virtual humans An example is shown in Figure 11 In contrast the previous Transformers take vertices as the output which are not directly applicable here and have to be converted into rotations first for this task More specifically the predicted vertices need to be fitted to the SMPL model Eq 4 in an iterative optimization manner which corresponds to projecting the results onto the SMPL manifold in Figure 10 Compared to the proposed solution which is essentially one step the two step approach vertices rotations not only leads to inefficiency issues due to the time consuming fitting process but also suffers from low accuracy caused by accumulative errors To quantitatively evaluate the rotation accuracy we in troduce a new metric called mean per body part rotation error MPRE which is defined as MPRE 180 œÄHHX i 1arccos trace RŒ∏iÀÜR Œ∏i 1 2 17 where RÀÜR represents the rotation matrix between the pre dicted rotation Rand the ground truth ÀÜR this can be seen from R RÀÜR ÀÜR Essentially Eq 17 describes the rota tion angle œâ in degree between the prediction and ground truth as the three eigenvalues of a rotation matrix are 1 and e iœâ and thus trace RÀÜR 1 eiœâ e iœâ 1 2 cos œâ As the ground truth rotations of Human3 6M are not available we evaluate the rotation accuracy on 3DPW The proposed SMPLer achieves an MPRE of 9 9 significantly outperforming the 57 0 of Mesh Graphormer which demon strates the advantage of the proposed method in controlling virtual avatars The visual comparison in Figure 11 further verifies the improvement 4 3 Analysis and Discussions We conduct a comprehensive ablation study on Human3 6M to investigate the capability of our method We also provide more analysis and discussions about the model efficiency as well as the attention mechanism We use SMPLer as our default model in the following sections Decoupled attention and SMPL based representation We propose a new Transformer framework that is able to exploit Reference Virtual avatar Mesh Graphormer SMPLer Fig 11 Thanks to the SMPL based target representation the proposed method can be well applied to control virtual avatars while the existing vertex based Transformers are error prone The virtual character is from Mixamo 96 high resolution features for accurate 3D human reconstruc tion At the core of this framework are a decoupled attention module and an SMPL based target representation as intro duced in Section 3 1 and 3 2 To analyze the effectiveness of these designs we study different choices of the attention operation full vs decoupled and the target representation vertex based vs SMPL based in Table 2 As shown by Table 2 b the straightforward approach of using a full attention and a vertex based representation leads to prohibitive memory and computation cost for ex ploiting multi resolution image features F With limited computational resources we cannot train this model with a proper batch size While the decoupled attention can well reduce the model complexity Table 2 c the memory footprint is still large due to the high dimensionality of the vertex based target representation In contrast the pro posed Transformer combines the decoupled attention and SMPL based representation Table 2 d which significantly lessens the memory and computational burden and thereby achieves more effective utilization of multi scale features Note that the feature dimensionality is not a computa tional bottleneck for our network anymore as it only leads to a marginal computation overhead for employing high resolution features as shown by Table 2 d and e Effectiveness of the multi scale attention As introduced in Section 3 3 we propose an attention operation hmsfor better exploiting multi scale image information As shown in Table 3 using single scale feature for 3D human shape and pose estimation either the low resolution only scale S or the high resolution one only scale 1 leads to inferior results compared to our full model On the other hand unifying multi scale features in Transformer is not a trivial task As introduced in Sec tion 3 3 instead of simply concatenating different resolution features into a single feature vector hmsin Eq 7 we sep arately process each scale with different projection weights i e hmsin Eq 8 As shown in Table 3 the straightforward concatenation method cannot fully exploit the multi scale information due to the undesirable restriction of using the same projection weights for different scales resulting in less accurate 3D human estimation compared to the proposed approach In addition we apply multi scale feature positional en coding to supplement position information for the attention operation As shown in Table 4 the model without feature positional encoding w o FPE suffers from a major perfor mance drop especially for MPJPE Furthermore instead of directly learning the feature positional embedding for all IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 11 Input Block 1 Block 2 Block 3 Fig 12 Output of Block 1 2 and 3 of SMPLer The reconstruction result is progressively refined in the hierarchical architecture a Input b Recovered mesh c Self attention d Joint aware attention e Multi scale Scale 1 f Multi scale Scale 2 g Multi scale Scale 3 h Multi scale Scale 4 Fig 13 Visualization of the attention learned by SMPLer The query joint is the right knee the yellow point in c For the self attention in c brighter color indicates stronger interactions For d h redder color indicates larger attention response scales all scale FPE we propose a pooling based strategy Eq 9 as illustrated in Figure 5 to better handle the spatial relation across scales Compared to all scale FPE in Ta ble 4 this strategy further improves the 3D human shape and pose estimation results Effectiveness of the joint aware attention Motivated by the SMPL based target representation we propose a joint aware attention hjain Eq 10 to better exploit local features around human joints As shown in Table 5 the model without hjasuffers from a significant performance drop showing the importance of this module in inferring relative body part rotations Further we include a relative positional encoding in Eq 10 to model the spatial relation between tar get embedding and image features which slightly improves the performance as shown in Table 5 w o Œ∑ Analysis of the hierarchical architecture We apply a hier archical architecture in Figure 3 that is composed of multi ple Transformer Blocks and multiple Transformer Units to progressively refine the 3D human estimation results We visualize the refinement process in Figure 12 where the reconstruction becomes more accurate after more Blocks Moreover we investigate the effect of different settings of the hierarchical architecture As shown in Table 6 the performance gain of adding more Blocks and Units is sig nificant at the start but converges quickly after B 1and U 1 Therefore we refrain from adding even more blocks and take B 3 U 2 as the default setting for a better tradeoff between performance and computation cost Attention visualization For a more comprehensive study of the proposed attention modules we visualize the learned attention maps in Figure 13 First we show the multi scale attention hmsin Figure 13 e h where the different scales correspond to the cross attention modules in Figure 4 The attention maps for higher resolutions e g in Figure 13 e High resolution Low resolutionDif fused ConcentratedKL Divergence between Attention and Uniform DistributionFig 14 Attention distributions of different scales averaged on Hu man3 6M We use the KL divergence between the attention map and a uniform distribution to quantitatively measure the spatial diffuseness of the attention A higher divergence from the uniform distribution indicates lower diffuseness in space in other words the attention in that scale is more concentrated at fine grained features tend to be more concentrated on fine grained features of specific body parts In particular the proposed Transformer relies on the foot and head poses to help infer body part rotation around the knee By contrast for lower resolutions e g in Figure 13 h the attention gets more diffused in a wider space indicating that the multi scale attention can exploit global information including the background to decide the global rotation and scale of the body To further analyze the attention of different scales we also provide quantitative results in Figure 14 We use the KL divergence between the attention map and a uniform distri bution to measure how much the attention distribution dif fuses in space As shown in Figure 14 the higher resolution attention modules have higher divergence from the uniform distribution indicating that their attention is less diffused and more concentrated at fine grained features This verifies the complementarity of low and high resolution features as discussed in Section 3 3 In addition we provide a visualization of the learned joint aware attention in Figure 13 d which shows intrigu ing local patterns where the information around the target human joint is emphasized for better 3D human recon struction Specifically it has higher weights on the upper and lower sides of the knee which captures the kinetic status around the joint and helps infer the relative rotation between the thigh and calf Lastly we employ a self attention layer hselffor atten tion decoupling Figure 7 which models the target target dependencies in Figure 1 c We visualize this layer by showing the interactions between one query joint and all other joints where the hselffocuses on neighboring joints on the kinetic tree 33 to produce more reasonable predictions Running speed As shown in Table 7 the inference of SMPLer and SMPLer L runs at 96 0 and 73 9 frames per sec ond fps respectively which is effectively real time While exploiting high dimensional multi resolution features SM PLer has a nearly three times running speed and one fifth GFlops of the baseline Transformers 1 2 that are solely based on low dimensional features showing the efficiency of the proposed algorithm Relationship with prior works The proposed SMPLer is IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 12 TABLE 2 Comparison across different choices of the attention operation and the target representation The GPU memory and GFlops are measured for a batch size of 1 Due to the immense GPU memory requirement we are not able to train b and c with a proper batch size d corresponds to our default setting of SMPLer Attention Target Feature Memory G GFlops MPJPE PA MPJPE a Full Vertex based FS 0 49 8 1 51 9 36 0 b Full Vertex based F1 FS 7 3 26 0 c Decoupled Vertex based F1 FS 1 15 11 0 d Decoupled SMPL based F1 FS 0 44 8 7 47 0 32 8 e Decoupled SMPL based FS 0 37 7 8 48 3 34 0 TABLE 3 Effectiveness of the multi scale attention Method MPJPE PA MPJPE only scale S 48 3 34 0 only scale 1 49 5 33 7 concatenation 48 5 33 9 full model 47 0 32 8 TABLE 4 Effectiveness of the feature positional encoding FPE Method MPJPE PA MPJPE w o FPE 50 4 33 7 all scale FPE 49 0 33 6 full model 47 0 32 8 based on two fundamental designs the attention decou pling and the SMPL based target representation These de signs substantially distinguish our algorithm from existing Transformers 1 2 which are based on full attention and vertex based representation and thereby can only use low resolution features in the attention operation Moreover the proposed framework also clearly differs from the existing SMPL based approaches such as HMR 4 SPIN 6 and RSC Net 72 which are solely based on CNNs and cannot exploit the powerful learning capabilities of Transformers for 3D human shape and pose estimation As a result they suffer from a large performance gap compared to the state of the arts as evidenced in Table 1 The two basic designs naturally lead to the development of several novel modules including the multi scale attention and the joint aware attention While these modules are conceptually simple to have them work properly in our framework is by no means trivial and requires meticulous algorithmic designs such as the approach to fuse multi scale features the pooling based positional encoding the relative positional encoding in the joint aware attention and the hierarchical architecture Eventually with the above designs SMPLer achieves significant improvement over the state of the art methods 1 2 with better efficiency 5 C ONCLUSIONS We have developed a new Transformer framework for high quality 3D human shape and pose estimation from a single image At the core of this work are the decou pled attention and the SMPL based target representation that allow efficient utilization of high dimensional features TABLE 5 Effectiveness of the joint aware attention Method MPJPE PA MPJPE w ohja 51 4 34 5 w oŒ∑ 48 7 33 3 full model 47 0 32 8 TABLE 6 Effect of different number of Transformer Blocks and Units in the hierarchical architecture B 3 U 2is the default setting Method MPJPE PA MPJPE B 1 U 2 48 6 34 4 B 2 U 2 47 4 33 1 B 3 U 2 47 0 32 8 B 3 U 1 48 5 33 9 B 3 U 3 47 1 32 8 These two strategies also motivate the design of the multi scale attention and joint aware attention modules which can be potentially extended to other areas where multi scale and multi scope information is valuable such as motion estimation 97 and image restoration 98 Meanwhile the proposed algorithm can also be applied to other 3D recon struction problems such as 3D animal reconstruction 99 and 3D hand reconstruction 100 by replacing SMPL with other parametric models e g SMAL 101 and MANO 102 Nevertheless this is beyond the scope of this work and will be an interesting direction for future research Similar to existing Transformers the proposed SMPLer is still a hybrid structure that consists of CNN layers in the backbone To incorporate attention based backbones in SMPLer such as 79 103 will be another direction worth exploring in future work REFERENCES 1 K Lin L Wang and Z Liu Mesh graphormer in ICCV 2021 1 2 3 4 5 7 8 9 11 12 2 End to end human pose and mesh reconstruction with transformers in CVPR 2021 1 2 3 4 5 7 8 9 11 12 3 F Bogo A Kanazawa C Lassner P Gehler J Romero and M J Black Keep it smpl Automatic estimation of 3d human pose and shape from a single image in ECCV 2016 1 2 4 A Kanazawa M J Black D W Jacobs and J Malik End to end recovery of human shape and pose in CVPR 2018 1 7 8 9 12 5 M Kocabas N Athanasiou and M J Black Vibe Video infer ence for human body pose and shape estimation in CVPR 2020 1 2 7 9 6 N Kolotouros G Pavlakos M J Black and K Daniilidis Learn ing to reconstruct 3D human pose and shape via model fitting in the loop in ICCV 2019 1 2 7 8 9 12 IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 13 TABLE 7 Running speed of different methods measured on the same computer with an Intel Xeon E5 2620 v3 CPU and an NVIDIA Tesla M40 GPU The GFlops are calculated for a single input image with size 224 224 Method Speed fps GFLops METRO 33 5 50 5 Mesh Graphormer 34 6 45 4 SMPLer 96 0 8 7 SMPLer L 73 9 16 5 7 N Kolotouros G Pavlakos and K Daniilidis Convolutional mesh regression for single image human shape reconstruction inCVPR 2019 1 2 7 9 8 J Rajasegaran G Pavlakos A Kanazawa and J Malik Tracking people with 3d representations in NeurIPS 2021 1 2 9 Tracking people by predicting 3d appearance location and pose in CVPR 2022 1 10 J Wang Y Zhong Y Li C Zhang and Y Wei Re identification supervised texture generation in CVPR 2019 1 2 11 X Xu and C C Loy 3d human texture estimation from a single image with transformers in ICCV 2021 1 2 12 Z Zheng T Yu Y Liu and Q Dai PaMIR Parametric model conditioned implicit representation for image based human re construction IEEE Transactions on Pattern Analysis and Machine Intelligence 2021 1 13 A Mir T Alldieck and G Pons Moll Learning to transfer texture from clothing images to 3D humans in CVPR 2020 1 14 Q Ma J Yang S Tang and M J Black The power of points for modeling humans in clothing in ICCV 2021 1 15 T Alldieck M Magnor B L Bhatnagar C Theobalt and G Pons Moll Learning to reconstruct people in clothing from a single rgb camera in CVPR 2019 1 16 T Alldieck M Magnor W Xu C Theobalt and G Pons Moll Video based reconstruction of 3d people models in CVPR 2018 1 17 T Alldieck G Pons Moll C Theobalt and M Magnor Tex2shape Detailed full human body geometry from a single image in ICCV 2019 1 18 T Alldieck M Magnor W Xu C Theobalt and G Pons Moll Detailed human avatars from monocular video in 3DV 2018 1 19 R Li S Yang D A Ross and A Kanazawa Ai choreographer Music conditioned 3d dance generation with aist in ICCV 2021 1 2 20 F Hong M Zhang L Pan Z Cai L Yang and Z Liu Avatarclip Zero shot text driven generation and animation of 3d avatars ACM Transactions on Graphics SIGGRAPH vol 41 no 4 pp 1 19 2022 1 21 S Sanyal A Vorobiov T Bolkart M Loper B Mohler L S Davis J Romero and M J Black Learning realistic human reposing using cyclic self supervision with 3d shape pose and appearance consistency in ICCV 2021 1 22 A Grigorev K Iskakov A Ianina R Bashirov I Zakharkin A Vakhitov and V Lempitsky Stylepeople A generative model of fullbody human avatars in CVPR 2021 1 23 S Peng Y Zhang Y Xu Q Wang Q Shuai H Bao and X Zhou Neural body Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans in CVPR 2021 1 24 Y Kwon D Kim D Ceylan and H Fuchs Neural human performer Learning generalizable radiance fields for human performance rendering in NeurIPS 2021 1 25 M Chen J Zhang X Xu L Liu Y Cai J Feng and S Yan Geometry guided progressive nerf for generalizable and effi cient neural human rendering in ECCV 2022 1 26 R Hartley and A Zisserman Multiple view geometry in computer vision Cambridge university press 2003 1 5 27 A Vaswani N Shazeer N Parmar J Uszkoreit L Jones A N Gomez L Kaiser and I Polosukhin Attention is all you need inNeurIPS 2017 1 3 5 28 J Devlin M W Chang K Lee and K Toutanova Bert Pre training of deep bidirectional transformers for language under standing in NAACL 2019 1 3 29 A Dosovitskiy L Beyer A Kolesnikov D Weissenborn X Zhai T Unterthiner M Dehghani M Minderer G Heigold S Gelly et al An image is worth 16x16 words Transformers for image recognition at scale in ICLR 2021 1 2 3 5 30 B Bosquet M Mucientes and V M Brea Stdnet Exploiting high resolution feature maps for small object detection Engi neering Applications of Artificial Intelligence vol 91 p 103615 2020 1 31 K Sun B Xiao D Liu and J Wang Deep high resolution representation learning for human pose estimation in CVPR 2019 1 4 8 32 J Long E Shelhamer and T Darrell Fully convolutional net works for semantic segmentation in CVPR 2015 1 33 M Loper N Mahmood J Romero G Pons Moll and M J Black Smpl A skinned multi person linear model ACM Transactions on Graphics SIGGRAPH Asia vol 34 no 6 p 248 2015 1 4 5 11 34 C Ionescu D Papava V Olaru and C Sminchisescu Human3 6m Large scale datasets and predictive methods for 3d human sensing in natural environments IEEE Transactions on Pattern Analysis and Machine Intelligence vol 36 no 7 pp 1325 1339 2013 2 8 35 H Y Tung H W Tung E Yumer and K Fragkiadaki Self supervised learning of motion capture in NeurIPS 2017 2 36 G Pavlakos L Zhu X Zhou and K Daniilidis Learning to estimate 3d human pose and shape from a single color image inCVPR 2018 2 37 C Lassner J Romero M Kiefel F Bogo M J Black and P V Gehler Unite the people Closing the loop between 3d and 2d human representations in CVPR 2017 2 8 38 M Omran C Lassner G Pons Moll P Gehler and B Schiele Neural body fitting Unifying deep learning and model based human pose and shape estimation in 3DV 2018 2 39 R A Guler and I Kokkinos Holopose Holistic 3d human reconstruction in the wild in CVPR 2019 2 40 Y Xu S C Zhu and T Tung Denserac Joint 3d pose and shape estimation by dense render and compare in ICCV 2019 2 41 W Jiang N Kolotouros G Pavlakos X Zhou and K Daniilidis Coherent reconstruction of multiple humans from a single im age in CVPR 2020 2 8 42 T Zhang B Huang and Y Wang Object occluded human shape and pose estimation from a single color image in CVPR 2020 2 43 W Zeng W Ouyang P Luo W Liu and X Wang 3d human mesh regression with dense correspondence in CVPR 2020 2 44 A Zanfir E G Bazavan H Xu W T Freeman R Sukthankar and C Sminchisescu Weakly supervised 3d human pose and shape reconstruction with normalizing flows in ECCV 2020 2 45 J Song X Chen and O Hilliges Human body model fitting by learned gradient descent in ECCV 2020 2 46 H Choi G Moon and K M Lee Pose2mesh Graph convolu tional network for 3d human pose and mesh recovery from a 2d human pose in ECCV 2020 2 7 8 9 47 G Georgakis R Li S Karanam T Chen J Ko Àáseck a and Z Wu Hierarchical kinematic human mesh recovery in ECCV 2020 2 48 G Moon and K M Lee I2l meshnet Image to lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image in ECCV 2020 2 7 8 9 49 H Zhang J Cao G Lu W Ouyang and Z Sun Learning 3d human shape and pose from dense body parts IEEE Transactions on Pattern Analysis and Machine Intelligence 2020 2 50 G Moon H Choi and K M Lee Accurate 3d hand pose estimation for whole body 3d human mesh estimation in CVPR Workshops 2022 2 51 J Li C Xu Z Chen S Bian L Yang and C Lu Hybrik A hybrid analytical neural inverse kinematics solution for 3d human pose and shape estimation in CVPR 2021 2 52 A Sengupta I Budvytis and R Cipolla Probabilistic 3d human shape and pose estimation from multiple unconstrained images in the wild in CVPR 2021 2 53 I Akhter and M J Black Pose conditioned joint angle limits for 3d human pose reconstruction in CVPR 2015 2 54 A Zanfir E G Bazavan M Zanfir W T Freeman R Sukthankar and C Sminchisescu Neural descent for visual 3d human pose and shape in CVPR 2021 2 IEEE TRANSACTIONS ON PATTERN ANAL YSIS AND MACHINE INTELLIGENCE 14 55 H Joo N Neverova and A Vedaldi Exemplar fine tuning for 3d human model fitting towards in the wild 3d human pose estimation in 3DV 2021 2 56 N Kolotouros G Pavlakos D Jayaraman and K Daniilidis Probabilistic modeling for human mesh recovery in ICCV 2021 2 57 S K Dwivedi N Athanasiou M Kocabas and M J Black Learning to regress bodies from images using differentiable semantic rendering in ICCV 2021 2 58 Y Sun Q Bao W Liu Y Fu M J Black and T Mei Monocular one stage regression of multiple 3d people in ICCV 2021 2 59 M Zanfir A Zanfir E G Bazavan W T Freeman R Sukthankar and C Sminchisescu Thundr Transformer based 3d human reconstruction with markers in ICCV 2021 2 60 H Zhang Y Tian X Zhou W Ouyang Y Liu L Wang and Z Sun Pymaf 3d human pose and shape regression with pyramidal mesh alignment feedback loop in ICCV 2021 2 8 61 M Kocabas C H P Huang J Tesch L M uller O Hilliges and M J Black Spec Seeing people in the wild with an estimated camera in ICCV 2021 2 62 M Kocabas C H P Huang O Hilliges and M J Black Pare Part attention regressor for 3d human body estimation in ICCV 2021 2 7 9 63 A Kanazawa J Y Zhang P Felsen and J Malik Learning 3d human dynamics from video in CVPR 2019 2 64 A Arnab C Doersch and A Zisserman Exploiting temporal context for 3d human pose estimation in the wild in CVPR 2019 2 65 Y Sun Y Ye W Liu W Gao Y Fu and T Mei Human mesh recovery from monocular images via a skeleton disentangled representation in ICCV 2019 2 66 C Doersch and A Zisserman Sim2real transfer learning for 3d human pose estimation motion to the rescue in NeurIPS 2019 2 67 Z Luo S A Golestaneh and K M Kitani 3d human motion estimation via motion compression and refinement in ACCV 2020 2 68 H Choi G Moon J Y Chang and K M Lee Beyond static features for temporally consistent 3d human pose and shape from a video in CVPR 2021 2 69 G H Lee and S W Lee Uncertainty aware human mesh recov ery from video by learning part based 3d dynamics in ICCV 2021 2 70 Z Wan Z Li M Tian J Liu S Yi and H Li Encoder decoder with multi level attention for 3d human shape and pose estimation in ICCV 2021 2 71 X Xu H Chen F Moreno Noguer L A Jeni and F De la Torre 3D human shape and pose from a single low resolution image with self supervised learning in ECCV 2020 2 8 72 3D human pose shape and texture from low resolution images and videos IEEE Transactions on Pattern Analysis and Machine Intelligence 2021 2 7 9 12 73 F Moreno Noguer 3d human pose estimation from a single image via distance matrix regression in CVPR 2017 2 74 Y Rong T Shiratori and H Joo Frankmocap A monocular 3d whole body pose estimation system via regression and integra tion in ICCV Workshops 2021 2 7 9 75 A Davydov A Remizova V Constantin S Honari M Salz mann and P Fua Adversarial parametric pose prior in CVPR 2022 2 76 G Tiwari D Antic J E Lenssen N Sarafianos T Tung and G Pons Moll Pose ndf Modeling human pose manifolds with neural distance fields in ECCV 2022 2 77 CMU graphics lab motion capture database http mocap cs cmu edu 2010 2 78 L Liu X Xu Z Lin J Liang and S Yan Towards garment sewing pattern reconstruction from a single image ACM Trans actions on Graphics SIGGRAPH Asia 2023 2 79 Z Liu Y Lin Y Cao H Hu Y Wei Z Zhang S Lin and B Guo Swin transformer Hierarchical vision transformer using shifted windows in ICCV 2021 2 6 12 80 N Carion F Massa G Synnaeve N Usunier A Kirillov and S Zagoruyko End to end object detection with transformers inECCV 2020 3 7 81 H Chen Y Wang T Guo C Xu Y Deng Z Liu S Ma C Xu C Xu and W Gao Pre trained image processing transformer inCVPR 2021 3 82 Z Shi X Xu X Liu J Chen and M H Yang Video frame interpolation transformer in CVPR 2022 3 83 H Zhang I Goodfellow D Metaxas and A Odena Self attention generative adversarial networks in ICML 2019 3 84 Y Jiang S Chang and Z Wang Transgan Two pure transform ers can make one strong gan and that can scale up in NeurIPS 2021 3 85 J L Ba J R Kiros and G E Hinton Layer normalization arXiv 1607 06450 2016 3 86 J S Dai Euler rodrigues formula variations quaternion conju gation and intrinsic connections Mechanism and Machine Theory vol 92 pp 144 152 2015 4 87 I T Jolliffe and J Cadima Principal component analysis a review and recent developments Philosophical Transactions of the Royal Society A Mathematical Physical and Engineering Sciences vol 374 no 2065 p 20150202 2016 4 88 Y Zhou C Barnes J Lu J Yang and H Li On the continuity of rotation representations in neural networks in CVPR 2019 7 89 T von Marcard R Henschel M J Black B Rosenhahn and G Pons Moll Recovering accurate 3d human pose in the wild using imus and a moving camera in ECCV 2018 8 90 C Zhang A Gupta and A Zisserman Temporal query net works for fine grained video understanding in CVPR 2021 7 91 C Zheng X Liu G J Qi and C Chen Potter Pooling attention transformer for efficient human mesh recovery in CVPR 2023 8 92 D Mehta O Sotnychenko F Mueller W Xu S Sridhar G Pons Moll and C Theobalt Single shot multi person 3d pose estima tion from monocular rgb in 3DV 2018 8 93 T Y Lin M Maire S Belongie J Hays P Perona D Ramanan P Doll ar and C L Zitnick Microsoft coco Common objects in context in ECCV 2014 8 94 M Andriluka L Pishchulin P Gehler and B Schiele 2d human pose estimation New benchmark and state of the art analysis inCVPR 2014 8 95 P H Sch onemann A generalized solution of the orthogonal procrustes problem Psychometrika vol 31 no 1 pp 1 10 1966 9 96 Mixamo https www mixamo com 10 97 D Sun X Yang M Y Liu and J Kautz Pwc net Cnns for op tical flow using pyramid warping and cost volume in CVPR 2018 12 98 S Nah T H Kim and K M Lee Deep multi scale convolu tional neural network for dynamic scene deblurring in CVPR 2017 12 99 B Biggs O Boyne J Charles A Fitzgibbon and R Cipolla Who left the dogs out 3D animal reconstruction with expec tation maximization in the loop in ECCV 2020 12 100 G Pavlakos V Choutas N Ghorbani T Bolkart A A Osman D Tzionas and M J Black Expressive body capture 3d hands face and body from a single image in CVPR 2019 12 101 S Zuffi A Kanazawa D Jacobs and M J Black 3D menagerie Modeling the 3D shape and pose of animals in CVPR 2017 12 102 J Romero D Tzionas and M J Black Embodied hands Model ing and capturing hands and bodies together ACM Transactions on Graphics SIGGRAPH Asia vol 36 no 6 2017 12 103 Y Yuan R Fu L Huang W Lin C Zhang X Chen and J Wang Hrformer High resolution transformer for dense prediction in NeurIPS 2021 12 
 Recent advancements in machine learning have led to novel imaging systems and algorithms that address ill posed problems Assess ingtheirtrustworthinessandunderstandinghowtodeploythemsafelyat test time remains an important and open problem We propose a method that leverages conformal prediction to retrieve upper lower bounds and statistical inliers outliers of reconstructions based on the prediction in tervals of downstream metrics We apply our method to sparse view CT for downstream radiotherapy planning and show 1 that metric guided bounds have valid coverage for downstream metrics while conventional pixel wise bounds do not and 2 anatomical differences of upper lower bounds between metric guided and pixel wise methods Our work paves the way for more meaningful reconstruction bounds Code available at https github com matthewyccheung conformal metric Keywords Uncertainty Conformal Prediction Inverse problems 1 ,Metric guided Image Reconstruction Bounds via Conformal Prediction Matt Y Cheung1 Tucker J Netherton2 Laurence E Court2 Ashok Veeraraghavan1 and Guha Balakrishnan1 1Department of Electrical and Computer Engineering Rice University mattyc guha vashok rice edu 2Department of Radiation Physics The University of Texas MD Anderson Cancer Center TJNetherton LECourt mdanderson org AbstractIntroduction Recent advancements in machine learning have led to novel imaging systems and algorithms that address ill posed problems Traditionally image reconstruction evaluation relies on common image quality metrics such as PSNR SSIM FID and Dice scores of segmentations on reconstructed images While these metrics provide a heuristic to gauge the overall model uncertainty in reconstruction dur ingevaluation theydonotprovideuncertaintiesandguaranteesattesttime and do not link reconstruction quality to uncertainties in downstream applications Conformal prediction CP provides distribution free valid and calibrated prediction intervals at test time 32 10 29 2 The idea is to use residuals from a calibration dataset to infer uncertainty in future test datasets 32 10 29 2 For regression tasks this uncertainty is given as a prediction interval 32 10 29 2 TheapplicationofCPtoimagereconstructionhasbeenrelativelylimited Thisis a difficult problem because quantiles in higher dimensional data are M quantiles meaning they have infinite solutions and only have unique solutions when a di rection is specified 6 7 How do we pick such a direction The conventionalarXiv 2404 15274v1 cs LG 23 Apr 2024 2 Cheung et al pixel wise method is to pick the direction where all pixels are independent 12 16 9 The upper and lower bounds of the estimated image can be calibrated based on pixel intensity 3 14 While these pixel wise prediction intervals are easy to interpret they do not consider spatial correlations and may lead to large interval lengths 4 The upper and lower bounds can also be calibrated in the di rection of principal components 4 While using principal components considers spatial correlations it does not capture meaningful and practical uncertainty for downstream processes and is prohibitively costly to compute for large images Furthermore both methods provide upper and lower bounds not sampled from the learned manifold yielding implausible images A reasonable answer is to calibrate the upper and lower bounds in the direction of semantic features 27 However this method requires training a generative model with disentangled latent spaces We argue that bounds should be computed in the direction of downstream metrics for more reliable downstream performance We propose Metric guided Image Reconstruction Bounds that leverages CP to form valid prediction in tervals of reconstructions in the direction of downstream metrics and retrieve reconstructions 1 closest to the upper and lower bounds 2 contained in the bounds statistical inliers and 3 outside the bounds statistical outliers Our method takes spatial correlations into account and produces plausible recon structions from the learned manifold We show that our method provides valid coverage for downstream tasks while the conventional pixel wise method does not and the upper lower bounds between methods are anatomically different We demonstrate our method on sparse view computed tomography sv CT and downstream radiotherapy planning Reconstruction is highly accurate for CT machines and uses sophisticated detectors and algorithms to obtain sub millimeter spatial resolution CT downtime significantly impacts the availability of radiotherapy planning in low resource settings 11 A low cost device with cone beam geometry could be manufactured and used to increase access to ra diotherapyplanningandothertherapeuticusecases Individualizedradiotherapy plans are made from CT scans and specify localized doses to a target treatment volume i e breast prostate We use downstream clinical application metrics from radiotherapy planning to retrieve reconstructions 2 Method We consider a 3 D reconstruction setting for a downstream application with a chosen downstream metric 3The measurement and reconstruction algorithms are assumed to be probabilistic We follow the split conformal prediction pro cedure 26 32 2 29 by using nppatients for the calibration dataset and 1 test patient np 1with unknown ground truth volume and metric as the test dataset For each patient iin the calibration dataset we reconstruct a set of volumes ÀÜVi ÀÜVi j nr j 1of size nr Each patient s reconstructed volumes are used to attain 3While we concentrate on imaging our method can be applied to any multidimen sional setting  Metric guided Image Reconstruction Bounds via Conformal Prediction 3 Fig 1 Overview of our approach Assume probabilistic measurement and reconstruc tion processes nppatients for calibration and 1 patient for testing For test patient np 1withunknowngroundtruthreconstructionandmetric 1 acquiremeasurements 2 attain a set of reconstructions 3 extract downstream metrics 4 adjust upper and lower bounds of metric based on a calibration procedure and 5 retrieve reconstruc tions with metrics closest to the calibrated upper and lower bounds contained within bounds statistical inliers and outside of bounds statistical outliers a set of estimated metrics ÀÜYi ÀÜYi j nr j 1 Each patient s ground truth volume is usedtoattainagroundtruthmetric Yi Forthetestpatient wereconstructaset of volumes ÀÜVnp 1 ÀÜVnp 1 j nr j 1and estimate metrics ÀÜYnp 1 ÀÜYnp 1 j nr j 1 4 Assuming ÀÜYi Yi fori 1 n p 1are exchangeable we leverage Conformal ized Quantile Regression CQR 26 to find the prediction interval C ÀÜYnp 1 satisfying the conformal coverage guarantee 33 P Ynp 1 C ÀÜYnp 1 1 Œ± 1 where Œ±is a user specified miscoverage rate We attain C ÀÜYnp 1 by adjusting the upper and lower bounds of ÀÜYnp 1with an offset qthat is computed from the calibration dataset to satisfy 1 C ÀÜYnp 1 QŒ± 2 ÀÜYnp 1 q Q1 Œ± 2 ÀÜYnp 1 q 2 where QŒ± is the function that estimates the Œ±th quantile 5Finally we retrieve the volumes 1 closest to the upper and lower bounds of the prediction intervals ÀÜVnp 1 LB ÀÜVnp 1 UB based on the L1norm 2 contained within the prediction inter vals inliers and 3 outside the prediction intervals outliers We provide an overview in Fig 1 and pseudo code in Algorithm 1 Similar to prior work 4 we use sample quantiles instead of regression quan tiles Our method can be interpreted as a discrete version of CQR that finds 4We do not have ground truth volume Vnp 1and metric Ynp 1at test time 5While we use symmetric adjustments in this work asymmetric bounds can also be used See 26 for more details  4 Cheung et al Algorithm 1 Metric guided Image Reconstruction Bounds Perform calibration to get upper and lower bound adjustment using CQR fori 1 npdo score i max QŒ± 2 ÀÜYi Yi Yi Q1 Œ± 2 ÀÜYi end for q Q np 1 1 Œ± np scores Compute prediction interval for patient in test dataset C ÀÜYnp 1 LB ÀÜYnp 1 UB ÀÜYnp 1 QŒ± 2 ÀÜYnp 1 q Q1 Œ± 2 ÀÜYnp 1 q Retrieve upper and lower bound reconstructions ÀÜVnp 1 LB arg minÀÜVnp 1 j ÀÜYnp 1 j LB ÀÜYnp 1 ÀÜVnp 1 UB arg minÀÜVnp 1 j ÀÜYnp 1 j UB ÀÜYnp 1 Retrieve inliers and outlier reconstructions forj 1 nrdo ifÀÜYnp 1 j LB ÀÜYnp 1 UB ÀÜYnp 1 then Add ÀÜYnp 1 jto inliers else Add ÀÜYnp 1 jto outliers end if end for marginalpredictionintervalsfordownstreammetricsgivenapatient Ourmethod is different to prior work in pixel wise 3 that produce prediction intervals per reconstruction Instead we provide prediction sets directly from a set of patient reconstructions where each patient has different reconstruction volume sizes We compare our method with conventional pixel wise bounds 3 Experiments Radiotherapy Planning We use the Radiation Planning Assistant RPA FDA 510 k cleared a web based tool for radiotherapy planning 1 8 18 RPA automates treatment planning on CT images and provides dose and plan reports for clinics in low and middle income countries 1 8 18 Dose statistics specify what percentage of organ volume receives a particular dose Structural statistics are from organ segmentation and specify metrics such as organ volume and Hausdorff distance 15 We use a dose prescription of 25 fractions in 50Gy 2 00Gy fraction for supraclavicular SCV and tangential field irradiation The RPA automatically segments organs at risk and then applies a single isocenter technique with matched tangential and SCV fields to treat the chest wall and SCV region Dataset We use a de identified CT dataset of 20 patients retrospectively treated with radiotherapy at The University of Texas MD Anderson Cancer Center All CT images were of patients who had received surgical mastectomy Metric guided Image Reconstruction Bounds via Conformal Prediction 5 Table 1 Metric guided bounds yield valid coverages while conventional pixel wise bounds do not Using 20 patients and target coverage of 90 we perform leave one out cross validation and compute average coverage using metric guided and pixel wise methods for maximum dose to the heart Heart D0 volume of ipsilateral lung that received 20Gy Right Lung V20 volume of ipsilateral lung Right Lung Volume and dose that 35 volume of the ipsilateral lung receives Right Lung D35 Method Heart D0Right Lung Volume Right Lung V20Right Lung D35 Metric guided 90 90 90 90 Pixel wise 75 0 50 50 to the right side of the body and radiotherapy to the post mastectomy chest wall and or axillary lymph nodes This research was conducted using an approved institutional review board protocol Each ground truth CT is of size 512 512 Number of slices For each patient we generate 10 digitally reconstructed ra diographs DRR from the ground truth CT scan using the TIGRE toolbox 5 The DRRs simulate image acquisition from a cone beam geometry We simulate physical randomness beam angle variability and sensor noise by generating DRRs with 3 noise and 50 random projections between 0 and 360 degrees The number of projections was increased from 2 to 50 until organ boundaries were perceptually discernible in the reconstruction by the RPA Because this work aims to showcase the feasibility of CP for image reconstruction we as sume that such a low cost sv CT device will be created in future work that gives acceptable reconstruction image quality We use a self supervised model Neural Attenuation Fields NAF for reconstruction 35 Each reconstruction is uncropped and contains the full scan We use the default parameter setting in NAF 35 and introduce computational randomness through random initializa tions of NAF 30 19 Ultimately we construct a dataset of 20 patients with 10 reconstructions each To construct the conventional pixel wise upper and lower bounds we take each pixel s upper and lower quantiles 3 1 Validation We validate our method by computing coverage Table 1 which is defined as the fraction of patients with ground truth metrics within the bounds For metric guided bounds we use leave one out cross validation on 20 patients and report the average coverage for metrics volume of ipsilateral lung that received 20Gy Right Lung V20 maxmimum dose to the heart Heart D0 and dose that 35 volume of the ipsilateral lung receives Right Lung D35 For conventional pixel wise bounds we compute the coverage of all patients We use the finite sample correction 1 Œ± adj np 1 1 Œ± np 26 2 for target coverage of 1 Œ± adj Our results show that metric guided bounds give valid coverages for downstream tasks while conventional pixel wise bounds do not  6 Cheung et al Fig 2 Metric guided bounds account for spatial correlations that affect downstream metrics For maximum dose to the heart D with target coverage of 90 we show contours for heart red right lung blue left lung yellow and body green overlaid on CT slices Pixel wise upper and lower bounds differ in pixel wise intensity while metric guided bounds differ in the spatial distribution of pixel intensities Pixel wise upperboundshavelargerheartvolumesthanlowerbounds whilemetric guidedbounds have similar heart volumes Retrieval error ŒµBis the difference between estimated and actual bound divided by the interval length 3 2 Upper and lower bound retrieval We retrieve metric guided and pixel wise upper and lower bounds for a target coverage of 90 for maximum dose to the heart Fig 2 To verify the retrieved images are representative of the bounds at test time we compute retrieval error defined as ŒµB ÀÜYnp 1 B Bnp 1 UBnp 1 LBnp 1 100 3 where Bdenotes the calibrated bound and can be upper bound UBor lower bound LB and ÀÜYnp 1 B arg minÀÜYnp 1 j ÀÜYnp 1 j Bnp 1 are the estimated met  Metric guided Image Reconstruction Bounds via Conformal Prediction 7 Fig 3 Metric guided and Pixel wise methods produce anatomically different upper and lower bounds We determine whether the upper and lower bound volumes from metric guided and pixel wise methods are different across methods using paired t tests For all three metrics volume of ipsilateral lung that received 20Gy Right Lung V20 maximum dose to the heart Heart D0 and dose that 35 volume of the ipsilateral lung receives Right Lung D35 we find that the differences are significant p 0 05 except for the upper bound reconstructions for Heart D0 rics closest to the calibrated bounds We find that pixel wise upper and lower bounds are perceptually similar and only differ in their intensity while metric guided bounds differ in the spatial distribution of pixel intensities This indicates that metric guided bounds take spatial correlations into account As a conse quence the pixel wise differences for metric guided bounds can be both positive and negative This indicates that single pixels do not carry sufficient information to explain the variations in dose We find that the segmentations of the heart are also perceptually different Pixel wise upper bounds tend to have larger volumes than lower bounds while this rule does not hold for metric guided bounds Furthermore this result suggests that pixel wise and metric guided methods may disagree on inliers and outliers Metric guided inlier reconstructions may have pixels considered as pixel wise outliers and metric guided outlier recon structions may have pixels considered as pixel wise inliers 3 3 Anatomical Differences Using organ segmentations from RPA we determine whether there is a statis tically significant difference in upper bound volume across methods and lower bound volume across methods using paired t tests We use the dose metrics in Table 1 We find statistically significant differences p 0 05 for upper and lower bounds across methods except for the upper bound reconstructions for Heart D0 p 7 2e 2 Fig 3 This suggests that the upper and lower bounds across methods are anatomically different  8 Cheung et al 4 Conclusion Weproposeamethodthatleveragesconformalpredictiontoretrieveupper lower bounds and statistical inliers outliers of reconstructions based on the prediction intervals of downstream metrics We apply our method to sv CT for downstream radiotherapy planning and show 1 metric guided bounds have valid coverage for downstream metrics unlike conventional pixel wise bounds and 2 statistically significant anatomical differences of upper lower bounds between metric guided and pixel wise methods 5 Discussion There are several areas for further investigation Factors affecting retrieval error Retrieval error may depend on number of samples the diversity of samples and the accuracy of the model The predic tion intervals and retrieval errors may also be very large if the model is highly biased Asymmetric bounds could help identify this bias 26 Furthermore we assume the downstream processes to be deterministic This is an appropriate assumption for the maximum dose to the heart but may not be for other param eters Opportunities lie in decoupling uncertainty from physical reconstruction algorithm and downstream algorithm randomness 13 Evaluating Safety and Equity We can perform patient specific safety evaluations and identify inequities across patients For a dose prescription of 50Gy 2Gy fraction a safe maximum dose to the heart is 5Gy and the vol ume of the ipsilateral lung getting 20Gy is 35 If the upper bound of the prediction interval is greater than these thresholds it may indicate that the re construction is unsuitable for planning Patients or measurement conditions with high uncertainty can be used for downstream interpretation 25 22 and action 34 20 They may correspond to specific clinical scenarios such as inadequately filled lungs or large distance from heart to chest wall Opportunities lie in ap plying causal methods 24 28 23 to identify factors causes of high uncertainty Testtimeevaluationmetricsforreconstruction Whileweshowinliers and outliers for one metric our method can be extended to multiple metrics 21 31 where we find reconstructions with all estimated metrics in the prediction intervals containing the ground truth metrics with confidence Opportunities lie in assessing reconstructions with multiple critical metrics Other applications Opportunities lie in extending our method to other medicalimagingapplications 22 17 andcriticalscenarios Additionally although not demonstrated in our work our method does not necessitate reconstruction samples to be of identical size or dimensions as calibration is conducted based on a scalar downstream metric Acknowledgments The authors would also like to acknowledge support from a fel lowship from the Gulf Coast Consortia on the NLM Training Program in Biomedical Informatics and Data Science T15LM007093 The authors would also like to thank the RPA team Joy Zhang Raphael Douglas for their support Tucker Netherton would like to acknowledge the support of the NIH LRP award  Metric guided Image Reconstruction Bounds via Conformal Prediction 9 References 1 Aggarwal A Burger H Cardenas C Chung C Douglas R du Toit M Jhin gran A Mumme R Muya S Naidoo K et al Radiation planning assistant a web based tool to support high quality radiotherapy in clinics with limited re sources Journal of Visualized Experiments Jove 200 2023 2 Angelopoulos A N Bates S A gentle introduction to conformal prediction and distribution free uncertainty quantification arXiv preprint arXiv 2107 07511 2021 3 Angelopoulos A N Kohli A P Bates S Jordan M Malik J Alshaabi T Upadhyayula S Romano Y Image to image regression with distribution free uncertainty quantification and applications in imaging In International Confer ence on Machine Learning pp 717 730 PMLR 2022 4 Belhasin O Romano Y Freedman D Rivlin E Elad M Principal uncer tainty quantification with spatial correlation for image restoration problems arXiv preprint arXiv 2305 10124 2023 5 Biguri A Dosanjh M Hancock S Soleimani M Tigre a matlab gpu toolbox for cbct image reconstruction Biomedical Physics Engineering Express 2 5 055010 2016 6 Breckling J Chambers R M quantiles Biometrika 75 4 761 771 1988 7 Breckling J Kokic P L√ºbke O A note on multivariate m quantiles Statistics probability letters 55 1 39 44 2001 8 Court L Aggarwal A Burger H Cardenas C Chung C Douglas R duToit M Jaffray D Jhingran A Mejia M et al Addressing the global expertise gap in radiation oncology the radiation planning assistant JCO Global Oncology 9 e2200431 2023 9 Edupuganti V Mardani M Vasanawala S Pauly J Uncertaintyquantification in deep mri reconstruction IEEE Transactions on Medical Imaging 40 1 239 250 2020 10 Fontana M Zeni G Vantini S Conformal prediction a unified review of theory and new challenges Bernoulli 29 1 1 23 2023 11 Frija G Bla≈æiƒá I Frush D P Hierath M Kawooya M Donoso Bach L Brkljaƒçiƒá B How to improve access to medical imaging in low and middle income countries EClinicalMedicine 38 2021 12 Gillmann C Saur D Wischgoll T Scheuermann G Uncertainty aware visu alization in medical imaging a survey In Computer Graphics Forum vol 40 pp 665 689 Wiley Online Library 2021 13 Gong Y Yao Y Lin X Divakaran A Gervasio M Confidence calibration for systems with cascaded predictive modules arXiv preprint arXiv 2309 12510 2023 14 Horwitz E Hoshen Y Conffusion Confidence intervals for diffusion models arXiv preprint arXiv 2211 09795 2022 15 Huttenlocher D P Klanderman G A Rucklidge W J Comparing images us ing the hausdorff distance IEEE Transactions on pattern analysis and machine intelligence 15 9 850 863 1993 16 Jalal A Arvinte M Daras G Price E Dimakis A G Tamir J Robust com pressed sensing mri with deep generative priors Advances in Neural Information Processing Systems 34 14938 14954 2021 17 Kazerouni A Aghdam E K Heidari M Azad R Fayyaz M Hacihaliloglu I Merhof D Diffusion models in medical imaging A comprehensive survey Medical Image Analysis p 102846 2023  10 Cheung et al 18 Kisling K McCarroll R Zhang L Yang J Simonds H Du Toit M Trauer nicht C Burger H Parkes J Mejia M et al Radiation planning assistant a streamlined fully automated radiotherapy treatment planning system JoVE Journal of Visualized Experiments 134 e57411 2018 19 Lakshminarayanan B Pritzel A Blundell C Simple and scalable predictive uncertainty estimation using deep ensembles Advances in neural information pro cessing systems 30 2017 20 Lekeufack J Angelopoulos A A Bajcsy A Jordan M I Malik J Confor mal decision theory Safe autonomous decisions from imperfect predictions arXiv preprint arXiv 2310 05921 2023 21 Lin M Ambsdorf J Sejer E P F Bashir Z Wong C K Pegios P Raheli A Svendsen M B S Nielsen M Tolsgaard M G et al Learning semantic im age quality for fetal ultrasound from noisy ranking annotation arXiv preprint arXiv 2402 08294 2024 22 Lu C Lemay A Chang K H√∂bel K Kalpathy Cramer J Fair conformal predictors for applications in medical imaging In Proceedings of the AAAI Con ference on Artificial Intelligence vol 36 pp 12008 12016 2022 23 Lundberg S M Lee S I A unified approach to interpreting model predictions Advances in neural information processing systems 30 2017 24 Ribeiro M T Singh S Guestrin C Model agnostic interpretability of machine learning arXiv preprint arXiv 1606 05386 2016 25 Romano Y Barber R F Sabatti C Cand√®s E With malice toward none Assessing uncertainty via equalized coverage Harvard Data Science Review 2 2 4 2020 26 Romano Y Patterson E Candes E Conformalized quantile regression Ad vances in neural information processing systems 32 2019 27 Sankaranarayanan S Angelopoulos A Bates S Romano Y Isola P Semantic uncertainty intervals for disentangled latent spaces Advances in Neural Informa tion Processing Systems 35 6250 6263 2022 28 Schwab P Karlen W Cxplain Causal explanations for model interpretation under uncertainty Advances in neural information processing systems 32 2019 29 Shafer G Vovk V Atutorialonconformalprediction JournalofMachineLearn ing Research 9 3 2008 30 S√ºnderhauf N Abou Chakra J Miller D Density aware nerf ensembles Quan tifying predictive uncertainty in neural radiance fields In 2023 IEEE International Conference on Robotics and Automation ICRA pp 9370 9376 IEEE 2023 31 Taksoee Vester C A Mikolaj K Bashir Z Christensen A N Petersen O B Sundberg K Feragen A Svendsen M B Nielsen M Tolsgaard M G Ai sup ported fetal echocardiography with quality assessment Scientific Reports 14 1 5809 2024 32 Vovk V Gammerman A Shafer G Algorithmic learning in a random world vol 29 Springer 2005 33 Vovk V Gammerman A Saunders C Machine learning applications of algo rithmic randomness 1999 34 Ye C T Han J Liu K Angelopoulos A Griffith L Monakhova K You S Learned uncertainty driven adaptive acquisition for photon efficient multiphoton microscopy arXiv preprint arXiv 2310 16102 2023 35 Zha R Zhang Y Li H Naf neural attenuation fields for sparse view cbct reconstruction In International Conference on Medical Image Computing and Computer Assisted Intervention pp 442 452 Springer 2022  
 Distributed decision problems features a group of agents that can only communicate over a peer to peer network without a central memory In applications such as network control and data ranking each agent is only affected by a small portion of the decision vector this sparsity is typically ignored in distributed algorithms while it could be leveraged to improve efficiency and scalability To address this issue our recent paper 1 introduces Estimation Network Design END a graph theoretical language for the analysis and design of distributed it erations END algorithms can be tuned to exploit the sparsity of specific problem instances reducing communication overhead and minimizing redundancy yet without requiring case by case convergence analysis In this paper we showcase the flexility of END in the context of distributed optimization In particular we study the sparsity aware version of many established methods including ADMM AugDGM and Push Sum DGD Simulations on an estimation problem in sensor networks demonstrate that END algorithms can boost convergence speed and greatly reduce the communication and memory cost I I ,Estimation Network Design framework for efficient distributed optimization Mattia Bianchi and Sergio Grammatico AbstractNTRODUCTION Modern big data optimization problems in network control and machine learning are typically partially separable 2 i e the cost function is the sum of Nindividual costs each depending only on a small portion of the overall optimization variable This structure is widely exploited in parallel algo rithms 2 3 where multiple distinct processors share the computation cost but having access to a common memory Yet this is not the case for distributed scenarios where the processors or agents are constrained to communicate uniquely with some neighbors over a communication net work In fact most distributed optimization methods entails the agents reaching consensus on the entire optimization vector 4 5 even when each agent eventually only uses a few components of the solution as in resource allocation and network control 6 This may result in prohibitive memory and communication requirements and in poor scalability if the decision vector grows with the network size Efficient solutions are known for partitioned problems where the local cost functions or constraints only directly couple each agent to its neighbors 6 10 Notably how ever this requires that the communication graph matches the interference graph describing the coupling among the agents in the cost or constraints which is usually not the case for wireless and ad hoc networks Remarkably general partially separable problems were ad dressed via distributed dual methods by Mota et al 11 and M Bianchi is with the Automatic Control Laboratory IfA ETH Z urich Switzerland mbianch ethz ch S Grammatico is with the Delft Center for Systems and Control DCSC TU Delft The Netherlands s grammatico tudelft nl This work is supported by NWO under research project OMEGA 613 001 702 by the ERC under research project COSMOS 802348 and by ETH Z urich funds later by Alghunaim et al 12 13 in this approach each component of the optimization variable is estimated by a suitably chosen cluster of agents only Unfortunately the dual formulation is only effective over undirected communication networks Other works 14 15 rely on the concept of locality which result in improved efficiency but at the cost of accuracy further any structure beyond distance on the communication graph is ignored To deal with these challenges in our recent work 1 we introduced END a graph theoretic language to describe how the estimates of any variable of interest e g optimization vector dual multipliers cost gradient are allocated and com bined among the agents in a generic distributed algorithm END allows assigning the estimate of each component of the variable of interest to a subset of the agents according to the sparsity structure of a given problem without resort ing to a case by case convergence analysis Leveraging the problem sparsity is especially convenient in repeated or time varying problems e g distributed estimation and model predictive control MPC 11 where the one time cost of efficiently assigning the estimates yields improved iterative online performance Although 1 focuses on distributed Nash equilibrium problems the END framework is flexible and applicable to virtually any distributed decision problem Contributions In this paper we apply and tailor the END framework 1 to distributed optimization problems thus unifying and generalizing several recent approaches For the case of dual algorithms our setup retrieves the formulation in 11 13 see Proposition 1 Here we present a novel sparsity aware ADMM but one can obtain the END version of virtually any dual algorithm Section III A Further com pared to 11 13 our framework has broader applicability i it can be used for primal methods To illustrate we present the END version of the ABC method 5 en compassing many established algorithms As an example we derive a gradient tracking iteration where each agent only needs to estimate a fraction of the cost gradient the END counterpart of AugDGM 16 Section III B ii it works on directed graphs Specifically we present the sparsity aware version of the Push Sum DGD algorithm 17 that is guaranteed to converge over time varying and column stochastic graphs Section III C It will be clear from our arguments that thanks to our power ful stacked notation the analysis of END algorithms presents little complication compared to their sparsity unaware coun terparts Nonetheless the impact in terms of flexibility and performance is remarkable We illustrate numerically this point on an estimation problem for wireless sensor network arXiv 2404 15273v1 math OC 23 Apr 2024 where we observe that END can decrease the communication cost by more than 90 Section IV A Background and notation 1 Basic notation Nis the set of natural numbers includ ing0 R R 0 is the set of nonnegative real numbers 0q Rq 1q Rq is a vector with all elements equal to 0 1 Iq Rq qis an identity matrix the subscripts may be omitted when there is no ambiguity eidenotes a vector of appropriate dimension with i th element equal to 1 and all other elements equal to 0 For a matrix A Rp q A i jis the element on rowiand column j null A x Rq Ax 0n and range A v Rp v Ax x Rq IfA A Rq q Œªmin A Œª1 A Œªq A Œªmax A denote its eigenvalues diag A1 AN is the block diagonal matrix with A1 ANon its diagonal Given Nvectors x1 x N col x1 x N x 1 x N denotes the Kronecker product Given a positive definite matrix Rq q Q 0 x y Q x Qyos the Q weighted inner product Qis the associated norm we omit the subscripts if Q I Given a function œà Rq R R its set valued subdifferential operator is denoted by œà Rq Rq x7 v Rq œà z œà x v z x z Rq 2 Graph theory A directed graph G V E consists of a nonempty set of vertices or nodes V 1 2 V and a set of edges E V V We denote by N v u u v E andN v u v u E the set of in neighbors or simply neighbors and out neighbors of vertex v V respectively A path from v1 V tovN V of length Tis a sequence of vertices v1 v2 v T such that vt vt 1 E for all t 1 T 1 Gis strongly connected if there exist a path from utov for all u v V in case Gis undirected namely if u v E whenever v u E we simply say that Gis connected The restriction of the graph Gto a set of vertices VA V is defined as G VA VA E VA VA We also write G VA VB E to highlight that Gis bipartite namely V VA VBand E VA VB We may associate to Ga weight matrix W RV Vcompliant with G namely wu v W u v 0if v u E wu v 0 otherwise Gis unweighted if wu v 1 if v u E Given two graphs GA VA EA andGB VB EB we write GA GBifGAis a subgraph ofGB i e if VA VBandEA EB we define GASGB VA VB EA EB A time varying graph Gk k N Gk V Ek isQ strongly connected ifS k 1 Q 1 t kQGtis strongly connected for all k N II END FOR DISTRIBUTED OPTIMIZATION We first recall the general END framework 1 that de scribes the information structure in any distributed algorithm It is characterized by 1 a set of agents I 1 2 N 2 a given directed communication graph GC I EC over which the agents can exchange information agent i can receive data from agent jif and only if j NC i 3 a variable of interest y Rny partitioned as y col yp p P P 1 P yp Rnyp 4 a given interference graph GI P I EI EI P I specifying which components of yare indispensable for each agent p NI i means that agent ineeds an esti mate of ypto perform some essential local computation 1 5 a bipartite estimate graph GE P I EE EE P I Since agents might be unable to access y each agent estimates some of the components yp s as specified by the estimate graph agent ikeeps an estimate yi p Rnyp ofypif and only if p NE i 6 Pdirected design graphs GD p p P GD p NE p ED p GD pdescribes how the agents that estimate ypexchange their estimates agent ican receive yj pfrom agent jif and only if i ND p j Specifically in this paper we apply the END framework to the distributed optimization problems min y RnyP i Ifi y 1 where fi Rny Ris a private cost function of agent i We choose the variable of interest in the END framework to coincide with the optimization variable2 Hence we partition the optimization variable as y col yp p P The common approach 17 18 to solve 1 over a communication network GCis to assign to each agent i I a copy yi col yi p p P Rnyof the whole decision variable and to let the agents exchange their estimates with every neighbor over GC in END notation we write this as3 EE P I GD p GC p P 2 Yet in several applications like network control and data ranking 2 each cost function fidepends only on some of the components of y as specified by an interference graph GI fidepends on ypif and only if p NI i P With some abuse of notation we highlight this fact by writing fi y fi yp p N I i 3 Clearly the standard choice 2 for the graphs GEand GD p p Pdoes not take advantage of the structure in 3 In fact agent ionly needs yp p N I i to evaluate the gradient of its local cost fi storing a copy of the whole vector y could be unnecessary and inefficient especially if GIis sparse and nyis large A Problem dependent design unified analysis From an algorithm design perspective the graphs GC andGIshall be considered fixed as part of the problem formulation In contrast the graphs GEand GD p p Pare design choices although with some constraints Standing Assumption 1 GEandGD pare chosen such that GI GEandGD p GCfor all p P 1For ease of notation assume NI p for all p P 2Except for Section III D where we instead select the dual variable as variable of interest see also 1 for different possible choices e g aggregative values in variational problems 3In the following we refer to 2 as the standard choice as it is the most widely studied scenario With Gc GD pwe also imply WC WD p  In particular GE GImeans that each agent estimates at least the components of ywhich are indispensable for local computation Moreover since the estimates are exchanged overGD pand communication can only happen over GC it must hold that GD p GC In addition we will always need some level of connectedness for each graph GD p to ensure that the agents can reach consensus on the estimates of yp as for instance in the following condition Assumption 1 For each p P GD pis undirected and connected Designing GEand GD p p Pto satisfy Standing Assump tion 1 and Assumption 1 is not difficult if GCis itself undirected and connected one trivial choice is 2 Yet one wishes to also consider efficiency specifications e g in terms of memory allocation communication or bandwidth by imposing extra soft constraints on GEand GD p p P We present a simple instance in Figure 1 and refer to 1 App A for more examples Such an optimal design is in general computationally expensive Nonetheless the performance advantages in terms of algorithm execution can be well worth the one time cost of an efficient algorithm design especially in repeated problems 11 19 where the same distributed problem is solved multiple times for different values of some parameters measurements Further while this design procedure is very problem and goal dependent it does not affect the analysis of END algo rithms By simply postulating some connectedness property as in Assumption 1 we can unify the convergence analysis of standard algorithms that use 2 with that of methods specifically devised for problems with unique sparsity B END Notation We next introduce the stacked END notation crucial in our analysis For all p P letNp NE p be the number of copies of yp Recalling that that yi pis the estimate of yp kept by agent i we define yp col yi p i NE p RNpnyp p P 4 y col yp p P Rny 5 where ny P p PNpnyp Note that ypcollects all the copies of ypkept by different agents We denote WD diag WD p Inyp p P 6 where WD pis the weight matrix of GD p Let Cp yp RNpnyp yp 1Np v v Rnyp 7 be the consensus space for yp where all the estimates of yp are equal C Q p PCpbe the overall consensus space C y col 1Np yp p P 8 For each p P for each i NE p we denote by ip P j NE p j i1 9 the position of iin the ordered set of nodes NE p For all i NE p we denote by R i p Rnyp Npnypthe matrix that selects yi pfrom yp i e yi p Ri pyp a b Fig 1 a A simple example of END design from 1 On the left the given communication and interference graphs with I 1 2 3 4 5 andP 1 2 On the right a possible choice for the design graphs and the corresponding estimate graphs b We focus on the design of GD 1 The given efficiency specification is to minimize the number of copies of y1 i e the number of nodes in GD 1 but provided that GD 1is connected and Standing Assumption 1 is met Note that agent 2has to estimate y1 i e 1 NE 2 even though agent 2is not directly affected by y1 i e 1 NI 2 otherwise the information could not travel between nodes 1and3 which are not communication neighbors In general a solution to this design problem can be obtained by solving an Unweighted Steiner Tree problem 20 for which distributed off the shelf algorithms are available 20 Sometimes it is useful to define agent wise quantities indicated with a tilde Let yi col yi p p N E i y col yi i I Rny 10 where yicollects all the estimates kept by agent i III D ISTRIBUTED OPTIMIZATION ALGORITHMS In this section we leverage the END framework to extend several distributed optimization algorithms by exploiting partial coupling We recall the cost coupled problem in 1 min y Rnyf y P i Ifi yp p N I i 11 where fiis a private cost function of agent i and we choose the optimization variable y col yp p P as the variable of interest in the END with some usual overloading we write fi y fi yp p N E i fi yp p N I i 12 LetY be the solution set of 11 assumed to be nonempty A END dual methods Under Standing Assumption 1 we can recast 11 by introducing local estimates and consensus constraints The following redormulation is not novel and in fact it was employed for the dual methods in 11 13  Proposition 1 Let Assumption 1 hold Then problem 11 is equivalent to min y RnyP i Ifi yi fi yi p p N E i s t yi p yj p p P i j ED p 13 IfGD p GCfor all p P then 13 reverts to the formulation used in standard dual methods 21 23 these algorithms require each agent to store a copy of the whole optimization vector Instead choosing a sparse GEcan con veniently reduces the number constraints in 13 Regardless due to its structure i e separable costs and coupling con straints compliant with GD p hence with the communication graph the problem in 13 can be immediately solved via several established Lagrangian based algorithms provided that the functions fi s are sufficiently well behaved In practice this allows one to extend most virtually all the existing dual methods to the END framework Example 1 END ADMM Let Assumption 1 hold and assume that fiis proper closed convex for all i I Applying the alternating direction method of multipliers ADMM in 22 to 13 4results in the iteration yk 1 i argmin yin fi yi P p N E i P j N Dp i yi p 2 zi j p yi p o 14a zk 1 i j p 1 Œ± zk i j p Œ±zk j i p 2Œ±yk 1 j p 14b where zi j p is an auxiliary variable kept by agent i for each i I p NE i j ND p i Then for any Œ± 0 1 yi pconverges to y p where y col y p p P is a solution of 11 for all i I andp NE i Note that performing the update 14b requires agent ito receive data from its neighbor j ND p i while 14b requires no communication If GD p GCfor all p P then the method retrieves the standard ADMM for consensus optimization 22 Eq 13 Yet in general 14 requires the agents to store and exchange less auxiliary variables While Proposition 1 would hold even if the graphs GD p s are only strongly connected distributed algorithms to efficiently solve 13 typically require undirected communication B END ABC algorithm In this subsection we propose an END version of the ABC algorithm recently developed in 5 For differentiable costs fi s let us consider the iteration i I p NE i yk 1 i p zk i p X j NE p Ap ip jpyk j p Œ≥ Bp ip jp ypfj yk j 15a zk 1 i p zk i p X j NE p Cp ip jpyk 1 j p 15b 4After decoupling the constraints in 13 by introducing auxiliary bridge variables as yi p h i j p h i j p h j i p h j i p yj p the approach is standard and we refer to 22 for a complete derivation where zi p Rnypis a local variable kept by agent i for allp P Ap Bp Cpare matrices in RNp Np Œ≥ 0is a step size and we recall the notation in 9 Note that if the matrices Ap Bp Cp s are compliant with the corresponding graphs GD p s e g Ap Bp C p WD p then the iteration 15 is distributed We can rewrite 15 in stacked form as yk 1 Ayk Œ≥B yf yk zk 16a zk 1 zk Cyk 1 16b where A diag Ap Inyp p P B diag Bp Inyp p P C diag Cp Inyp p P belong to Rny ny z col zp p P with zp col zi p i NE p and f y P i Ifi yi IfGD p GCfor all p and Ap Bp Cp are independent of p then 16 retrieves the ABC algorithm 5 Eq 3 We next charachterize the asymptotic behavior of 16 for appropriately chosen A B C all the proofs are in appendix We recall the notation in 7 8 Theorem 1 LetD diag Dp Inyp p P for some Dp RNp Np p P Assume that fiisL smooth and convex for each i I and that a A BD andB 0 D 0 b y C Dy y By y c C 0 null C C d BandCcommute BC CB e I 1 2C BD B 0 Lety Y y C y and consider the merit function M y max Œ† y yf y f y f y 17 Then for any y0 Rny z0 0ny Œ≥ Œªmin D L the sequence yk k Ngenerated by 16 satisfies M yk avg O 1 k 18 for all k N where yk avg 1 kPk t 1yt It is shown in 5 that many celebrated schemes for con sensus optimization can be retrieved as particular instances of the ABC algorithm by suitably choosing the matrices A B C 5 Tab 2 EXTRA 24 NEXT 18 DIGing 4 NIDS 25 and others Theorem 1 allows the extension of each of these methods to the END framework We only discuss an example below for the other schemes the analysis can be carried out analogously see also 5 III A Example 2 END AugDGM The following gradient tracking algorithm is the END version of 16 Alg 1 i I p NE i yk 1 i p X j N Dp i WD p ip jp yk j p Œ≥vk j p vk 1 i p X j N Dp i WD p ip jp vk j p ypfj yk 1 ypfj yk or in stacked form yk 1 WD yk Œ≥vk 20a vk 1 WD vk yf yk 1 yf yk 20b  we impose y 0 0 v 0 WD yf y0 Here vi p represents an estimate of ypP j Ifj y Npkept by agent i Note that agent ionly estimates and exchanges the components of the cost gradient and of the optimization variable specified by NE i instead of the whole vector as in 16 Alg 1 the two algorithms coincide only if WD p WCfor all p P By eliminating the vvariable in 20 we obtain yk 2 2WDyk 1 WD 2yk Œ≥ WD 2 yf yk 1 yf yk 21 Instead eliminating zfrom 16 we get yk 2 I C A yk 1 Ayk Œ≥B yf yk 1 yf yk 22 which retrieves 21 for A B WD 2 C I WD 2 5 This choice satisfies the conditions in Theorem 1 with D I under Assumption 1 and doubly stochasticity 6 Corollary 1 Let Assumption 1 hold assume that WD p1Np 1Np WD p WD p for all p P and that fiis L smooth and convex for all i I Then for any Œ≥ 0 1 L the rate 18 holds for 20 Theorem 1 requires a recovery procedure i e 18 holds for the running average only as e g in 26 but pointwise convergence could be shown for several special cases of 16 see e g 16 We note that Theorem 1 enhances cus tomizability with respect to 5 Th 24 even in the standard scenario 2 the sparsity oblivious case by allowing for non identical blocks Ap s Bp s Cp s corresponding to integrating different methods for the components of y C END Push sum DGD Techniques to solve optimization problems over switching or directed graphs also find their counterpart in the END framework As an example here we generalize the push sum subgradient algorithm in 17 Eq 1 Let the agents communicate over a time varying network GC k k N GC k I EC k Given a fixed estimate graph GE GI for each p P we consider a time dependent design graph GD k p k N GD k p NE p ED k p GC k note that the set of nodes is fixed in GD k p For all i I and p NE i agent iperforms the following updates qk 1 i p P j NE p WD k p ip jpqk j p 23a wk 1 i p P j NE p WD k p ip jpzk j p 23b gk 1 i p ypfi yk 1 i yk 1 i p wk 1 i p qk 1 i p 23c zk 1 i p wk 1 i p Œ≥kgk 1 i p 23d initialized at z0 i p Rnyp q0 i p 1 With respect to 17 Eq 1 agent ikeeps one scalar qi pfor each p NE i 5In fact the sequence yk generated by 16 coincide with that generated by 20 for the given initialization 6Note that the properties of WD p s easily translate to WDdue to the block structure in 6 For instance under the stated conditions clearly null I WD p 2 range 1Np and null I WD 2 C instead of one overall but does not store and exchange the variables zi p Rnypforp NE i Assumption 2 For all k Nandp P it holds that i Self loops for all i NE p i i ED k p ii Column stochasticity 1 NpWD k p 1 Np iii Finite weights WD k p ip jp ŒΩ 0 i j ED k p Assumption 3 There exists an integer Q 0such that for all p P GD k p k NisQ strongly connected Example 3 Choosing time varying design graphs As sume GC S k 1 Q 1 t kQGC k for all k Nand some strongly connected graph GC Choose some graphs GD p p P that satisfy Standing Assumption 1 and such that each GD p is strongly connected Then Assumption 3 holds by settihg GD k p GD pTGC k for all p P and all k N Theorem 2 Let Assumptions 2 and 3 hold Assume that for all i I fiis convex and there is L 0such that gi L for all y Rnyandgi yfi y Let Œ≥k k Nbe a positive non increasing sequence such thatP k 0Œ≥k P k 0 Œ≥k 2 Then the sequence yk k Ngenerated by 23 converges to C y for some y Y D Constraint coupled distributed optimization Finally we study a different constraint coupled problem min xi Rnxi i IP i Ifi xi s t P i NI p Ap ixi ap i 0 p P 24a 24b for a given interference graph GI P I EI where fiand Ap i Rnyp nxi ap i Rnyp p N I i are private data kept by agent i and the constraints 24b are notcompliant with the communication graph GC namely NI p NC i for anyi Differently from what we did with the cost coupled problem in 11 here we choose as the variable of interest the dual variable associated with the constraints in 24b y col yp p P Rny Typical distributed methods to solve 24 require each agent to store a copy of the entire dual variable and possibly of other variables in Rny e g an estimate of the constraint violation 27 28 END primal dual or dual methods can improve efficiency by exploiting the sparsity of GI For instance a simplified version of the algorithm in 1 Eq 31 can be directly used to solve 24 Alternatively let us consider the dual of 24 max y RnyP i IœÜi yp p N I i 25 œÜi y min xi Rnxifi xi P p N I i yp Ap ixi ap i note that 25 is in the form 11 In fact 25 was solved in 12 via the reformulation 13 this approach has the disadvantage of requiring undirected communication Nonetheless 25 can also be solved over directed time varying networks e g via the iteration in 23 7 7If each fiis convex with compact domain where the subgradients of the local dual function œÜican be computed as gk i p Ap ix i yk i ap i withx i yi argminxi Rnxifi xi P p N I i yi p Ap ixi ap i  Fig 2 Distribution of sources red and sensors blue Sensors in the red circle receive signal from source p Sensors in the blue circle can receive data by but not necessarily send to sensor i IV I LLUSTRATIVE APPLICATION In this section we study numerically a regression problem with sparse measurements 2 13 arising from distributed estimation in wireless and ad hoc sensor networks Let us consider some sensors 1 2 N Iand some sources 1 2 P P spatially distributed on a plane in the square 0 1 0 1 as illustrated in Figure 2 Each source p emits a signal yp R sensed by all the sensors in a radius rs 0 in turn each sensor imeasures hi H icol yp p N I i wi 26 where hi Rnhi Hiis a known output matrix wiis the measurement noise Sensor ican send information to all the peers in a radius ri c e g proportional to the sensor specific power this induces a directed communication network GC I EC among the sensors which we assume to be strongly connected 1 Linear regression In our first simulation the sensors goal is to collaboratively solve the least square problem min y RPX i I hi Hicol yp p N I i 2 27 where NI i is the set of sources positioned less than rs away from sensor i Problem 27 is in the form 11 We seek a solution via algorithm 23 with fixed communication graph comparing the performance for two choices of the design graphs Standard GD p s are chosen as in 2 with this choice 23 boils down to the standard Push sum DGD 17 Customized each GD pis designed to exploit the sparsity in 27 In particular we aim at optimizing the memory allocation for the estimates by minimizing the number of nodes in GD p provided that GD pmust be strongly connected and Standing Assumption 1 must be satisfied Design ing such a GD pcorresponds to approximately solving a Strongly Connected Steiner Subgraph Problem 29 where GD pis a subgraph of GC 8 We set N 100 P 20 and randomly generate sensor sources positions as in Figure 2 We choose rs 0 2 and draw each ri cuniformly in rmin c rmin c 0 1 For alli I we fix nhi 10 we generate Hiby first 8We use all the available edges i e GD p GC NE p 10010310610 2100102 0 05 0 1 0 15 0 2 0 2510 410 310 210 1Fig 3 Linear regression via algorithm 23 for different values of the minimum sensor communication radius rmin cand stopping criterion V y 10 2 bottom and the trajectories obtained with rmin c 0 1 top A larger rmin cinduces a denser graph GC uniformly drawing entries in 0 1 and then normalizing the rows to unitary norm we draw each element of wi from an unbiased normal distribution with variance 0 1 each signal ypis uniformly randomly chosen in 0 1 the step size is set as Œ≥k k 0 51in 23 9The ad vancement is evaluated via the merit function V y max diag 1 NpI p P Œ† y yf y f Œ† y f y where y C y andy solves 27 Figure 3 shows the results for different values of rmin c For rmin c 0 1 the customized method is 15 times faster then the standard one Increasing rmin c only marginally reduces the per iteration communication cost of the customized method In fact already for rmin c 0 25 the graph GC NI p is strongly connected for all p P soGE GIcan be chosen in other terms each agent only estimates and exchanges the components of ythat directly affect its local cost while it also has to estimate other components for smaller rmin c In this situation the customized method achieves a reduction of the communication cost where sending a variable to all the neighbors on GChas a cost of 1 in a broadcast fashion of over 99 9 2 LASSO Next we assume that only 30 of the sources emits a signal at a given instant the vector yis sparse The sensors collaboratively solve the following problem regularized to promote sparsity min y RP y 1 X i I hi Hicol yp p N I i 2 where 1is the ‚Ñì1norm By defining fi yp p N I i hi Hicol yp p N I i 2 P p N I i 1 NI p yp we re trieve the form 11 We set N 10 P 20 rmin c 9Although the bounded subgradient assumption in Theorem 2 fails boundedness of the sequences generated by 23 and hence convergence can be established based on coercivity of the cost function  0 2 0 3 0 4 0 5 0 6 0 7 0 810 210 1100Fig 4 LASSO via algorithm 23 and different source ranges rs 0 1 nhi 1 for all i generate random positions for the sensors and sources and choose the other parameters as above for both the standard and customized methods Figure 4 compares the results for different values of rs For larger rs the interference graph GIis denser and the gap between customized and standard method decreases in fact forrs 0 8the two algorithms coincide as GIis complete Nonetheless when GIis sparse the customized algorithm saves up to 99 of the communication cost In conclusion while requiring some initial computational effort to choose the design graphs GD p the sparsity aware method results in substantial efficiency improvement es pecially if the estimation problem is solved repeatedly e g each time new signals are received from the sources V C ONCLUSION We have shown that the END framework 1 can be applied to a variety of distributed optimization problems to enhance efficiency by accounting for the intrinsic sparsity in the agents coupling Besides revisiting dual methods we derived the END i e sparsity aware version of the very general ABC method and of Push sum DGD and we showed how to efficiently tackle constraint coupled problems with sparse constraints even over directed graphs Our simula tions show that END algorithms can substantially reduce the computational and communication cost while introducing little complication in the convergence proof with respect to their sparsity unaware counterparts As sparsity aware END algorithms require some initial design effort for the allocation of the estimates their use is particularly recommended for problems with special struc ture 1 App A 4 or repeated time varying problems like distributed estimation and MPC Future work should focus on computationally efficient and distributed methods to perform the allocation of the estimates online thus avoiding the need for any a priori design A Proof of Theorem 1 We adapt the proof of 5 Th 24 We note that z0 0ny range B by the conditions a and d the update in 16 and an induction argument we have yk zk range B for all k 1 Hence we rewrite 16 as yk Byk zk Œ≥Bzk 28a yk 1 Dyk Œ≥ yf yk zk 28b zk 1 zk 1 Œ≥Cyk 1 28c for all k 1 Let Œ¶ y z f y y z the form in 28 can be exploited to prove the following lemma Lemma 1 Let yk yk zk be a sequence generated by 28 Then for all y C z C it holds that Œ¶ yk 1 avg z Œ¶ y z 1 2kh y z where h y z 1 Œ≥ y0 y 2 D Œ≥ B Œ† Œª z 2andŒª min Œª2 Cp p P Proof The proof is analogous to that of 5 Lemma 23 and omitted here Note that 5 uses a matrix notation i e y RI n while we need a stacked notation as the vectors yi i Iare not homogeneous in size Nonetheless 28 matches 5 Eq 33 which allows us to repeat all the steps in 5 Lem 23 with the only precaution of replacing J span 1m Œª2 C in 5 with Œ† C Œª For all z C so that z y 0 setting y y in Lemma 1 together with the definition of Œ¶ yields f yk avg f y yk avg z 1 2kh y z Further choosing z 2Œ† yk avg Œ† ykavg z with z yf y leads to f yk avg f y 2 z Œ† yk avg 1 2kh y 2z 29 By convexity and since z C by optimality conditions it holds that f yk avg f y yk avg y z Œ† yk avg z Œ† yk avg z the latter inequality and 29 imply M yk avg 1 2kh y 2z B Proof of Theorem 2 Note that for each p P 23 is the standard perturbed push sum protocol 17 Eq 4 with perturbation term Œ≥kgk 1 i p Therefore since gk 1 i p is uniformly bounded by assumption and by the choice of Œ≥k k N we can apply 17 Lem 1 to infer that for all i I p NE i lim k yk i p zk p 0 30 P k 0Œ≥k yk i p zk p 0 31 where zk p 1 NpP i NE p zk i p Rnyp for all k N Let us also define zk col zk p p P Rny By 23 and Assumption 2 ii it follows that zk 1 p zk p Œ≥k1 NpP i NE p gk 1 i p 32 We next show that limk zk y Y then the theorem follows by 30 The main complication with respect to the proof of 17 Th 1 is that we need a modification of 17 Lem 8 to cope with the non homogeneity of the estimates Lemma 2 For all y Y for all k N it holds that zk 1 y 2 D zk y 2 D 2Œ≥k f zk f y 4LŒ≥kX i IX p NE i zk p yk 1 i p Œ≥k 2NL2  where D diag NpInp p P Proof By 32 we have zk 1 y 2 D zk y 2 D 2Œ≥kX p PD zk p y p P i NE p gk 1 i pE Œ≥k 2X p P1 Np P i NE p gk 1 i p 2 33 The third addend on the right hand side of 33 is bounded above by Œ≥k 2NL2 For the second addend we have X p PD zk p y p P i NE p gk 1 i pE X i IX p NE i zk p yk 1 i p yk 1 i p y p gk 1 i p a X i I L col zk p p NE i yk 1 i fi yk 1 i fi y b X i I 2L col zk p p NE i yk 1 i fi zk 1 fi y where in a we used that gk 1 i yifi yk 1 i and convexity offi and b follows by adding and subtracting inside the sum fi zk 1 p p N E i fi zk 1 and by L Lipschitz continuity of fi The result follows by substituting the bound back into 33 We finally note that due to 31 and the choice of Œ≥k k N the inequality in Lemma 2 satisfies all the conditions of 17 Lem 7 in the norm D hence we can conclude that zk y for some y Y REFERENCES 1 M Bianchi and S Grammatico The END Estimation Network Design for games under partial decision information IEEE Transactions on Control of Network Systems Accepted for publication Online Available https arxiv org abs 2208 11377 2 I Necoara and D Clipici Parallel random coordinate descent method for composite minimization Convergence analysis and error bounds SIAM Journal on Optimization vol 26 no 1 pp 197 226 2016 3 P Richt arik and M Tak aÀác Distributed coordinate descent method for learning with big data Journal of Machine Learning Research vol 17 2016 4 A Nedi c A Olshevsky and W Shi Achieving geometric conver gence for distributed optimization over time varying graphs SIAM Journal on Optimization vol 27 no 4 pp 2597 2633 2017 5 J Xu Y Tian Y Sun and G Scutari Distributed algorithms for composite optimization Unified framework and convergence analy sis IEEE Transactions on Signal Processing vol 69 pp 3555 3570 2021 6 I Notarnicola R Carli and G Notarstefano Distributed partitioned big data optimization via asynchronous dual decomposition IEEE Transactions on Control of Network Systems vol 5 no 4 pp 1910 1919 2018 7 T Erseghe A distributed and scalable processing method based upon ADMM IEEE Signal Processing Letters vol 19 no 9 pp 563 566 2012 8 M Todescato N Bof G Cavraro R Carli and L Schenato Partition based multi agent optimization in the presence of lossy and asynchronous communication Automatica vol 111 p 108648 2020 9 P Giselsson M D Doan T Keviczky B D Schutter and A Rantzer Accelerated gradient methods and dual decomposition in distributed model predictive control Automatica vol 49 no 3 pp 829 833 2013 Online Available https www sciencedirect com science article pii S0005109813000101 10 E Dall Anese H Zhu and G B Giannakis Distributed optimal power flow for smart microgrids IEEE Transactions on Smart Grid vol 4 no 3 pp 1464 1475 2013 11 J F Mota J M Xavier P M Aguiar and M Puschel Distributed optimization with local domains Applications in MPC and network flows IEEE Transactions on Automatic Control vol 60 no 7 pp 2004 2009 2015 12 S A Alghunaim K Yuan and A H Sayed A proximal diffusion strategy for multiagent optimization with sparse affine constraints IEEE Transactions on Automatic Control vol 65 no 11 pp 4554 4567 2020 13 S A Alghunaim and A H Sayed Distributed coupled multiagent stochastic optimization IEEE Transactions on Automatic Control vol 65 no 1 pp 175 190 2020 14 P Rebeschini and S Tatikonda Locality in network optimization IEEE Transactions on Control of Network Systems vol 6 2019 15 R Brown F Rossi K Solovey M Tsao M T Wolf and M Pavone On local computation for network structured convex optimization in multi agent systems IEEE Transactions on Control of Network Systems 2021 16 J Xu S Zhu Y C Soh and L Xie Augmented distributed gradient methods for multi agent optimization under uncoordinated constant stepsizes in 2015 54th IEEE Conference on Decision and Control CDC 2015 pp 2055 2060 17 A Nedi c and A Olshevsky Distributed optimization over time varying directed graphs IEEE Transactions on Automatic Control vol 60 no 3 pp 601 615 2015 18 P D Lorenzo and G Scutari NEXT In network nonconvex opti mization IEEE Transactions on Signal and Information Processing over Networks vol 2 no 2 pp 120 136 19 M Bianchi and S Grammatico Fully distributed Nash equilibrium seeking over time varying communication networks with linear con vergence rate IEEE Control Systems Letters vol 5 no 2 pp 499 504 2021 20 J C Parinya and Fakcharoenphol Simple distributed algorithms for approximating minimum Steiner trees L Wang Ed Springer Berlin Heidelberg 2005 pp 380 389 21 S Boyd N Parikh E Chu B Peleato and J Eckstein Distributed optimization and statistical learning via the alternating direction method of multipliers Foundations and Trends in Machine Learning vol 3 2010 22 N Bastianello R Carli L Schenato and M Todescato Asyn chronous distributed optimization over lossy networks via relaxed ADMM Stability and linear convergence IEEE Transactions on Automatic Control vol 66 no 6 pp 2620 2635 23 C A Uribe S Lee A Gasnikov and A Nedi c A dual approach for optimal algorithms in distributed optimization over networks Optimization Methods and Software vol 36 pp 1 37 2021 24 W Shi Q Ling G Wu and W Yin EXTRA An exact first order algorithm for decentralized consensus optimization SIAM Journal on Optimization vol 25 no 2 pp 944 966 2015 25 Z Li W Shi and M Yan A decentralized proximal gradient method with network independent step sizes and separated convergence rates IEEE Transactions on Signal Processing vol 67 no 17 pp 4494 4506 26 G Qu and N Li Harnessing smoothness to accelerate distributed optimization IEEE Transactions on Control of Network Systems vol 5 pp 159 166 2018 27 A Falsone I Notarnicola G Notarstefano and M Prandini Tracking ADMM for distributed constraint coupled optimization Automatica vol 117 p 108962 2020 28 X Li G Feng and L Xie Distributed proximal algorithms for multiagent optimization with coupled inequality constraints IEEE Transactions on Automatic Control vol 66 no 3 pp 1223 1230 2021 29 M Charikar C Chekuri T yat Cheung Z Dai A Goel S Guha and M Li Approximation algorithms for directed Steiner problems Journal of Algorithms vol 33 pp 73 91 1999  
 We study interactive learning of language agents based on user edits made to the agent s output In a typical setting such as writing assistants the user interacts with a language agent to generate a response given a context and may optionally edit the agent response to personalize it based on their latent preference in addition to improving the correctness The edit feedback is naturally generated making it a suitable candidate for improving the agent s alignment with the user s preference and for reducing the cost of user edits over time We propose a learning frame work PRELUDE to conduct PRE ference Learning from User s Direct Edits by inferring a description of the user s latent preference based on historic edit data and using it to define a prompt policy that drives future response generation This avoids fine tuning the agent which is costly challenging to scale with the number of users and may even degrade its performance on other tasks Furthermore learn ing descriptive preference improves interpretability allowing the user to view and modify the learned preference However user preference can be complex subtle and vary based on context making it challenging to learn To address this we propose a simple yet effective algorithm named CIPHER Consolidates Induced Preferences based on Historical Edits with Retrieval CIPHER leverages a large language model LLM to infer the user preference for a given context based on user edits In the future CIPHER retrieves inferred preferences from the k closest contexts in the history and forms an aggregate preference for response generation We introduce two interactive environments summarization and email writing for evaluation using a GPT 4 simulated user We compare with algorithms that directly retrieve user edits but do not learn descriptive preference and algorithms that learn context agnostic preference On both tasks CIPHER outperforms baselines by achieving the lowest edit distance cost Meanwhile CIPHER has a lower computa tional expense as using learned preference results in a shorter prompt than directly using user edits Our further analysis reports that the user preference learned by CIPHER shows significant similarity to the ground truth latent preference 1 1 ,Aligning LLM Agents by Learning Latent Preference from User Edits Ge Gao Alexey Taymanov Eduardo Salinas Paul Mineiro Dipendra Misra Department of Computer Science Cornell University Microsoft Research New York ggao cs cornell edu ataymano edus pmineiro dimisra microsoft com AbstractIntroduction Language agents based on large language models LLMs have been developed for a variety of applications Dohmke 2022 Brynjolfsson et al 2023 following recent breakthroughs in improving LLMs Achiam et al 2023 Ouyang et al 2022b Team et al 2023 However despite their impres sive zero shot performance LLMs still need to adapt and personalize to a given user and task Mysore et al 2023 Li et al 2023 In many applications a natural feedback for LLM based agents is user edits where a user queries the agent and edits the agent s response before their own final use In Equal contribution 1Our code and data are publicly available at https github com gao g prelude Preprint arXiv 2404 15269v1 cs CL 23 Apr 2024 Interactive Learning from User Edits Agent incurs a cost Round t latexit sha1_base64 A0YNLl7TgtZmToQd LF2 q Bj78 AAAB6HicdVDLSgNBEJyNrxhfUY9eBoPgaZmNRpNb0IvHBMwDkiXMTmaTMbMPZnqFsOQLvHhQxKuf5M2 cZKsoKIFDUVVN91dXiyFBkI rNzK6tr6Rn6zsLW9s7tX3D9o6yhRjLdYJCPV9ajmUoS8BQIk78aK08CTvONNrud 554rLaLwFqYxdwM6CoUvGAUjNWFQLBG7QpzahYOJTRYwpFwhtSrBTqaUUIbGoPjeH0YsCXgITFKtew6JwU2pAsEknxX6ieYxZRM64j1DQxpw7aaLQ2f4xChD7EfKVAh4oX6fSGmg9TTwTGdAYax e3PxL6 XgF91UxHGCfCQLRf5icQQ4fnXeCgUZyCnhlCmhLkVszFVlIHJpmBC PoU 0 aZds5s8vN81L9Kosjj47QMTpFDrpEdXSDGqiFGOLoAT2hZ vOerRerNdla87KZg7RD1hvnyQAjSo latexit Step 1 User and the world provides a context to the LLM agent xt latexit sha1_base64 fOBpdrE9QlvaWNMd4DFWhYg2sCA AAAB6nicdVDLSgNBEOyNrxhfUY9eBoPgaZmNRpNb0IvHiOYByRJmJ7PJkNkHM7NiWPIJXjwo4tUv8ubfOElWUNGChqKqm 4uLxZcaYw rNzS8srqWn69sLG5tb1T3N1rqSiRlDVpJCLZ8YhigoesqbkWrBNLRgJPsLY3vpz57TsmFY CWz2JmRuQYch9Tok20s19X eLJWxXsFM7cxC28RyGlCu4VsXIyZQSZGj0i 9QUSTgIWaCqJU18GxdlMiNaeCTQu9RLGY0DEZsq6hIQmYctP5qVN0ZJQB8iNpKtRorn6fSEmg1CTwTGdA9Ej99mbiX1430X7VTXkYJ5qFdLHITwTSEZr9jQZcMqrFxBBCJTe3IjoiklBt0imYEL4 Rf TVtl2Tuzy9WmpfpHFkYcDOIRjcOAc6nAFDWgChSE8wBM8W8J6tF6s10Vrzspm9uEHrLdPtQSOFQ latexit xt latexit sha1_base64 fOBpdrE9QlvaWNMd4DFWhYg2sCA AAAB6nicdVDLSgNBEOyNrxhfUY9eBoPgaZmNRpNb0IvHiOYByRJmJ7PJkNkHM7NiWPIJXjwo4tUv8ubfOElWUNGChqKqm 4uLxZcaYw rNzS8srqWn69sLG5tb1T3N1rqSiRlDVpJCLZ8YhigoesqbkWrBNLRgJPsLY3vpz57TsmFY CWz2JmRuQYch9Tok20s19X eLJWxXsFM7cxC28RyGlCu4VsXIyZQSZGj0i 9QUSTgIWaCqJU18GxdlMiNaeCTQu9RLGY0DEZsq6hIQmYctP5qVN0ZJQB8iNpKtRorn6fSEmg1CTwTGdA9Ej99mbiX1430X7VTXkYJ5qFdLHITwTSEZr9jQZcMqrFxBBCJTe3IjoiklBt0imYEL4 Rf TVtl2Tuzy9WmpfpHFkYcDOIRjcOAc6nAFDWgChSE8wBM8W8J6tF6s10Vrzspm9uEHrLdPtQSOFQ latexit Step 2 LLM Agent generates a response given the context yt latexit sha1_base64 ix6PfFjCx5pvhchPC5u71ke6Vac AAAB6nicdVDLSgNBEOz1GeMr6tHLYBA8LbPRaHILevEY0TwgWcLsZDYZMvtgZlZYlnyCFw KePWLvPk3TpIVVLSgoajqprvLiwVXGuMPa2l5ZXVtvbBR3Nza3tkt7e23VZRIylo0EpHsekQxwUPW0lwL1o0lI4EnWMebXM38zj2TikfhnU5j5gZkFHKfU6KNdJsO9KBUxnYVO VzB2Ebz2FIpYrrNYycXClDjuag9N4fRjQJWKipIEr1HBxrNyNScyrYtNhPFIsJnZAR6xkakoApN5ufOkXHRhkiP5KmQo3m6veJjARKpYFnOgOix q3NxP 8nqJ9mtuxsM40Syki0V IpCO0OxvNOSSUS1SQwiV3NyK6JhIQrVJp2hC PoU U aFds5tSs3Z XGZR5HAQ7hCE7AgQtowDU0oQUURvAAT BsCevRerFeF61LVj5zAD9gvX0CtoqOFg latexit xt latexit sha1_base64 fOBpdrE9QlvaWNMd4DFWhYg2sCA AAAB6nicdVDLSgNBEOyNrxhfUY9eBoPgaZmNRpNb0IvHiOYByRJmJ7PJkNkHM7NiWPIJXjwo4tUv8ubfOElWUNGChqKqm 4uLxZcaYw rNzS8srqWn69sLG5tb1T3N1rqSiRlDVpJCLZ8YhigoesqbkWrBNLRgJPsLY3vpz57TsmFY CWz2JmRuQYch9Tok20s19X eLJWxXsFM7cxC28RyGlCu4VsXIyZQSZGj0i 9QUSTgIWaCqJU18GxdlMiNaeCTQu9RLGY0DEZsq6hIQmYctP5qVN0ZJQB8iNpKtRorn6fSEmg1CTwTGdA9Ej99mbiX1430X7VTXkYJ5qFdLHITwTSEZr9jQZcMqrFxBBCJTe3IjoiklBt0imYEL4 Rf TVtl2Tuzy9WmpfpHFkYcDOIRjcOAc6nAFDWgChSE8wBM8W8J6tF6s10Vrzspm9uEHrLdPtQSOFQ latexit yt latexit sha1_base64 ix6PfFjCx5pvhchPC5u71ke6Vac AAAB6nicdVDLSgNBEOz1GeMr6tHLYBA8LbPRaHILevEY0TwgWcLsZDYZMvtgZlZYlnyCFw KePWLvPk3TpIVVLSgoajqprvLiwVXGuMPa2l5ZXVtvbBR3Nza3tkt7e23VZRIylo0EpHsekQxwUPW0lwL1o0lI4EnWMebXM38zj2TikfhnU5j5gZkFHKfU6KNdJsO9KBUxnYVO VzB2Ebz2FIpYrrNYycXClDjuag9N4fRjQJWKipIEr1HBxrNyNScyrYtNhPFIsJnZAR6xkakoApN5ufOkXHRhkiP5KmQo3m6veJjARKpYFnOgOix q3NxP 8nqJ9mtuxsM40Syki0V IpCO0OxvNOSSUS1SQwiV3NyK6JhIQrVJp2hC PoU U aFds5tSs3Z XGZR5HAQ7hCE7AgQtowDU0oQUURvAAT BsCevRerFeF61LVj5zAD9gvX0CtoqOFg latexit Step 3 User edits the response to before using it y0t latexit sha1_base64 xey nIxyXLaQVpdktF5dXCISDcs AAAB63icdVBNSwMxEM3Wr1q qh69BIvoaclWq 2t6MVjBVsL7VKyabYNTbJLkhXK0r gxYMiXv1D3vw3ZtsVVPTBwOO9GWbmBTFn2iD04RSWlldW14rrpY3Nre2d8u5eR0eJIrRNIh6pboA15UzStmGG026sKBYBp3fB5Crz7 6p0iySt2YaU1 gkWQhI9hk0vR4YAblCnJryGucexC5aA5LqjXUqCPo5UoF5GgNyu 9YUQSQaUhHGvd81Bs BQrwwins1I 0TTGZIJHtGepxIJqP53fOoNHVhnCMFK2pIFz9ftEioXWUxHYToHNWP 2MvEvr5eYsO6nTMaJoZIsFoUJhyaC2eNwyBQlhk8twUQxeyskY6wwMTaekg3h61P4P lUXe Urd6cVZqXeRxFcAAOwQnwwAVogmvQAm1AwBg8gCfw7Ajn0XlxXhetBSef2Qc 4Lx9Ahdkjkc latexit yt latexit sha1_base64 ix6PfFjCx5pvhchPC5u71ke6Vac AAAB6nicdVDLSgNBEOz1GeMr6tHLYBA8LbPRaHILevEY0TwgWcLsZDYZMvtgZlZYlnyCFw KePWLvPk3TpIVVLSgoajqprvLiwVXGuMPa2l5ZXVtvbBR3Nza3tkt7e23VZRIylo0EpHsekQxwUPW0lwL1o0lI4EnWMebXM38zj2TikfhnU5j5gZkFHKfU6KNdJsO9KBUxnYVO VzB2Ebz2FIpYrrNYycXClDjuag9N4fRjQJWKipIEr1HBxrNyNScyrYtNhPFIsJnZAR6xkakoApN5ufOkXHRhkiP5KmQo3m6veJjARKpYFnOgOix q3NxP 8nqJ9mtuxsM40Syki0V IpCO0OxvNOSSUS1SQwiV3NyK6JhIQrVJp2hC PoU U aFds5tSs3Z XGZR5HAQ7hCE7AgQtowDU0oQUURvAAT BsCevRerFeF61LVj5zAD9gvX0CtoqOFg latexit 123y0t latexit sha1_base64 xey nIxyXLaQVpdktF5dXCISDcs AAAB63icdVBNSwMxEM3Wr1q qh69BIvoaclWq 2t6MVjBVsL7VKyabYNTbJLkhXK0r gxYMiXv1D3vw3ZtsVVPTBwOO9GWbmBTFn2iD04RSWlldW14rrpY3Nre2d8u5eR0eJIrRNIh6pboA15UzStmGG026sKBYBp3fB5Crz7 6p0iySt2YaU1 gkWQhI9hk0vR4YAblCnJryGucexC5aA5LqjXUqCPo5UoF5GgNyu 9YUQSQaUhHGvd81Bs BQrwwins1I 0TTGZIJHtGepxIJqP53fOoNHVhnCMFK2pIFz9ftEioXWUxHYToHNWP 2MvEvr5eYsO6nTMaJoZIsFoUJhyaC2eNwyBQlhk8twUQxeyskY6wwMTaekg3h61P4P lUXe Urd6cVZqXeRxFcAAOwQnwwAVogmvQAm1AwBg8gCfw7Ajn0XlxXhetBSef2Qc 4Lx9Ahdkjkc latexit ct edit yt y0t latexit sha1_base64 CTH5mNcE52uui2J dLNhhi0z8zI AAACEHicdVDJSgNBEO1xN25Rj14ag6ggoSeuOQhBPXiMYFRIwtDTqWhjz0J3jTgM QQv ooXD4p49ejNv7GzCCr6oODxXhVV9fxYSYOMfThDwyOjY MTk7mp6ZnZufz8wpmJEi2gJiIV6QufG1AyhBpKVHARa CBr Dcvz7s uc3oI2MwlNMY2gG DKUbSk4WsnLrwoP6T5tHIFC7mUNhFvUQQYtiZ3OWurhBk1XPVz38gVW3GZuecelrMh6sKS0zcp7jLoDpUAGqHr590YrEkkAIQrFjam7LMZmxjVKoaCTayQGYi6u SXULQ15AKaZ9R7q0BWrtGg70rZCpD31 0TGA2PSwLedAccr89vrin959QTbe81MhnGCEIr onaiKEa0mw5tSQ0CVWoJF1raW6m44poLtBnmbAhfn9L yVmp6G4WSydbhcrBII4JskSWyRpxyS6pkGNSJTUiyB15IE k2bl3Hp0X57XfOuQMZhbJDzhvnyjGnK0 latexit minimizeTXt 1ct latexit sha1_base64 YuFQWylcNpMQgh4JYusIx EE f0 AAACHHicdZBNSyNBEIZ7 Dbrulk9emkMC Jh6Inr10EQ9 JRwaiQiUNPp2Iau3uG7prFOOSHePGvePHgIl48CP4bOzGCLrsvNLw8VUV1vWmupEPGXoKx8YnJqemZ2cqXua z36rfF45dVlgBDZGpzJ6m3IGSBhooUcFpboHrVMFJevFrUD 5DdbJzBxhL4eW5udGdqTg6FFSXYt1ml2WWhqp5RX0aRxX4q7LuYDVkoWbQnvkCp2UuBP1z46oSDCp1li4zqLtjYiykA3lTX2dbW8xGo1IjYx0kFSf4nYmCg0GheLONSOWY6vkFqVQ0K EhQO 8YKfQ9NbwzW4Vjk8rk9 eNKmncz6Z5AO6ceJkmvnejr1nZpj1 1dG8B 1ZoFdrZapTR5gWDE26JOoShmdJAUbUsLAlXPGy6s9H losstF jzrPgQ3i l zfH9TBaC uHP2u7e6M4ZsgSWSYrJCKbZJfskwPSIIJck1tyT 4EN8Fd8BA8vrWOBaOZRfJJwfMrcLGhkg latexit Minimize cumulative costFarming a part of agriculture involves growing crops and rearing animals for food and raw materials It began thousands of years ago likely in the Fertile Crescent and led to the Neolithic Revolution as people transitioned from nomadic hunting to settled farming This allowed for a signiÔ¨Åcant increase in human population Article user provided article Please summarize the above article Farming as a part of agriculture involves growing crops cultivation and animal rearing for food and raw materials Originated It began thousands of years ago likely in the Fertile Crescent leading to the Neolithic Revolution Transition as people transitioned from nomadic hunting to settled farming resulted in signiÔ¨Åcant human population increase Figure 1 Illustration of interactive learning from user edits Color coding in edits is for visualization only our agent takes the plain revised text as feedback contrast typical feedback used for fine tuning such as the comparison based preference feedback in RLHF is explicitly collected by providing annotators with model responses and asking them to rank Ziegler et al 2019 Stiennon et al 2020 Nakano et al 2021 Ouyang et al 2022a inter alia making such feedback an expensive choice for improving alignment Motivated by this observation we focus on interactive learning of LLM based language agents using user edits as feedback Consider the scenario in Figure 1 where a user interacts with an LLM based writing assistant agent to complete their task The interaction starts with the user and the world providing a context to the agent This context may include a query prompt provided by the user along with additional information provided by the world such as the content on the screen current time and the user s calendar information The agent generates a textual response to the user given the context In the beginning the agent s response may not be optimal for the user as it is not personalized to this user s individual needs and preference As most users are not familiar with prompt engineering and LLMs are often able to generate an acceptable response for the task therefore users may find it the most convenient to simply edit the response when it is not ideal to suit their needs rather than trying different prompts to get new responses The example in Figure 1 illustrates that the user directly edits the summary generated by the agent to satisfy their preference on bullet point format It takes time and efforts for the user to make edits We can measure such cost using a variety of metrics such as the edit distance between the agent generated response and the user revised text Zero edit from the user is also a useful feedback reflecting that the agent s response satisfies this user s needs One important feature of our setting is that every natural use of the agent yields an edit feedback for learning Since there is no distinction between training and testing in this setting we care about minimizing the user s efforts across all rounds of interaction with the agent In summary our goal is to learn from the implicit feedback in user edit history to minimize the cumulative cost of the user s efforts We conjecture that user edits are driven by user s hidden preference which can be described in natural language These preference descriptions are different from the notion of comparison based preference used in RLHF In this paper we use the word preference to mean preference descriptions For instance preference of the user in Figure 1 can be described as bullet points In practice user preference can be compound such as preferring bullet point informal with emojis at the same time and also context dependent e g informal tone when writing an email to a family member and formal tone when writing to a colleague In more complex settings user preference can evolve with time non stationary or depend on information unavailable in the context partially observed Such user preference may not be fully derivable from the context and the user may not even be fully aware of all their preference These considerations imply that user preference is latent to the language agent If the agent could learn the latent preference correctly it can significantly improve its performance by generating satisfactory responses accordingly Furthermore preference learned by the agent can be shown to the user to enhance interpretability and can even be modified by the user to improve correctness Motivated by this we propose a learning framework PRELUDE PRE ference Learning from User s Direct Edits where we seek to learn a textual description of the user preference for a given context using the history of user edits 2 In a typical real world scenario such as writing assistants one has to potentially update the LLM based agent for every user Efficient approaches therefore must scale with the number of users This makes approaches that perform a full fine tuning of the LLM used by the agent very hard to scale Furthermore LLMs typically undergo evaluation on a variety of metrics before being released and thus fine tuning them often results in breaking the generalization guarantees offered by these tests For example fine tuning GPT 4 for millions of users can quickly turn very expensive Approaches such as adding LORA and Adapter layers and only updating them or using federated learning can reduce the expense to some extent while the loss of generalizable alignment remains as a concern In this work we focus on leveraging a frozen black box LLM and instead learning a prompt policy that can infer textual description of user s preference for a given context and then use it to directly drive the response generation We introduce a simple yet effective algorithm CIPHER Consolidates Induced Preferences based on Historical Edits with Retrieval under the PRELUDE framework For a given context CIPHER first retrieves the k closest contexts from history and aggregates inferred preferences for these kcontexts It relies on this aggregate preference to generate a response for the given context If the user performs no edits then it saves this aggregate preference as the correct preference for the given context Otherwise it queries the LLM to infer a plausible preference that explains these user edits made to the agent response and saves this inferred preference as the correct preference for the given context A key advantage of CIPHER is that it typically leads to significantly shorter prompts compared to other retrieval methods that use the entire documents or context as inferred preferences are much shorter than retrieved documents or contexts This results in a significant reduction in the computational expense of querying the LLM We introduce two interactive environments for evaluation inspired by writing assistant applications In the first environment we evaluate the agent s ability to summarize a given document articles from different sources In the second environment we evaluate the agent s ability to compose an email using content from a given document notes for various purpose In both tasks we simulate a GPT 4 user that can generate edits based on a pre designed latent preference We use documents from several existing domains as our user provided context and vary the GPT 4 user s preference based on the domain in order to capture the real world context dependent nature of human user s preference We evaluate CIPHER against several baselines including approaches that learn context agnostic user preferences and retrieval based approaches that do not learn preferences but directly use past user edits for generation We show that for both tasks CIPHER achieves the lowest user edit cost compared to baselines and significantly reduces the cumulative cost compared to using the frozen base agent Additionally CIPHER results in a lower LLM query cost than other retrieval based baselines Finally we qualitatively and quantitatively analyze preferences learned by our agents and find that they show significant similarity to the ground truth latent preferences in our setup 2 Interactive Learning from User Edits and the PRELUDE Framework We first describe LLM agents and the general learning framework from user edits We then describe our specialized PRELUDE framework for learning descriptive user preference and discuss associated learning challenges LLM and Language Agents We assume access to a language agent that internally relies on an LLM We make no assumption about the language agent except that it can take input xtas a piece of content and an additional prompt which can be in context learning examples or learned preferences and generate a response yt The language agent may simply perform greedy decoding on the LLM or may perform complex planning using the given LLM to generate a response Protocol 1 Interactive Learning from User Edits 1 fort 1 2 Tdo 2 User and the world provide a context xt 3 Agent generates a response ytgiven the context xt 4 User edits the response to y t 5 Agent receives a cost of ct edit yt y t 6 Evaluate the agent and learning algorithm onPT t 1ct 3 Interactive Learning from User Edits In an application such as a writing assistant a user interacts with the language agent over Trounds Protocol 1 shows such learning protocol In the tthround the user and the world provide a context xt X where Xis the space of all possible contexts This context will include the user prompt in text along with additional information provided by the user or the world and may include multimodal data as well such as images Given the context xt the language agent generates a response yt Y in text where Yis the space of all texts The user edits the response yttoy t If the user does not perform any edits we treat this as setting y t yt The agent receives a cost of ct edit yt y t for this round which measures the user s efforts on making edits The goal of the agent is to minimize the sum of costs across all roundsPT t 1ct In our experiments we use editas Levenshtein edit distance Levenshtein 1965 in the token space which computes the minimum number of total token addition token deletion and token substitution necessary to convert yttoy t In general a higher edit distance implies that the user has made more edits and spent more efforts We note that our framework is general enough to accommodate situations where the user tries different prompts with the same demand We treat each call to the language agent as a different round with a different context as the context includes the user prompt PRELUDE Framework We describe our PRELUDE framework in Protocol 2 which is a special ization of the general learning setup described above in Protocol 1 In PRELUDE in the tthround the agent infers the preference of the user as ft and uses it to generate a response We assume that in this round and for the given context xt the user has a latent preference f tthat drives the user to perform all edits Furthermore we assume that if the agent was able to infer this latent preference ft f t then it will lead to minimal possible edits 2To remove the dependence on performance due to the choice of the base LLM agent we compare with an oracle agent that has access to f tat the start of each round We assume that the LLM remains frozen across all methods in this work Protocol 2 PRELUDE PRE ference Learning from User s Direct Edits 1 fort 1 2 Tdo 2 User presents a text context xt 3 Agent infers a preference ftusing the history x‚Ñì y‚Ñì y ‚Ñì t 1 ‚Ñì 1and context xt 4 Agent uses ftandxtto generate a response yt 5 User edits the response to y tusing their latent preference f t 6 Agent incurs a cost ct yt y t 7 ReturnPT t 1ct Challenges of Learning User Preference Learning user preference from edits is challenging In practice user preference are multifaceted and complex Furthermore user s preference can also signif icantly vary based on the context The feedback in the form of user edits emerges naturally but is inher ently implicit lacking direct expressions of the actual preference and carrying subtleties that may lead to diverse interpretations The combination of preference variability and the implicit nature of feed back poses considerable challenges for agents in accurately learning and integrating these preferences 3 Learning User Preference through Retrieval and Aggregation In this section we present our method CIPHER Consolidates Induced Preferences based on Historical Edits with Retrieval that learns user preference based on user edits Algorithm 1 shows CIPHER which implements the PRELUDE framework CIPHER maintains a preference history Dt x‚Ñì f‚Ñì t 1 ‚Ñì 1of past contexts x‚Ñìalong with a preference f‚Ñìinferred by the agent CIPHER assumes access to a context representation function œï X Rdthat can map a context to a vector representation For a given round twith context xt the agent first retrieves the k closest contexts from the interaction history Dt We use cosine similarity for computing proximity although other metrics such as Euclidean distance or Hamming distance when œïoutputs a binary vector can be used Given the retrieved contexts and their inferred preferences xzi fzi k i 1 we 2The edit cost in practice may not always be 0 as the language agent could be incapable of adeptly using the correct preference or the user may perform edits that are inconsistent with their preference 4 query the underlying LLM to summarize the inferred preferences fzi k i 1into a single preference ft In the beginning when t k we retrieve all the past tcontexts In particular for t 1we have f1as an empty string as the agent has no prior knowledge of this user s preference 3 The agent uses the inferred preference ftto generate the response This is done by concatenating the context xtwith an agent prompt such as This user has a preference of ft which must be used when generating the response where ft indicates where we insert the inferred preference ft We list the actual template used in our experiments in Table 7 in Appendix A Given the user edits y t if the user edits are minimal i e edit yt y t Œ¥for a hyperparameter Œ¥ then we set the inferred preference for this round as ft ftas using ftfor generating a response resulted in minimal edits However if edit yt y t Œ¥ then we query the LLM a third time to generate the inferred preference ftthat explains why the user edited yttoy t We call this the Latent Preference Induction LPI step In both cases we append xt ft to the preference history Note that we cannot query the LLM for the inferred preference in the first case where the user edit costctis small i e ct Œ¥ In this case querying the LLM to infer the preference to explain the edits iny tgiven yt will result in the LLM outputting that the agent has no preference This is incorrect as it merely shows that the preference ftused to generate ytwas sufficiently good to include most of the true user preference f t Computational Cost of CIPHER In a given round CIPHER adds a maximum of 3 LLM calls on top of the cost of calling the underlying inference algorithm of the agent in line 6 CIPHER further reduces the memory storage by only storing the representation of contexts in the preference string instead of the input itself Finally CIPHER only adds a small prompt to the context xt before calling the agent s inference algorithm This only slightly increases the length of the prompt thereby reducing the query cost associated with LLMs that scales with the number of input tokens Algorithm 1 CIPHER œï k Œ¥ A context representation function œï X Rd the retrieval hyperparameter k and tolerance hyperparameter Œ¥ 0 1 D 2 fort 1 2 Tdo 3 User and the world presents a context xt 4 Retrieve the top kexamples œï xzi fzi k i 1inDwith maximum cosine similarity to œï xt 5 Ifk 1 then query the LLM to aggregate these preferences fzi k i 1intoft else ft fz1 6 Agent generates a text response ytbased on xtandft 7 User edits the response to y tusing their latent preference f t 8 Agent incurs a cost ct edit yt y t 9 ifct Œ¥then 10 ft ft 11 else 12 Query the LLM to generate a preference ftthat best explains user edits in yt y t 13 D D œï xt ft 14 ReturnPT t 1ct 4 Experiment In this section we first introduce two interactive tasks for evaluating agents that learn from user edits These tasks can be used more broadly even outside the PRELUDE framework and can be of independent interest We then describe our baselines and provide implementation details of CIPHER Finally we provide quantitative results in terms of user edit cost and qualitative analysis of the learned preferences 3In practice one can initialize with a publicly available preference history 5 Table 1 Latent user preference design specific to the document source Doc Source Latent User Preference Scenario Summarization News article See et al 2017 targeted to young children storytelling short sentences playful language interactive positiveintroduce a political news to kids Reddit post Stiennon et al 2020 second person narrative brief show emotions invoke personal reflection immersivefor character development in cre ative writing Wikipedia page Foundation 2022 bullet points parallel structure brief take notes for key knowledge Paper abstract Clement et al 2019 tweet style simple English inquisitive skillful foreshadowing with emojispromote a paper to invoke more attention and interests Movie review Maas et al 2011 question answering style direct concise quickly get main opinions Email Writing Personal problem Stiennon et al 2020 informal conversational short no closing share life with friends Paper review Hua et al 2019 casual tone positive clear call to action peer review to colleague Paper tweet Bar 2022 engaging personalized professional tone thank ful closingnetworking emails for researchers Paper summary Kershaw Koeling 2020 structured straight to the points respectful pro fessional greeting and closingmilestone report to superiors 4 1 Two Interactive Writing Assistant Environments for Learning from User Edits Task We introduce two tasks inspired by the use of LLMs as writing assistants Mysore et al 2023 Shen et al 2023 Wang et al 2023 In the first task we evaluate the agent s ability to summarize a given document We use documents from 5 existing sources listed in Table 1 4These sources represent a diverse category of documents that a writing assistant would typically encounter including news articles that are formal and concise movie reviews that are informal and paper abstracts that are tech nical In the second task we evaluate the agent s ability to compose an email given notes For this task we use notes from four different sources including a variety of tasks such as writing emails to friends describing reports to managers and writing reviews for colleagues In any given round the user is pro vided a context that is a document from one of the document sources for the given task Importantly the agent is unaware of the source of the given document which as we discuss later will determine the user preference For both tasks we run an experiment for T 200 rounds with an equal number of randomly sampled documents from each document source We mix documents from different sources and shuffle them to remove any temporal correlation in document source across rounds Two Stage GPT 4 Simulated User We simulate a user that can edit a given response We define a set of latent user preferences for the user that vary based on the document source Table 1 lists the preference and the corresponding document source This captures the context dependent nature of user preferences as the document source influences the type of context For example the Personal problem document source contains documents pertaining to discussions with a friend and a user may have a different preference when writing an email to a friend compared to writing an email to a colleague In real world settings the context dependence of the user preference can be more complex than just the document source We assume that our user is aware of the document source dtof a given context xt This implies that we can express the true user preference for xtasf t F dt where F maps a given document source to the user preference Recall that the agent in our learning setup is never provided the document source of any context We model our user using GPT 4 with a two stage approach Given an agent response ytand the context xt we first query GPT 4 to check if the given response satisfies the preference in f t If the answer is yes then the user preforms no edits and returns y t yt If the answer is no then we use GPT 4 to generate the edited response y tgiven ytandf t We use prompting to condition GPT 4 on these latent preferences We provide examples of edits made by our GPT 4 user in Table 5 in Appendix A 4Table 4 in Appendix provides links to each source dataset used as user provided context in our tasks 6 We found that our two stage GPT 4 user can generate high quality edits consistent with observations in prior work that LLM written feedback is high quality and useful to learn from Bai et al 2022 Saunders et al 2022 We adopted a two stage process since we found that using GPT 4 to directly edit the response ytalways resulted in edits even when the response satisfied the preference f t We evaluated several different prompts for modeling our two stage GPT 4 user until we found a prompt such that an oracle GPT 4 agent with access to f tachieves a minimal user cost Evaluation Metric We propose three metrics for evaluating agents learning from user edits Our main metric is the cumulative user edit costPT t 1ctoverTrounds In any given round we compute the user edit cost ct edit yt y t using Levenshtein edit distance between agent response ytand user edits y t To compute the edit distance we perform BPE tokenization using Tiktoken tokenizer and compute the edit distance in the token space In general one can learn a metric that better captures the cognitive load associated with a user edit However Levenshtein edit distance provides a clean transparent metric that is easy to interpret Additionally it doesn t have concerns shared by learned metrics such as erroneous evaluations when applying the metric to examples not covered by the metric s training distribution For CIPHER and any other method in the PRELUDE framework we additionally evaluate the accuracy of the inferred user preference ftused to generate the response yt Formally given a context xtcontaining a document from source dt we evaluate if the inferred preference ftis closer to the true preference f t F dt than preference F d of any other document source d dt Let there be Ndocument sources for a given task and we index d 1 2 N Then we compute this metric as1 TPT t 11 dt arg max d N BERTScore ft F d where BERTScore Zhang et al 2020 is a popular text similarity metric 5 Finally we evaluate the token expense associated with querying the LLM across all methods We compute the total number of tokens both generated by or provided as input to the LLM across all rounds This is a typical metric used by popular LLM providers to charge their customers 4 2 Details of CIPHER and Comparison Systems We use GPT 4 as our base LLM for CIPHER and all baselines We do not perform fine tuning of the GPT 4 and do not add any additional parameters to the model We use a prompt based GPT 4 agent for all methods that uses a single prompt with greedy decoding to generate the response Our main method CIPHER and the baselines can be extended to more complex language agents that perform multiple steps of reasoning on top of the base LLM before generating a response CIPHER Details We use a simple agent that uses GPT 4 with a prompt template to generate the response ytgiven the context xtand preference ft We list templates in Table 7 in Appendix A We experiment with MPNET Song et al 2020 and BERT Devlin et al 2019 as our two context representation functions œï and use cosine similarity for retrieval We experiment with two different values of the number of retrieved examples k 1 5 Baselines We evaluate CIPHER against baselines that either perform no learning or learn context agnostic preferences and against methods that do not learn preferences but directly use past user edits for generating a response 1 No learning The agent performs no learning based on interaction with the user In each step the agent generates a response ytgiven the context xt 2 Explore then exploit E then e LPI This baseline is based on the classic explore then exploit strategy in interactive learning Garivier et al 2016 The agent first generates responses for the first Terounds without performing any learning exploration stage It then infers a single user preference feusing the user edits in the first Terounds using the LPI step similar to line 12 in CIPHER Algorithm 1 It then uses the learned preference to generate the response for all remaining rounds exploitation step 3 Continual LPI This method is similar to explore then exploit except that it never stops exploring In any given round t it uses the data of all past edits yi y i t 1 i 1to learn a 5We use the microsoft deberta xlarge mnli to implement BERTScore 7 preference ftby performing the LPI step It then generates a response using this preference In contrast to explore then exploit approach Continual LPI can avoid overfitting to the first Terounds but both approaches learn preferences that are independent of xt 4 ICL edit This is a standard retrieval based in context learning ICL baseline Brown et al 2020 In a given round t the agent first retrieves the closest kexamples yz‚Ñì y z‚Ñì k ‚Ñì 1 to the given context xtusing the representation function œï It then creates an ICL prompt containing these kexamples where yz‚Ñìis presented as the input and y z‚Ñìis presented as the desired output The agent then uses the context xtand the ICL prompt to generate the response This approach doesn t infer preferences but must instead use the user edit data directly to align to the given user preference However unlike explore then exploit LPI and Continual LPI this approach can perform context dependent learning as the generated response attends on both the given context xtand the historical data Baseline Hyperparameters Forexplore then exploit LPI andcontinual LPI baselines we set the number of exploration Teas 5 For ICL edit baselines we experiment with different kvalues for retrieval and report our best results with k 5 Oracle Method We additionally run an oracle preference method to provide an approximated upper bound on performance In each round t we let the GPT 4 agent generate a response by conditioning on the ground truth latent preference f tand the context xt This method can test whether our setup is well defined e g in a poorly designed setup the user always edits the agent response no matter what the agent generates including providing user edits back to the user and thus no method can effectively minimize the cost over time in this case If the oracle method achieves a zero or a minimal user edit cost then learning the optimal preference leads to success 4 3 Main Result and Discussion Main Results Table 2 reports the performance of baselines and our methods on summarization and email writing tasks on three metrics edit distance which measures cumulative user edit cost accuracy which measures mean preference classification accuracy and expense measuring the total BPE token cost of querying LLM 6We report the mean and standard deviation across 3 different random seeds 7 Table 2 Performance of baselines and our methods in terms of cumulative edit distance cost and classification accuracy ¬µœÉdenotes the mean ¬µand standard deviation œÉacross 3 runs over different seeds Expense column shows budget as the average number of input and output BPE tokens across 3 runs unit is 105 We use kin method names to denote that we use kretrieved examples Numbers in bold are the best performance in each column excluding oracle preference method underline for the second best and dotted underline for the third best Method Summarization Email Writing Edit Distance Accuracy Expense Edit Distance Accuracy Expense Oracle Preference 6 573 1 451 1 000 1 67 1 851 243 1 000 1 62 No Learning 48 269 957 1 50 31 103 900 1 65 E then e LPI 65 218 17 466 0 218 0 003 1 99 24 562 1 022 0 263 0 003 1 73 Continual LPI 57 915 2 210 0 233 0 010 8 89 26 852 1 464 0 243 0 019 8 63 ICL edit 5 MPNET 38 560 1 044 8 00 32 405 1 307 12 12 ICL edit 5 BERT 39 734 1 929 7 96 30 949 3 250 11 55 CIPHER 1 MPNET 33 926 4 000 0 520 0 022 2 74 10 781 1 711 0 435 0 084 1 94 CIPHER 5 MPNET 32 974 195 0 478 0 010 3 00 10 058 1 709 0 467 0 081 2 09 CIPHER 1 BERT 37 637 3 025 0 565 0 053 2 81 12 634 4 868 0 487 0 125 1 99 CIPHER 5 BERT 35 811 3 384 0 478 0 028 3 03 8 391 3 038 0 363 0 075 2 22 6Table 9 in Appendix shows the breakdown of expense in terms of input and output 7We randomize the context sampling from source datasets so experiments on different seeds contain different sets of input contexts On the same seed experiments across different methods are strictly comparable as both the set of input contexts and the order of input context seen are the same in our implementation 8 Figure 2 Learning curves of different methods based on cumulative cost over time average across 3 seeds In the legend kmeans with top kretrieved examples Bfor BERT and Mfor MPNET 0 40 80 120 160 2001234567 104 RoundCumulative CostSummarization Oracle No Learning E then e Continual ICL edit B ICL edit M CIPHER 1 B CIPHER 5 B CIPHER 1 M CIPHER 5 M 0 40 80 120 160 2000 511 522 533 5 104 RoundCumulative CostEmail Writing Oracle No Learning E then e Continual ICL edit B ICL edit M CIPHER 1 B CIPHER 5 B CIPHER 1 M CIPHER 5 M Discussion of Main Result We observe that not performing learning results in a high edit cost whereas using the Oracle preferences achieves a significantly smaller edit cost This shows that our environments are sound and well conditioned E then e LPI and Continual LPI learn context agnostic preferences which cannot capture the context dependent preferences in the environments and end up doing poorly For the summarization task they end up with a higher edit distance than even performing no learning One explanation is that using context agnostic preferences can push the model to specialize to a given preference much more than the base model resulting in more edits when that preference is incorrect We see this in preference accuracy which is low for both of these baselines and lower for the summarization task than the email writing task where they outperform no learning baselines Further Continual LPI has a higher expense cost due to constantly querying the LLM to infer the user preference ICL edit baselines perform significantly better on the summarization task However using a list of user edits in the prompt results in a higher token expense cost as the responses and their edits can be significantly long in practice Further the ICL edit baselines provide no interpretable explanation for their response or for explaining user behavior Finally CIPHER achieves the smallest edit distance cost reducing edits by 31 in the summarization task and 73 in the email writing task We observe that retrieving k 5preferences and aggregating them achieves lower edit distance however the choice of ideal representation œïseems task dependent Further CIPHER achieves the highest preference accuracy showing that CIPHER can learn pref erences that correlate more with the ground truth preference than preferences of other document sources Note that the performance of a random preference classifier is only 20 for summarization and 25 for email writing Further CIPHER achieves a smaller cost than ICL edit and Continual LPI baselines as it doesn t use long user edits in the prompt for generating a response Overall CIPHER provides a cheap more effective and interpretable method than our baselines 4 4 More Analysis Learning Curves We plot mean cumulative user edit costs over rounds in Figure 2 The cumulative user edit costs in Figure 2 show that the angle of the learning curves decreases for CIPHER after an initial number of rounds showing that learning helps decrease the rate at which user edits are accu mulated In contrast the angle of the learning curve for the no learning baseline remains unchanged 9 Figure 3 Normalized cost and percentage of zero cost examples of CIPHER over time binned per 20 rounds to show the trend average across 3 seeds In the legend kmeans with top kretrieved examples Bfor BERT and Mfor MPNET 40 80 120 160 20000 20 40 6 RoundNormalized CostSummarizationCIPHER 5 B CIPHER 1 M CIPHER 5 M Oracle 40 80 120 160 20000 20 4 RoundNormalized CostEmail Writing 40 80 120 160 20000 20 40 60 8 Round Zero cost Ex per Bin 40 80 120 160 2000 20 40 6 Round Zero cost Ex per Bin Evaluating Normalized Edit Cost The cumulative user edit cost measures the total effort of the user but is susceptible to outlier examples as the edit distance for a given round is potentially unbounded Therefore we also compute a normalized edit distance edit yt y t yt by dividing the edit distance by max yt y t i e the max length of the agent output or user revised text As Levenshtein distance edit yt y t is upper bounded by max yt y t therefore the normalized cost is at most 1 Figure 3 reports normalized cost over rounds for the top 3 methods We notice that for all variants of CIPHER for the summarization task and for CIPHER 5 M for the email writing task the normalized cost decreases notably as training progresses indicating learning As the cost is normalized by the response length even a small decrease can lead to a significant reduction in the number of tokens edited Evaluating Fraction of Edited Response Recall that the first stage of our GPT 4 user checks if the agent response satisfies the latent user preference f If it does then the user performs no edits Otherwise in the second stage the user edits the response To measure how many times the agent response isn t edited we also plot the percentage of examples with zero edit cost per 20 rounds bin in Figure 3 We notice a small increase in the number of examples with zero edit cost This indicates that gains come from reducing edits across all examples and not just by increasing the number of examples that avoid getting edited in stage 1 of our user Qualitative Analysis of Learned Preferences We qualitatively analyze the learned preferences for CIPHER to understand the quality of learned preferences We present our analysis on the summarization task where our methods have a larger gap with the oracle performance compared to the email writing task Table 3 lists 3 learned preferences per document source for CIPHER 5 MPNET which are randomly sampled at the beginning middle and end of the interaction history We see that overall the agent can learn a reasonable description of the latent preference For example it can learn bullet points preference for Wikipedia articles and second person narrative for Reddit posts and QA style for Movie reviews CIPHER can pick some preferences fairly early such as bullet points for Wikipedia and emojis for Paper abstract whereas some are learned only later such as Structured Q Afor Movie reviews This shows using CIPHER can quickly learn useful preferences but further interaction continues to help Failure Cases CIPHER notably reduces the edit cost and learns useful preference however significant gaps to the oracle method remain especially in the summarization task We manually analyze failure cases on summarization task with the best performing method CIPHER 5 MPNET Table 10 in the Appendix reports the summary and example of our findings categorized as preference 10 Table 3 Examples of learned preferences on summarization task with CIPHER 5 MPNET grouped based on the document source and corresponding latent preference We randomly sample 3 examples per type at the beginning middle and end of the interaction history Latent User Preference Round Learned Preference News article targeted to young children storytelling short sentences playful lan guage interactive positive 22 Fairy tale narrative style informal and conversational tone use of rhetorical questions simplified language 115 Simplified childlike storytelling with playful language and imagery 192 Simplified and playful storytelling language Reddit post second person narrative brief show emo tions invoke personal reflec tion immersive 14 Concise and coherent storytelling 102 The user prefers a second person narrative and a more direct personal tone 194 Poetic and descriptive language narrative perspective shift to second person Wikipedia page bullet points parallel structure brief 19 Concise Bullet Pointed Structured Summaries with a Narrative Q A Style 124 Concise and factual writing style bullet point formatting 197 Concise and streamlined formatting with bullet points and clear subhead ings for easy scanning Paper abstract tweet style simple English inquisitive skillful foreshadowing with emojis 20 Concise conversational summaries with bullet points and emojis 111 Concise conversational whimsical bullet point summaries with emojis 193 Concise conversational and whimsical bullet point summaries with emojis Movie review question an swering style 12 The user prefers a straightforward clear and concise writing style with factual formatting 123 The user prefers a clear and concise question and answer format with straightforward language 199 Concise Structured Q A with Whimsical Clarity inference from output revision pair consolidation of inferred preferences and retrieval 8In brief the most common type of failure is on the preference inference step given the agent output and user revision For example the agent often misses the exact keyword for brief orshort sentences and sometimes struggles with inferring the second person narrative aspect 5 Related Work We describe related work in this area grouped by main themes in this work Learning from Feedback Besides pair wise comparison feedback from annotators used in Rein forcement Learning from Human Feedback RLHF research Ziegler et al 2019 Stiennon et al 2020 Nakano et al 2021 Ouyang et al 2022a inter alia prior work has also studied free form text feedback provided by annotators Fernandes et al 2023 such as on the task of dialog We ston 2016 Li et al 2016 Hancock et al 2019 Xu et al 2022 Petrak et al 2023 question answering Li et al 2022 Malaviya et al 2023 summarization Saunders et al 2022 and general decision making Cheng et al 2023 This feedback tailored to each example is often utilized to rank candidate outputs thereby improving task performance Some work studies learning from text feedback to generate outputs directly Scheurer et al 2023 Bai et al 2022 Shi et al 2022 by generating multiple refinements of the original output based on the feedback and fine tuning the original model to maximize the likelihood of the best refinement In grounded settings such as instruction based navigation one line of work has also used hindsight feedback that explicitly provides a text instruction for the generated trajectory to train policies Nguyen et al 2021 Misra et al 2024 Moving beyond the conventional focus on text feedback that explicitly articulates human intent we investigate feedback in the form of direct edits on the original model output Such revisions by users occur naturally during model deployment in practice Additionally we examine the learning of user preferences through historical interactions aiming to surpass the constraints of example specific feedback 8We provide additional analysis on the accuracy of retrieval in Table 11 11 Language Agents and Personalization LLMs have enabled the development of language agents for a variety of tasks from writing assistants Lee et al 2024 coding assistants Dohmke 2022 and customer service assistants Brynjolfsson et al 2023 Since these LLM based assistants are often used by individuals a natural question has arisen on how to personalize these agents for each user Straightforward approaches for fine tuning LLMs includes supervised learning online DPO Guo et al 2024 learning to search Chang et al 2023 and reinforcement learning Ouyang et al 2022b These approaches can be directly applied to our setting For example one can use yt y t in Protocol 1 as the preference data where y tis preferred over yt or use y tas the ground truth for supervised learning However fine tuning is expensive and hard to scale with the number of users Therefore a line of work has explored improving the alignment of frozen LLMs by prompt engineering such as learning a personalized retrieval model Mysore et al 2023 learning a prompt policy given a reward function Deng et al 2022 or more generally learning to rewrite the entire prompt Li et al 2023 We focus on learning a prompt policy by learning from user edits and specifically using them to extract textural descriptions of user preference Edits and Revisions Many prior work on editing model output focuses on error correction such as fixing source code Yin et al 2018 Chen et al 2018 Reid et al 2023 and improving the factual consistency of model summaries Cao et al 2020 Liu et al 2022 Balachandran et al 2022 A line of work has explored understanding human edits based on edit history of Wikipedia Botha et al 2018 Faltings et al 2020 Rajagopal et al 2022 Reid Neubig 2022 Laban et al 2023 or revisions of academic writings Mita et al 2022 Du et al 2022 D Arcy et al 2023 Prior work explores predicting text revisions with edit intents Brody et al 2020 Kim et al 2022 Chong et al 2023 and modeling edits with various approaches including latent vectors Guu et al 2017 Marrese Taylor et al 2020 2023 structured trees Yao et al 2021 discrete diffusion process Reid et al 2023 or a series of singular edit operations Stahlberg Kumar 2020 Mallinson et al 2020 Agrawal Carpuat 2022 Zhang et al 2022 Liu et al 2023 However these methodologies predominantly target generic improvements in model performance overlooking the intricacies of individual user satisfaction and preference Our research takes a distinct direction focusing on understanding edits across a variety of examples to study user level preferences with a practical goal of aligning the agent to individual preferences 6 Conclusion We study aligning LLM based agents using user edits that arise naturally in applications such as writing assistants We conjecture that user edits are driven by a latent user preference that can be captured by textual descriptions We introduce the PRELUDE framework that focuses on learning descriptions of user preferences from user edit data and then generating an agent response accordingly We propose a simple yet effective retrieval based algorithm CIPHER that infers user preference by querying the LLM retrieves relevant examples in the history and aggregates induced preferences in retrieved examples to generate a response for the given context We introduce two interactive environments with a GPT 4 simulated user to study learning from edits which can be of independent interest In this work we focus on aligning an LLM agent with a frozen LLM in part due to the challenge of scaling fine tuning based approaches with the number of users However for settings where computational cost is not a barrier applying fine tuning approaches would be an interesting future work direction Another promising future work direction is to learn user preference based on different levels of edits words sentences paragraphs to generate a satisfactory response Acknowledgments Gao was a research intern in MSR NYC and later was partially supported by NSF project 1901030 All content represents the opinion of the authors which is not necessarily shared or endorsed by their respective employers and or sponsors We thank MSR NYC research community Jonathan D Chang Daniel D Lee Claire Cardie and Sasha Rush for helpful discussions and support References Josh Achiam Steven Adler Sandhini Agarwal Lama Ahmad Ilge Akkaya Florencia Leoni Aleman Diogo Almeida Janko Altenschmidt Sam Altman Shyamal Anadkat et al Gpt 4 technical report arXiv preprint arXiv 2303 08774 2023 12 Sweta Agrawal and Marine Carpuat An imitation learning curriculum for text editing with non autoregressive models ArXiv abs 2203 09486 2022 Yuntao Bai Saurav Kadavath Sandipan Kundu Amanda Askell Jackson Kernion Andy Jones Anna Chen Anna Goldie Azalia Mirhoseini Cameron McKinnon Carol Chen Catherine Olsson Christopher Olah Danny Hernandez Dawn Drain Deep Ganguli Dustin Li Eli Tran Johnson Ethan Perez Jamie Kerr Jared Mueller Jeffrey Ladish Joshua Landau Kamal Ndousse Kamile Lukosuite Liane Lovitt Michael Sellitto Nelson Elhage Nicholas Schiefer Noemi Mercado Nova DasSarma Robert Lasenby Robin Larson Sam Ringer Scott Johnston Shauna Kravec Sheer El Showk Stanislav Fort Tamera Lanham Timothy Telleen Lawton Tom Conerly Tom Henighan Tristan Hume Samuel R Bowman Zac Hatfield Dodds Ben Mann Dario Amodei Nicholas Joseph Sam McCandlish Tom Brown and Jared Kaplan Constitutional ai Harmlessness from ai feedback 2022 Vidhisha Balachandran Hannaneh Hajishirzi William Cohen and Yulia Tsvetkov Correcting diverse factual errors in abstractive summarization via post editing and language model infilling ArXiv abs 2210 12378 2022 Nitsan Bar Papertweet https github com bnitsan PaperTweet 2022 Jan A Botha Manaal Faruqui John Alex Jason Baldridge and Dipanjan Das Learning to split and rephrase from wikipedia edit history ArXiv abs 1808 09468 2018 Shaked Brody Uri Alon and Eran Yahav A structural model for contextual code changes Proceed ings of the ACM on Programming Languages 4 1 28 2020 Tom Brown Benjamin Mann Nick Ryder Melanie Subbiah Jared D Kaplan Prafulla Dhariwal Arvind Neelakantan Pranav Shyam Girish Sastry Amanda Askell et al Language models are few shot learners Advances in neural information processing systems 33 1877 1901 2020 Erik Brynjolfsson Danielle Li and Lindsey R Raymond Generative ai at work Technical report National Bureau of Economic Research 2023 Mengyao Cao Yue Dong Jiapeng Wu and Jackie Chi Kit Cheung Factual error correction for abstractive summarization models ArXiv abs 2010 08712 2020 Jonathan D Chang Kiante Brantley Rajkumar Ramamurthy Dipendra Misra and Wen Sun Learning to generate better than your llm arXiv preprint arXiv 2306 11816 2023 Zimin Chen Steve Kommrusch Michele Tufano Louis No√´l Pouchet Denys Poshyvanyk and Monperrus Martin Sequencer Sequence to sequence learning for end to end program repair IEEE Transactions on Software Engineering 47 1943 1959 2018 Ching An Cheng Andrey Kolobov Dipendra Misra Allen Nie and Adith Swaminathan Llf bench Benchmark for interactive learning from language feedback arXiv preprint arXiv 2312 06853 2023 Ruining Chong Cunliang Kong Liu Wu Zhenghao Liu Ziye Jin Liner Yang Yange Fan Hanghang Fan and Erhong Yang Leveraging prefix transfer for multi intent text revision Annual Meeting of the Association for Computational Linguistics 2023 Colin B Clement Matthew Bierbaum Kevin P O Keeffe and Alexander A Alemi On the use of arxiv as a dataset 2019 Mike D Arcy Alexis Ross Erin Bransom Bailey Kuehl Jonathan Bragg Tom Hope and Doug Downey Aries A corpus of scientific paper edits made in response to peer reviews ArXiv abs 2306 12587 2023 Mingkai Deng Jianyu Wang Cheng Ping Hsieh Yihan Wang Han Guo Tianmin Shu Meng Song Eric P Xing and Zhiting Hu Rlprompt Optimizing discrete text prompts with reinforcement learning arXiv preprint arXiv 2205 12548 2022 Jacob Devlin Ming Wei Chang Kenton Lee and Kristina Toutanova Bert Pre training of deep bidirectional transformers for language understanding North American Chapter of the Association for Computational Linguistics 2019 13 Thomas Dohmke Github copilot is generally available to all developers https github blog 2022 06 21 github copilot is generally available to all developers 2022 Accessed April 20 2024 Wanyu Du Vipul Raheja Dhruv Kumar Zae Myung Kim Melissa Lopez and Dongyeop Kang Understanding iterative revision from human written text ArXiv abs 2203 03802 2022 Felix Faltings Michel Galley Gerold Hintz Chris Brockett Chris Quirk Jianfeng Gao and Bill Dolan Text editing by command ArXiv abs 2010 12826 2020 Patrick Fernandes Aman Madaan Emmy Liu Ant√≥nio Farinhas Pedro Henrique Martins Amanda Bertsch Jos√© G C de Souza Shuyan Zhou Tongshuang Sherry Wu Graham Neubig and Andr√© F T Martins Bridging the gap A survey on integrating human feedback for natural language generation ArXiv abs 2305 00955 2023 Wikimedia Foundation Wikimedia downloads https dumps wikimedia org 2022 Aur√©lien Garivier Tor Lattimore and Emilie Kaufmann On explore then commit strategies Ad vances in Neural Information Processing Systems 29 2016 Shangmin Guo Biao Zhang Tianlin Liu Tianqi Liu Misha Khalman Felipe Llinares Alexandre Rame Thomas Mesnard Yao Zhao Bilal Piot et al Direct language model alignment from online ai feedback arXiv preprint arXiv 2402 04792 2024 Kelvin Guu Tatsunori B Hashimoto Yonatan Oren and Percy Liang Generating sentences by editing prototypes Transactions of the Association for Computational Linguistics 6 437 450 2017 Braden Hancock Antoine Bordes Pierre Emmanuel Mazar√© and Jason Weston Learning from dialogue after deployment Feed yourself chatbot Annual Meeting of the Association for Computational Linguistics 2019 Xinyu Hua Mitko Nikolov Nikhil Badugu and Lu Wang Argument mining for understanding peer reviews Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies Volume 1 Long and Short Papers June 2019 Daniel James Kershaw and R Koeling Elsevier oa cc by corpus ArXiv abs 2008 00774 2020 doi https doi org 10 48550 arXiv 2008 00774 URL https elsevier digitalcommonsdata com datasets zm33cdndxs Zae Myung Kim Wanyu Du Vipul Raheja Dhruv Kumar and Dongyeop Kang Improving iterative text revision by learning where to edit from other revision tasks ArXiv abs 2212 01350 2022 Philippe Laban Jesse Vig Wojciech Kryscinski Shafiq R Joty Caiming Xiong and Chien Sheng Wu Swipe A dataset for document level simplification of wikipedia pages Annual Meeting of the Association for Computational Linguistics 2023 Mina Lee Katy Ilonka Gero John Joon Young Chung Simon Buckingham Shum Vipul Raheja Hua Shen Subhashini Venugopalan Thiemo Wambsganss David Zhou Emad A Alghamdi Tal August Avinash Bhat Madiha Zahrah Choksi Senjuti Dutta Jin L C Guo Md Naimul Hoque Yewon Kim Seyed Parsa Neshaei Agnia Sergeyuk Antonette Shibani Disha Shrivastava Lila Shroff Jessi Stark S Sterman Sitong Wang Antoine Bosselut Daniel Buschek Joseph Chee Chang Sherol Chen Max Kreminski Joonsuk Park Roy Pea Eugenia H Rho Shannon Zejiang Shen and Pao Siangliulue A design space for intelligent and interactive writing assistants Conference on Human Factors in Computing Systems abs 2403 14117 2024 Vladimir I Levenshtein Binary codes capable of correcting deletions insertions and reversals Soviet physics Doklady 10 707 710 1965 Cheng Li Mingyang Zhang Qiaozhu Mei Weize Kong and Michael Bendersky Automatic prompt rewriting for personalized text generation arXiv preprint arXiv 2310 00152 2023 14 Jiwei Li Alexander H Miller Sumit Chopra Marc Aurelio Ranzato and Jason Weston Dialogue learning with human in the loop ArXiv abs 1611 09823 2016 Zichao Li Prakhar Sharma Xing Han Lu Jackie Chi Kit Cheung and Siva Reddy Using in teractive feedback to improve the accuracy and explainability of question answering systems post deployment ArXiv abs 2204 03025 2022 Ruibo Liu Chenyan Jia Ge Zhang Ziyu Zhuang Tony X Liu and Soroush V osoughi Second thoughts are best Learning to re align with human values from text edits ArXiv abs 2301 00355 2023 Yixin Liu Budhaditya Deb Milagro Teruel Aaron L Halfaker Dragomir R Radev and Ahmed Has san Awadallah On improving summarization factual consistency from natural language feedback Annual Meeting of the Association for Computational Linguistics 2022 Andrew L Maas Raymond E Daly Peter T Pham Dan Huang Andrew Y Ng and Christopher Potts Learning word vectors for sentiment analysis Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics Human Language Technologies June 2011 Chaitanya Malaviya Subin Lee Dan Roth and Mark Yatskar Pachinko Patching interpretable qa models through natural language feedback ArXiv abs 2311 09558 2023 Jonathan Mallinson Aliaksei Severyn Eric Malmi and Guillermo Garrido Felix Flexible text editing through tagging and insertion ArXiv abs 2003 10687 2020 Edison Marrese Taylor Machel Reid and Yutaka Matsuo Variational inference for learning repre sentations of natural language edits ArXiv abs 2004 09143 2020 Edison Marrese Taylor Machel Reid and Alfredo Solano Edit aware representation learning via levenshtein prediction The Fourth Workshop on Insights from Negative Results in NLP 2023 Dipendra Misra Aldo Pacchiano and Robert E Schapire Provable interactive learning with hindsight instruction feedback arXiv preprint arXiv 2404 09123 2024 Masato Mita Keisuke Sakaguchi Masato Hagiwara Tomoya Mizumoto Jun Suzuki and Kentaro Inui Towards automated document revision Grammatical error correction fluency edits and beyond ArXiv abs 2205 11484 2022 Sheshera Mysore Zhuoran Lu Mengting Wan Longqi Yang Steve Menezes Tina Baghaee Em manuel Barajas Gonzalez Jennifer Neville and Tara Safavi Pearl Personalizing large language model writing assistants with generation calibrated retrievers arXiv preprint arXiv 2311 09180 2023 Reiichiro Nakano Jacob Hilton S Arun Balaji Jeff Wu Long Ouyang Christina Kim Christopher Hesse Shantanu Jain Vineet Kosaraju William Saunders Xu Jiang Karl Cobbe Tyna Eloundou Gretchen Krueger Kevin Button Matthew Knight Benjamin Chess and John Schulman Webgpt Browser assisted question answering with human feedback ArXiv 2021 Khanh X Nguyen Dipendra Misra Robert Schapire Miroslav Dud√≠k and Patrick Shafto Interactive learning from activity description International Conference on Machine Learning pp 8096 8108 2021 Long Ouyang Jeff Wu Xu Jiang Diogo Almeida Carroll L Wainwright Pamela Mishkin Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray John Schulman Jacob Hilton Fraser Kelton Luke E Miller Maddie Simens Amanda Askell Peter Welinder Paul Francis Christiano Jan Leike and Ryan J Lowe Training language models to follow instructions with human feedback ArXiv 2022a Long Ouyang Jeffrey Wu Xu Jiang Diogo Almeida Carroll Wainwright Pamela Mishkin Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray et al Training language models to follow instructions with human feedback Advances in neural information processing systems 35 27730 27744 2022b 15 Dominic Petrak Nafise Sadat Moosavi Ye Tian Nikolai Rozanov and Iryna Gurevych Learning from free text human feedback collect new datasets or extend existing ones ArXiv abs 2310 15758 2023 Dheeraj Rajagopal Xuchao Zhang Michael Gamon Sujay Kumar Jauhar Diyi Yang and Eduard H Hovy One document many revisions A dataset for classification and description of edit intents International Conference on Language Resources and Evaluation 2022 Machel Reid and Graham Neubig Learning to model editing processes Conference on Empirical Methods in Natural Language Processing 2022 Machel Reid Vincent J Hellendoorn and Graham Neubig Diffuser Diffusion via edit based reconstruction International Conference on Learning Representations 2023 William Saunders Catherine Yeh Jeff Wu Steven Bills Ouyang Long Jonathan Ward and Jan Leike Self critiquing models for assisting human evaluators ArXiv abs 2206 05802 2022 J er emy Scheurer Jon Ander Campos Tomasz Korbak Jun Shern Chan Angelica Chen Kyunghyun Cho and Ethan Perez Training language models with language feedback at scale ArXiv abs 2303 16755 2023 Abigail See Peter J Liu and Christopher D Manning Get to the point Summarization with pointer generator networks Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics Volume 1 Long Papers July 2017 Zejiang Shen Tal August Pao Siangliulue Kyle Lo Jonathan Bragg Jeff Hammerbacher Doug Downey Joseph Chee Chang and David Sontag Beyond summarization Designing ai support for real world expository writing tasks arXiv preprint arXiv 2304 02623 2023 Weiyan Shi Emily Dinan Kurt Shuster Jason Weston and Jing Xu When life gives you lemons make cherryade Converting feedback from bad responses into good labels ArXiv abs 2210 15893 2022 Kaitao Song Xu Tan Tao Qin Jianfeng Lu and Tie Yan Liu Mpnet Masked and permuted pre training for language understanding ArXiv abs 2004 09297 2020 Felix Stahlberg and Shankar Kumar Seq2edits Sequence transduction using span level edit opera tions Conference on Empirical Methods in Natural Language Processing 2020 Nisan Stiennon Long Ouyang Jeff Wu Daniel M Ziegler Ryan J Lowe Chelsea V oss Alec Radford Dario Amodei and Paul Christiano Learning to summarize from human feedback ArXiv abs 2009 01325 2020 Gemini Team Rohan Anil Sebastian Borgeaud Yonghui Wu Jean Baptiste Alayrac Jiahui Yu Radu Soricut Johan Schalkwyk Andrew M Dai Anja Hauth et al Gemini a family of highly capable multimodal models arXiv preprint arXiv 2312 11805 2023 Sitong Wang Lydia B Chilton and Jeffrey V Nickerson Writing with generative ai Multi modal and multi dimensional tools for journalists The Second Workshop on Intelligent and Interactive Writing Assistants at ACM CHI 2023 Jason Weston Dialog based language learning ArXiv abs 1604 06045 2016 Jing Xu Megan Ung Mojtaba Komeili Kushal Arora Y Lan Boureau and Jason Weston Learn ing new skills after deployment Improving open domain internet driven dialogue with human feedback Annual Meeting of the Association for Computational Linguistics 2022 Ziyu Yao Frank F Xu Pengcheng Yin Huan Sun and Graham Neubig Learning structural edits via incremental tree transformations ArXiv abs 2101 12087 2021 Pengcheng Yin Graham Neubig Miltiadis Allamanis Marc Brockschmidt and Alexander L Gaunt Learning to represent edits ArXiv abs 1810 13337 2018 16 Jiyang Zhang Sheena Panthaplackel Pengyu Nie Junyi Jessy Li and Milo≈° Gligori c Coditt5 Pretraining for source code and natural language editing Proceedings of the 37th IEEE ACM International Conference on Automated Software Engineering 2022 Tianyi Zhang Varsha Kishore Felix Wu Kilian Q Weinberger and Yoav Artzi Bertscore Evaluating text generation with bert International Conference on Learning Representations 2020 Daniel M Ziegler Nisan Stiennon Jeff Wu Tom B Brown Alec Radford Dario Amodei Paul Christiano and Geoffrey Irving Fine tuning language models from human preferences ArXiv 2019 17 Appendix A Additional Details Dataset Examples We list links to dataset sources for our user provided context in Table 4 GPT 4 User s Edits We list examples of OUR GPT 4 user s edits with different latent preference on summarization in Table 5 GPT 4 User Template Prompt templates used by our GPT 4 user are provided in Table 6 CIPHER Templates Prompt templates used by CIPHER are provided in Table 7 ICL edit Templates Prompt templates used by ICL edit baseline are provided in Table 8 B Additional Analysis Detailed Expense Analysis We list a detailed computational expense of different methods in Ta ble 9 Failure Cases We summarize our failure case analysis of CIPHER on summarization in Table 10 Retrieval Accuracy We calculate retrieval accuracy for CIPHER as the fraction of all retrieved contexts that are of the same document type as the currently given context across all seeds and time steps We report the results in Table 11 We find that the retrieval accuracy is higher on the summarization task than on email writing and using MPNET typically performs better than using Bert to encode context Table 4 Link to each source dataset from which we randomly sample examples as the user provided context in our tasks Data Source Link and Example CNN Daily Mail See et al 2017 https huggingface co datasets cnn_dailymail SLF5K Stiennon et al 2020 https huggingface co datasets JeremyAlain SLF5K Wikidump Foundation 2022 https huggingface co datasets wikipedia Arxiv Clement et al 2019 https huggingface co datasets CShorten ML ArXiv Papers IMDb Maas et al 2011 https huggingface co datasets imdb PaperTweet Bar 2022 https huggingface co datasets nitsanb paper_tweet Elsevier OA CC By Kershaw Koeling 2020 https huggingface co datasets orieg elsevier oa cc by 18 Table 5 Examples of user edits with different latent preference on summarization The agent output column shows the agent generation without conditioning on any preference The latent preference for the user edits is in bold and parenthesis We truncate some examples due to limited space Agent Output User Edits Leon Fenner the suspected gunman in a 2 million diamond heist at Watch Standard Jewelry in New York City was arrested in Maryland The robbery occurred on November 11 and resulted in the in jury of the store owner s elderly father Fenner s accomplice Rondu Frisby was a friend of the store owner and was apprehended a week after the heist The robbery took place during the city s Veteran s Day Parade leading to a block shutdown Fenner and Frisby stole numerous jewels and 20 luxury watches but were caught on surveillance cameras Fenner is set to appear before a federal court in Maryland targeted to young children storytelling short sentences playful language interactive posi tive Once upon a time in the big bustling city of New York during a parade filled with music and march ing on Veteran s Day something sneaky happened at a place called Watch Standard Jewelry This wasn t just any store it was a treasure chest full of sparkling diamonds and shiny watches that could make your eyes twinkle like stars Now two not so nice men Leon Fenner and his buddy Rondu Frisby thought they could be sneaky and take these treasures without asking On November 11 while everyone was watching the parade they tiptoed into the store But oh no They accidentally hurt the store owner s dear old dad which wasn t very kind at all TRUNCATED The article describes a situation where a neigh bor s cat one of seven rescued by a cat lady has started frequenting the author s front yard after leaving its original home due to the presence of other cats Over the past few months the cat has become more consistent in its visits meowing at the author and their family members for attention The cat allows itself to be petted and shows af fectionate behavior such as rubbing against legs and headbutting hands until it is fed After eating the cat typically leaves but returns in the evening The author is seeking advice on how to handle the situation expressing concern about feeding the cat because it is not legally theirs and there could be potential ownership and legal issues second person narrative brief show emotions invoke personal reflection immersive Imagine yourself in a quaint neighborhood where the gentle purrs of a feline friend have become a familiar part of your daily routine This isn t just any cat it s one of seven a charming escapee from the home of a local cat lady seeking solace in the tranquility of your front yard Over recent months this whiskered wanderer has turned your doorstep into its personal stage meowing for attention with a persistence that tugs at your heartstrings Each encounter with this affectionate creature is a dance of mutual admiration You find yourself captivated by its soft fur and the way it eagerly rubs against your legs headbutting your hands in a silent plea for affection TRUNCATED The Puerto Plata Province is a northern province in the Dominican Republic known for its beautiful beaches and as a popular tourist destination since the late 1990s Its capital is San Felipe de Puerto Plata named after the silver clouds Columbus saw on a mountain nearby in 1493 The city of Puerto Plata was founded in 1502 Historically the first European town in the Ameri cas La Isabela was established by Columbus near Puerto Plata in 1493 In 1605 the governor An tonio de Osorio destroyed Puerto Plata and other towns to curb illegal trade leading to the formation of new settlements Puerto Plata became a Maritime District in 1865 and was later recognized as a province in the 1907 constitution Over time it expanded to include more municipalities and municipal districts with the latest additions in 2007 TRUNCATED bullet points parallel structure brief Location Northern Dominican Republic Capital San Felipe de Puerto Plata Founding 1502 by Europeans Historical site La Isabela 1493 first European town in the Americas 1605 Destruction by Governor Antonio de Oso rio to stop illegal trade Status Became Maritime District in 1865 recog nized as province in 1907 Expansion Added more municipalities latest in 2007 Borders Atlantic Ocean Espaillat Santiago Valverde Monte Cristi provinces TRUNCATED 19 The article discusses the challenge of detecting and classifying stuttered speech which affects many people but has been subject to limited research The authors present a new model that differs from most existing methods which typically use lan guage models for stutter identification Instead their model is based on acoustic features which allows it to identify various kinds of stuttering dis fluencies without relying on speech recognition technology The proposed model combines a deep residual network with bidirectional long short term memory LSTM layers to classify different stutter types It has been shown to significantly outper form current leading methods reducing the aver age miss rate to 10 03 which is a 27 improve ment over the state of the art tweet style simple English inquisitive skillful foreshadowing with emojis Ever struggled with stuttering or know someone who does Research on detecting stutters hasn t been huge but guess what A new study is changing the game Instead of just looking at words this smart model listens to how things are said It s like a detective for sounds using cool tech called deep learning and memory networks to spot different stutter types And it s crushing it beating old methods by 27 Missed stutters Down to just 10 This could be a big deal for speech therapy Stuttering SpeechTech Innovation The article discusses a movie that begins promis ingly but later becomes confusing due to a poorly depicted portrayal of ancient black magic rituals The male lead delivers a strong performance de spite his character s unfavorable outcome in the climax The movie is compared to a mix of Boomerang and Extremities question answering style direct concise What is the general sentiment of the article about the movie The article suggests that the movie starts off well but eventually becomes muddled The depiction of ancient black magic rituals is criticized for being confusing How does the article describe the male lead s per formance The male lead s performance is described as strong even though his character does not have a favorable outcome in the climax What films is the movie compared to in the article The movie is compared to Boomerang and Extremities in the article Table 6 Prompt templates for the AI user The first step is to prompt the user for yes no answer regarding satisfaction If the answer is no the second step is to ask the user edit the agent output according to the latent preference If the answer is yes the agent output receives 0 edits Summarization Email Writing Step 1 Article user provided article Summary agent generated summary Is the above summary of the above article good for person who would love to use the following style latent user preference Please answer yes or no Notes user provided notes Email agent generated email Is the above email based on the above notes good for a user who wants the following style latent user preference Please answer yes or no Step 2 Summary agent generated summary Please revise the above summary of an article to meet your style latent user preference Email agent generated email Assume that you prefer latent user preference Please revise the above email to meet your style 20 Table 7 Prompt templates for CIPHER Summarization Email Writing Task prompt conditioned on inferred preference line 6 in Al gorithm 1 Article user provided article Assume that you need to summarize the above article for a user who prefers the following style inferred user preference Please write a summary of the above article to address those specified preferences Notes user provided notes These notes are written by a user who prefers the following style of emails inferred user preference Please write a short email based on the above notes to address those specified preferences Prompt to in fer user pref erence based on revision line 12 in Al gorithm 1 Original summary of an article agent generated summary Revised summary by a user user revision Based on the edits and revision by this user on the original summary in the above examples what do you find about this user s generic pref erence in terms of writing style and formatting Please answer in a short phrase and only recom mend those preferences that are widely used Original email agent generated email Revised email user revision Based on the edits and revision by this user on the original email in the above examples what do you find about this user s generic preference in terms of writing style and formatting Please answer in a short phrase and only recommend those preferences that are widely used Prompt to consolidate inferred preferences from history line 5 in Al gorithm 1 List of user preferences successfully being used to generate summaries of similar documents inferred preference in a retrieved example inferred preference in a retrieved example Based on the the above examples please come up with short phrase with the most represented summarization preferences of the user List of user preferences successfully being used to generate emails of a similar kind inferred preference in a retrieved example inferred preference in a retrieved example Based on the the above examples please come up with short phrase with the most represented writing preferences of this user Table 8 Prompt templates for the ICL edit baseline Summarization Email Writing Prompt with retrieved user edit examplesOriginal summary of an article agent generated summary in a retrieved example Revised summary by a user user revision in a retrieved example Original summary of an article agent generated summary in a retrieved example Revised summary by a user user revision in a retrieved example Article user provided article Based on the edits and revision by this user on the original summary in the above examples Please summarize the above article Original summary of an article agent generated summary in a retrieved example Revised summary by a user user revision in a retrieved example Original summary of an article agent generated summary in a retrieved example Revised summary by a user user revision in a retrieved example Notes user provided notes Based on the edits and revision by this user on the original email in the above examples Please write an email based on the above notes for this user Table 9 Expense of different methods number of BPE tokens in terms of input output and total Each number is the average across 3 runs unit is 105 Method Summarization Email Writing Input Output Total Input Output Total Oracle Preference 1 14 0 53 1 67 0 91 0 71 1 62 No Learning 1 06 0 44 1 50 0 85 0 80 1 65 E then e LPI 1 16 0 83 1 99 0 94 0 79 1 73 Continual LPI 8 14 0 75 8 89 7 89 0 73 8 63 ICL edit 5 MPNET 7 35 0 65 8 00 11 05 1 06 12 12 ICL edit 5 BERT 7 32 0 64 7 96 10 51 1 03 11 55 CIPHER 1 MPNET 2 02 0 72 2 74 1 21 0 73 1 94 CIPHER 5 MPNET 2 27 0 73 3 00 1 44 0 64 2 09 CIPHER 1 BERT 2 10 0 71 2 81 1 27 0 73 1 99 CIPHER 5 BERT 2 32 0 71 3 03 1 48 0 73 2 22 21 Table 10 Summary of failure cases on summarization task with CIPHER 5 MPNET Type of Failures Summary Examples Preference inference based on an output revision pair ft the most common fail ure type 1 Not totally wrong but in sufficient i e the inferred preference only captures a few aspects of user s latent preference This is most common for news articles and Reddit posts for which the user shows nuanced pref erence for several aspects The dominant missing aspect is brief orshort sen tences across different context although the agent can infer keywords such as concise For news article context the agent tends to infer the prefer ence keyword whimsical The agent has difficulty to infer subtle aspects including invoke personal reflection immersive positive parallel structure inquisitive and skillful foreshadowing 2 Sometimes fail to in fer some important aspects even though the user edits clearly show such prefer ence The agent often could not infer second person nar rative For question answering style the agent occasionally only learns consistent format Consolidation of induced preferences from retrieved interac tions ft Overall this step can cap ture the majority preference relatively well although it tends to result in a more gen eral preference compared to the retrieved ones When both specific phrase second person narra tiveand general phrase narrative ornarration oc cur in retrieved examples the agent often chooses to give a final preference not including the second person perspective aspect Retrieval of historical examples relevant to the given contextThe retrieval part in general works reasonably well with more than half of the re trieved example being truly relevant to the given con text Note that one incor rect retrieved example typ ically does not affect the performance as we instruct the agent to only use the most represented preference keywords among all five re trieved examples The agent sometimes retrieves wrong examples for Wikipedia context when its topic very relates to other context e g wrongly retrieving past ex amples on news articles and movie reviews when the topic in the given Wikipedia context relates to these domains Table 11 We report retrieval accuracy as the percentage of total retrieved document representations across all time steps and seeds that are of the same document source type as the context document for which they were retrieved We use 3 seeds We retrieve 600 examples for k 1and 2970 examples fork 5 Method Summarization Email Writing CIPHER 1 B 72 00 25 83 CIPHER 1 M 82 00 26 33 CIPHER 5 B 65 79 26 57 CIPHER 5 M 76 33 25 45 22 
 The fields of effective resistance and optimal transport on graphs are filled with rich connections to combinatorics geometry machine learning and beyond In this article we put forth a bold claim that the two fields should be understood as one and the same up to a choice of p We make this claim precise by introducing the parameterized family of p Beckmann distances for probability measures on graphs and relate them sharply to certain Wasserstein distances Then we break open a suite of results including explicit connections to optimal stopping times and random walks on graphs graph Sobolev spaces and a Benamou Brenier type formula for 2 Beckmann distance We further explore empirical implications in the world of unsupervised learning for graph data and propose further study of the usage of these metrics where Wasserstein distance may produce computational bottlenecks Keywords effective resistance optimal transport graph Laplacians convex optimization random walks MSC 65K10 05C21 90C25 68R10 05C50 Contents 1 ,All You Need is Resistance On the Equivalence of Effective Resistance and Certain Optimal Transport Problems on Graphs Sawyer Robertson Zhengchao Wan and Alexander Cloninger April 24 2024 AbstractIntroduction and notation 2 1 1 Introduction 2 1 2 Outline of this paper 3 1 3 Related work 4 1 4 Mathematical background 4 2 General properties of the Beckmann problem 8 2 1 Duality theory for the p Beckmann problem 8 2 2 Comparing p Beckmann and p Wasserstein distances 9 2 3 Worked examples Paths and trees 11 3 The 2 Beckmann problem Three perspectives 13 3 1 Effective resistance between measures 13 3 2 Sobolev norms and a graph Benamou Brenier formula 18 3 3 A linearized optimal transportation distance 20 4 Acknowledgements 24 A Proofs 29 A 1 Proofs from section 2 30 A 2 Proofs from section 3 32 Department of Mathematics University of California San Diego Halƒ±cƒ±o glu Data Science Institute University of California San Diego 1arXiv 2404 15261v1 math OC 23 Apr 2024 1 Introduction and notation 1 1 Introduction The theory of optimal transportation OT which traces its roots to the Monge formulation as early as 1781 56 as well as the Kantorovich relaxation articulated in 1942 41 is typically framed as an optimization problem for some convex or affine function which measures the cost of mass transportation over all feasible plans which provide recipes for transporting some mass distribution to another OT has received significant attention due to its connections to geometry 85 33 partial differential equations 7 30 29 applied mathematics 63 71 72 32 69 and many other fields For discrete measures and domains in particular OT and Wasserstein distance have been used and studied for a variety of purposes including document retrieval 44 statistics 81 61 11 image registration 38 distance approximation 75 political redistricting 55 15 1 graph neural networks 16 17 92 and graph Ollivier Ricci curvature 83 4 70 59 When the underlying metric space of the transportation process is a graph much attention has been focused on the 1 Wasserstein metric which can be informally set up as a linear program of the following form W1pŒ± Œ≤q inf √ø ijœÄijdpi jq œÄPŒ†pŒ± Œ≤q 1 where Œ†pŒ± Œ≤qconsists of the couplings of two generic probability distributions Œ± Œ≤on the nodes of a graph anddpi jqis the shortest path metric on the vertex set V see Definition 1 3 for a complete description It so happens on graphs that this metric can also be written in a fashion often termed the Beckmann formulation W1pŒ± Œ≤q inf √ø e Jpeq we J E√ëR BJ Œ± Œ≤ 2 where Bis the graph incidence matrix Jis any map on the edges of the graph and which is identified as a vector in R E and weis the nonnegative weight for edge ePE see Definition 1 4 for a complete description This formulation is named after Martin Beckmann who studied dynamic formulations of OT beginning in the 1950s 5 and whose models for OT in the continuous setting Eq 2 closely mirrors in a discrete sense The variable Jis sometimes called a feasbile flow between Œ±andŒ≤ see Fig 1 and as such Eq 2 is also sometimes deemed a flow based formulation for OT on graphs Seemingly separately effective resistance ER on graphs has long been studied for its metric properties between nodes and its connections to many other problems in graph theory Recall that for two nodes i jPV and corresponding standard basis vectors Œ¥i Œ¥jPR V the ER between them is given by rij pŒ¥i Œ¥jqTL pŒ¥i Œ¥jqwhere L is the Moore Penrose psuedo inverse of the graph Laplacian Applications and research on ER include foundational work by Spielman and Srivastava on spectral graph sparsifcation using ER 76 as well as spectral clustering models leveraging computational advantages of ER solvers 42 ER has also been used for computing and studying statistics associated to random walks on graphs 25 82 50 Interestingly in the case of directed and connection graphs the opposite is the case as random walk statistics have been used to obtain versions of ER 78 19 Moreover there exist geometric applications involving ER as an ingredient for defining discrete Ricci curvature 22 23 For other references which study and utilize effective resistance in various contexts see e g 25 40 34 91 36 12 18 10 This paper is built on an observation which seems to have not yet appeared in the field that effective resistance when extended to probability measures on a graph G can be realized as an OT distance obtained from a Beckmann type problem with a 2 norm penalty The resulting metric while removed from a conven tional coupling based formulation of OT is intricately connected to the theory of random walks on graphs the graph Laplacian matrix and graph Sobolev spaces 2 Theorem 1 1 Informal statement of Theorem 3 4 LetG pV E wqbe a weighted connected graph with Laplacian matrix Land psuedoinverse L Denote the oriented vertex edge incidence matrix of GbyB and letŒ± Œ≤ be fixed probability measures on Vregarded as vectors in R V For 1ƒèpƒÉ8 define BppŒ± Œ≤qby the following convex optimization problem BppŒ± Œ≤qp inf √ø ePE Jpeq pwe J E√ëR BJ Œ± Œ≤ 3 Then it holds a When p 1 B1pŒ± Œ≤q W1pŒ± Œ≤q b When p 2 B2pŒ± Œ≤q pŒ± Œ≤qTL pŒ± Œ≤q The metric introduced in Theorem 1 1 for p 2 is a squared norm distance so its properties and structures are fundamentally different from the nonlinear Wasserstein metric in general However this metric seems to posses a rich array of properties and characterizations which demonstrate strong homophily with the classical results in OT This metric also shows promise for use in kernel based learning methods for data defined on graphs and in terms of complexity is significantly less expensive to compute at scale compared to the Wasserstein metric Our main contributions are summarized as follows 1 We introduce the notion of p Beckmann distance between probability measures and derive various bounds between p Beckmann distances and p Wasserstein distances for measures on general graphs e g Corollary 2 11 2 We provide an explicit formula for Bpon trees Proposition 2 15 3 We obtain a generalized commute time formula for 2 Beckmann distance or measure effective resis tance in terms of optimal access times of the simple random walk on a graph Theorem 3 13 4 We realize 2 Beckmann distance as a negative Sobolev type semi norm distance and use this property to obtain a Benamou Brenier type formula for 2 Beckmann distance Theorem 3 22 5 We apply 2 Beckmann distance to an unsupervised kernel based clustering method involving handwrit ten digit data and compare the results with the 2 Wasserstein kernel and thereby establish empirical evidence for the usage of a 2 Beckmann kernel on a drop in basis in place of a 2 Wasserstein kernel for datasets defined on graphs Fig 6 1 2 Outline of this paper In Section 1 4 we provide relevant mathematical background and notation In Section 2 we delve into the general properties of the p Beckmann problem including duality theory in Section 2 1 and estimates between Beckmann and Wasserstein distances Section 2 2 In Section 2 3 we highlight some specialized results in the cases of paths and trees In Section 3 we focus exclusively in the properties and theory of the 2 Beckmann distance In Section 3 1 we frame the metric as effective resistance between measures and relate it to optimal stopping times and access times between probability measures on G In Section 3 2 we frame the metric as a negative Sobolev type seminorm and obtain a graph Bemaou Brenier type formula Finally in Section 3 3 we apply the 2 Beckmann distance to an unsupervised learning problem and explore various empirical observations and implications Lastly in Appendix A we provide proofs of the results mentioned in the main component of the paper 3 1 3 Related work In this section we wish to highlight some particularly relevant works on which this paper is based in parts and in doing so better situate the novelty of our contributions in a broader context The most relevant investigation of OT on graphs including regularization and convex duality appears in work by Essid and Solomon 28 and further studies in the field can be found in 75 74 62 57 67 49 65 The introduction of p Beckmann distances in Section 2 and its study as an effective resistance metric in Section 3 1 should be understood as extensions of the notion of p resistances from work of Alamgir and Luxburg 2 to general probability measures from the case of nodes i e Dirac measures Such generalizations of resistances and their implications in unsupervised models were also considered by Nguyen and Hamitsuka 60 and Saito and Herbster 68 but only between nodes The notions of measure access times and optimal stopping times have been studied at length by Lov asz Winkler and Beveridge across several papers 50 51 52 9 8 Separately the notion of using negative Sobolev distance as a linearization of quadratic Wasserstein distance was studied by Greengard Hoskins Marshall and Singer in 37 and is a homophilous framework appearing in the continuous setting The proof of the Benamou Brenier type formula for 2 Beckmann distance is a direct discretization of the proof appearing in 35 It is also notable that our model of 2 Beckmann distance as a negative Sobolev distance on graphs is parallel to the works by Le T Nguyen Phung V A Nguyen and Fukumizo and appearing in 46 47 48 but our definitions differ slightly from theirs Linearized OT and its usage in the unsupervised setting is also explored by Moosm uller and AC 58 and results therein form a basis for our Theorem 3 25 on linear separation of measures in the 2 Beckmann metric space 1 4 Mathematical background LetG pV E wqbe a graph where V t1 2 nuis the set of vertices EƒÇ V 2 is a set of undirected edges of cardinality mƒõ0 and w pwijqi jPVis a choice of real edge weights satisfying wijƒõ0 wij wji andwijƒÖ0 if and only if ti juPE In order to ensure the feasibility of optimal transportation problems and simplify the exposition in places we assume that Gis finite has no multiple edges or loops and is connected For our purposes a path in Gis an ordered sequence of nodes P pi0 i1 i kqsuch that i‚Ñì i‚Ñì 1for 0ƒè‚Ñìƒèk 1 If Pis a path P is the integral length of the path i e P k and P wis the weighted length of P or the sum of the weights of edges it contains i e P w ≈ôk 1 ‚Ñì 0wi‚Ñìi‚Ñì 1 For any two nodes i jPV Ppi jqis the set of all paths which begin at iand end at j In this paper we will consider several different metrics on V A metric on Vis a map Œ∫ VÀÜV√ëR such that i Nonnegativity Œ∫pi jqƒõ0 for each i jPV ii Definiteness Œ∫pi jq 0 if and only if i j iii Symmetry Œ∫pi jq Œ∫pj iqfor each i jPV iv Triangle inequality Œ∫pi kqƒèŒ∫pi jq Œ∫pj kqfor each i j kPV We will primarily use variants of the shortest path metric on Vand the metrics induced by effective resistance Definition 1 2 Let 1ƒèpƒÉ8 Define the weighted p shortest path metric onVby the formula dppi jq inf k 1√ø ‚Ñì 0wp i‚Ñìi‚Ñì 1 1 p P pi i0 i1 i k jqPPpi jq 4 4 It is relatively straightforward to verify that dpsatisfies the properties necessary for a metric on Vfor each p w We define the Adjacency matrix APRnÀÜnentrywise by Aij wijifi j 0 otherwise 5 where the notation i jmeansti ju PE For each iPVwe define its degree di ≈ô j iwij We will also use the diagonal degree matrix D diagpd1 d nq PRnÀÜnand the diagonal edge weight matrix W diagpwe1 w emqPRmÀÜm For technical purposes we may ocassionally refer to the index oriented and bi oriented edge sets E1and E2 respectively defined below E1 tpi jq i jPV i j iƒÉju 6 E2 tpi jq pj iq i jPV i ju 7 We define the incidence matrix BPRnÀÜm with rows indexed by Vand columns indexed by E1 by the formula Bi ej 1 if ej pi q 1 if ej p iq 0 otherwise 8 We also occasionally use the matrix rBPRnÀÜ2m with rows indexed by Vand columns indexed by E2 defined by an identical formula as in Eq 8 We define the Laplacian matrix Lby the formula L D A or L BW BT Note that rBƒÇWrBT 2L whereƒÇWPR2mÀÜ2mis the diagonal matrix of edge weights of E2 We recall that Lis symmetric positive semi definite and has a set of nonnegative eigenvalues 0 Œª0ƒèŒª1ƒè ƒèŒªn 9 We use Œõ diagpŒª0 Œª1 Œª nqPRnÀÜnfor the diagonal matrix of eigenvalues and U u1u2 un for the respective orthonormal eigenvectors of L The spectral decomposition of Lis given by the equation L UŒõUT For any matrix XPR‚ÑìÀÜk we will use the notation X PRkÀÜ‚Ñìfor its Moore Penrose psuedoinverse We recall that X is the unique matrix satisfying the following four properties 6 i XX X X ii X XX X iii pXX qT XX iv pX XqT X X wherep qTindicates matrix transpose In the case of the Laplacian matrix L we have L UŒõ 1UT where with an abuse of notation we write Œõ 1 diagpŒª0 0 Œª 1 1 Œª 1 nqPRnÀÜn 10 5 Similarly we write L 1 2 UŒõ 1 2UT For a finite set S we define ‚Ñì2pSqto be the linear space of functions f S√ëRwith the standard Euclidean inner product xf gy‚Ñì2pSq gTf We identify ‚Ñì2pVqwithRnand‚Ñì2pE1qwithRm respectively For a vector xPRnand 1ƒèpƒÉ8 we define the vector p norm by x p n√ø i 1 xi p 1 p For any finite set S in a similar manner as before we can define ‚ÑìppSqto be the linear linear space of functions f S√ëRwith the norm p If in some cases we wish to refer simply to the linear space of functions on S such as when we employ multiple norms at once we use the notation ‚ÑìpSqand specify the norm or inner product in context IfJP‚ÑìpE1q then we can weight the norm pby the edge weights as follows J w p √ø ePE1 Jpeq pwe 1 p 11 and similarly for ‚ÑìpE2q We will use the subscript of wto indicate the presence of edge weights As a matter of notation functions in ‚ÑìpVqwill be denoted with lowercase Latin and Greek letters in the case of measures and functions in ‚ÑìpE1qand‚ÑìpE2qwill be denoted with uppercase Latin letters Note that the matrices B BT and Lact on their respective function spaces by matrix multiplication We summarize their pointwise formulas below pBJqpiq √ø ePE1 e pi qJpeq √ø ePE1 e p iqJpeq i PV JP‚ÑìpE1q 12 pBTfqpe pi jqq fpiq fpjq e PE1 fP‚ÑìpVq 13 pLfqpiq √ø j ipfpiq fpjqqwij i PV fP‚ÑìpVq 14 It is worth noting that in some contexts e g 27 26 Bp qandBTp qare considered the graph equivalents of the divergence and gradient operators respectively We will also occasionally use the Dirichlet energy functional for fP‚ÑìpVq defined by the quadratic form of the Laplacian matrix fTLf √ø pi jqPE1wijpfpiq fpjqq2 BTf 2 w 2 15 We define the probability measure simplex PpVqby the set PpVq Œ±P‚ÑìpVq Œ±ƒõ0 √ø iPVŒ±piq 1 16 ForiPV Œ¥iis the Dirac or unit measure at node i identified with the i th standard basis vector in Rn The primary objects of study for this paper are two different optimal transportation distances between probability measures on V The first which is classical and extensively studied on graphs is p Wasserstein distance Definition 1 3 LetŒ± Œ≤PPpVq 1ƒèpƒÉ8 and kany metric on V Define the set of transportation couplings between Œ±andŒ≤ denoted Œ†pŒ± Œ≤q by the following set Œ†pŒ± Œ≤q œÄPRnÀÜn œÄƒõ0 œÄ1 Œ± 1TœÄ Œ≤T 17 6 Œ± Œ≤ a p 1 B1pŒ± Œ≤q W1pŒ± Œ≤q 9 3 Œ± Œ≤ b p 2 W2pŒ± Œ≤q 1 225 B2pŒ± Œ≤q 1 499 Figure 1 A side by side comparison of optimal flows for the 1 Beckmann and 2 Beckmann problems on a 4ÀÜ4 hexagonal lattice graph The masses of Œ± Œ≤at each node are rendered proportionally to opacity and similarly the optimal flow values at each edge are rendered proportionally to opacity The arrows indicate orientation of the flow value i e √ë 1if the optimal flow Jsatisfies Jp 1qƒÖ0 and √ê 1ifJp 1qƒÉ0 where 1PRnis the vector containing all ones We define the pk pq Wasserstein distance between two probability measures denoted Wk ppŒ± Œ≤qby the following optimization problem Wk ppŒ± Œ≤q inf √ø i jPVœÄijkpi jqp 1 p œÄPŒ†pŒ± Œ≤q 18 As a matter of convention when k d1 that is the weighted 1 shortest path metric we write Wd1 p Wp The second optimal mass transportation distance is much less studied in its own right except in the cases of p 1 and to a seemingly much lesser extent p 2 Definition 1 4 Let 1ƒèpƒÉ8 andŒ± Œ≤PPpVq Define the set of feasible edge flows between Œ±andŒ≤ denoted JpŒ± Œ≤q by the affine region JpŒ± Œ≤q JP‚Ñì2pE1q BJ Œ± Œ≤ 19 Then the p Beckmann distance between Œ± Œ≤ denoted BppŒ± Œ≤qis given by the following constrainted norm optimization problem BppŒ± Œ≤q inft J w p JPJpŒ± Œ≤qu 20 Note that JpŒ± Œ≤q for any Œ± Œ≤ since the column space of Bis exactly the mean zero functions onV The p Wasserstein and p Beckmann problems are essentially two different perspectives on optimal transportation for measures on graphs The Wasserstein philosophy being that mass transportation is tracked between all pairs of nodes and is accounted according to dp ij We can also think of this philosophy as one of mass teleportation the primal variable or coupling does not reveal howmass œÄijmoves from ito j only that it did so along a shortest path somewhere or a combination of shortest paths The Beckmann philosophy is the viewpoint that mass must move along edges and the transportation should be accounted 7 in terms of edge flows which are really just signed mass values on each oriented edge In this formulation the penalty lies on Jpeq p so transportation flows can often be non sparse or occur along weighted combinations of shortest and non shortest paths Comparisons of optimal flows for values p 1 2 in an example setting are shown in Fig 1 Remark 1 5 A slightly different way to formulate Definition 1 4 would be through E2with a nonnegativity constraint That is let J2pŒ± Œ≤q JP‚Ñì2pE2q Jƒõ0 rBJ Œ± Œ≤ 21 and then BppŒ± Œ≤q inf J w p JPJ2pŒ± Œ≤q 22 It is not hard to show that these two formulations arrive at the same value Definition 1 4 simply allows for signed flows for example a flow value of 1 along the oriented edge p1 2qrepresents one unit of mass transported from node 2 to node 1 We opt to drop the nonnegativity constraint for this paper and focus on the former formulation as it simplifies the calculations slightly but we may have occassion to use this formulation instead In the Proposition below we summarize two relevant properties of WpandBp Proposition 1 6 LetŒ± Œ≤PPpVq i The objective functions for both WppŒ± Œ≤qandBppŒ± Œ≤qare convex for all 1ƒèpƒÉ8 and the infima are always achieved ii The objective for BppŒ± Œ≤qis strictly convex for pƒÖ1 The infimum is always achieved and is unique in the case pƒÖ1 iii WpandBppŒ± Œ≤qdefine metrics on PpVq Proofs of items i and ii are straightforward since the objectives are defined either by affine functions or common norms whose convexity properties are well understood For iii a proof for Wpcan be found in 62 Ch 2 and the case of Bpis relatively straightforward 2 General properties of the Beckmann problem In this section we look at a few interesting properties of the Beckman problem BppŒ± Œ≤qfor general 1 ƒè pƒÉ8 In Section 2 1 we derive the Lagrangian dual to the p Beckmann problem in Section 2 2 we obtain bounds that relate certain Beckmann and Wasserstein distances and finally in Section 2 3 we work through some example results on paths and trees to highlight examples of Wasserstein and Beckmann distances 2 1 Duality theory for the p Beckmann problem In this subsection we study the convex duality of the p Beckmann problem In 2 Alamgir and Luxburg intro duced formulations of p resistances that is generalizations of effective resistance obtained by Beckmann type min cost flow problems penalized by a p norm They studied the primal and dual forms of these problems and thereby obtained so called potential based formulations of the resistance problems As described in Section 1 3 the results presented in this section may be considered extensions of the results concerning p resistances to the case of general probability measures namely we understand the p resistance between generic nodes i jto be p Beckmann distance between Dirac measures at i j In Theorem 2 1 we obtain the dual formulation of the p Beckmann optimization problem and thus generalize the Kantorovich Rubenstein duality on graphs to all 1 ƒèpƒÉ8 see Remark 2 3 8 Theorem 2 1 LetŒ± Œ≤PPpVqand1ƒÉpƒÉ8 be fixed Let qbe the conjugate of pso that1 p 1 q 1 Then the dual to the p Beckmann problem given by BppŒ± Œ≤q inf J w p JP‚ÑìpE1q BJ Œ± Œ≤ 23 is given by the maximization problem BppŒ± Œ≤q sup œÜTpŒ± Œ≤q œÜP‚ÑìpVq BTœÜ w1 q qƒè1 24 and strong duality holds In the special case of p 1andq 8 we have B1pŒ± Œ≤q inf J w 1 JP‚ÑìpE1q BJ Œ± Œ≤ sup œÜTpŒ± Œ≤q œÜP‚ÑìpVq BTœÜ w 1 8ƒè1 25 We defer the proof to Appendix A 1 Remark 2 2 If we specialize to the case where the edge weights are all unit value i e Gis unweighted then the weights can be dropped from the preceding results In particular the dual norm to pis simply qand BppŒ± Œ≤q inf J p JP‚ÑìpE1q BJ Œ± Œ≤ sup œÜTpŒ± Œ≤q œÜP‚ÑìpVq BTœÜ qƒè1 26 Remark 2 3 The case p 1 and q 8 is the conventional Kantorovich Rubenstein duality on graphs 89 which can be more conventionally written as inf √ø ePE1 Jpeq we JP‚ÑìpE1q BJ Œ± Œ≤ sup œÜTpŒ± Œ≤q œÜP‚ÑìpVq œÜ Lipƒè1 27 where the Lipschitz seminorm œÜ Lip BTœÜ w 1 8on‚ÑìpVqis defined by œÜ Lip max œÜpiq œÜpjq dpi jq max i j œÜpiq œÜpjq wij 28 Note also that this duality occurs in a homophilous manner for Beckmann s minimal flow problem in the continuous setting 71 Sec 4 2 which is the namesake for the minimal flow formulations on graphs 2 2 Comparing p Beckmann and p Wasserstein distances In this section we establish some bounds between BpandWpin certain situations Section 2 2 1 introduces a technique for obtaining an edge flow from a given coupling with some control on the cost and uses it to derive elementary bounds for general p and in Section 2 2 2 we focus on the case of W1and obtain sharp estimates related to B2 the latter of which is of particular interest in the ensuing sections 2 2 1 From coupling to flow An essential ingredient in comparing the optimal values of transportation metrics is having the ability to convert a coupling œÄto a feasible edge flow J and vice versa In this subsection we will make use of the bi oriented edge flow formulation of Bp which was discussed in Remark 1 5 Definition 2 4 Leti jPV Let PPPpi jqbe a path between i j Write P pi i0 i1 i k jq Define IP k 1√ø ‚Ñì 0Œ¥pi‚Ñì i‚Ñì 1qP‚ÑìpE2q 29 9 Proposition 2 5 LetŒ± Œ≤PPpVq 1ƒèpƒÉ8 and suppose œÄPŒ†pŒ± Œ≤q For each pair of nodes i jPV with i j letPijPPpi jqbe a fixed path between i j Define the edge flow JœÄ JœÄ pPijqP‚ÑìpE2qby the expression JœÄ √ø i jPVœÄijIPij 30 Then JœÄis a feasible flow for BppŒ± Œ≤q i e JœÄƒõ0andBJœÄ Œ± Œ≤ and the following estimate holds JœÄ w pƒè√ø i jPVœÄij IPij w p 31 We include a proof in Appendix A 1 We can use Proposition 1 6 to derive at least two estimates which relate BptoWk pfor different choices of k The first is somewhat brutal but is informative for psmall Theorem 2 6 LetŒ± Œ≤PPpVq 1ƒèpƒÉ8 and assume that wijƒõ1for each i jPVsuch that i j Then BppŒ± Œ≤qƒèWppŒ± Œ≤qp 32 Proof LetœÄPŒ†pŒ± Œ≤qbe an optimal coupling for WppŒ± Œ≤q For each i j letPijPPpi jqbe a choice of path which is minimal in the sense of d1 and let JœÄ JœÄ pPijqbe the feasible flow as in Proposition 2 5 Then BppŒ± Œ≤qƒè JœÄ w pƒè√ø i jPVœÄij IPij w pƒè√ø i jPVœÄijd1pi jqp WppŒ± Œ≤qp 33 since w pƒè p w 1provided wijƒõ1 If we allow for some flexibility when picking the underlying metric kofWk p then we can provide a tighter estimate when Wis large Theorem 2 7 LetŒ± Œ≤PPpVq 1ƒÉpƒÉ8 and let qbe the conjugate of psuch that1 p 1 q 1 Then BppŒ± Œ≤qƒèn2 qWdp ppŒ± Œ≤q 34 The proof of Theorem 2 7 follows mainly from H older s inequality and Proposition 2 5 we provide the proof in Appendix A 1 Note in particular that when p 2 we have q 2 and thus it holds B2pŒ± Œ≤qƒè nWd2 2pŒ± Œ≤q 2 2 2 Estimating from the p 1case When p 1 and the choice of metric is k d1there is a direct overlap between B1andW1 62 28 Theorem 2 8 LetŒ± Œ≤PPpVq Then B1pŒ± Œ≤q W1pŒ± Œ≤q For a proof of this result we recommend the exposition in 62 Ch 6 Note that for pƒõ1 and JP‚ÑìpE1q we have that since pƒè 1in general J w p W1 pJ pƒè W1 pJ 1ƒè J w 1ÀÜ max ePE1w1 p 1 e Thus if Cw p max ePE1w1 p 1 e we have J w pƒèCw p J w 1 IfJ is an optimal flow for B1pŒ± Œ≤q we have BppŒ± Œ≤qƒè J w pƒèCw p J w 1 Cw pB1pŒ± Œ≤q 35 Therefore we have the following two estimates for any choice of Œ± Œ≤andpƒõ1 10 Corollary 2 9 LetŒ± Œ≤PPpVqand1ƒèpƒÉ8 Then the following estimates hold BppŒ± Œ≤qƒèCw pB1pŒ± Œ≤q 36 BppŒ± Œ≤qƒèCw pW1pŒ± Œ≤q 37 Moreover via similar logic as before since 1ƒèm1 1 p pon‚ÑìpE1q where we recall m E we have for JP‚ÑìpE1q J w 1 W J 1ƒèm1 1 p W J pƒèm1 1 p W1E1 p 1 p 8 J w p 38 Therefore if we set Cw m p m1 1 p W1E1 p 1 p 8 we have J w 1ƒèCw m p J w pand using a similar argument as before the following corollary holds Corollary 2 10 LetŒ± Œ≤PPpVqand1ƒèpƒÉ8 Then the following estimates hold B1pŒ± Œ≤qƒèCw m pBppŒ± Œ≤q 39 W1pŒ± Œ≤qƒèCw m pBppŒ± Œ≤q 40 An application of this allows us to relate 2 Beckmann distance to W1 which we state as a corollary below Corollary 2 11 LetŒ± Œ≤PPpVq Then B2pŒ± Œ≤qƒèCw 2W1pŒ± Œ≤qƒèCw 2Cw m 2B2pŒ± Œ≤q 41 If in particular the graph is unweighted then we have that Cw 2 1andCw m 2 m1 2 so that B2pŒ± Œ≤qƒèW1pŒ± Œ≤qƒèm1 2B2pŒ± Œ≤q 42 Remark 2 12 Note that the bound in Corollary 2 11 is sharp without making additional assumptions on the structure of GorŒ± Œ≤ Suppose Gis a path on nvertices with m n 1 then the upper and lower bounds are achieved respectively when we have i Œ± Œ¥1 Œ≤ Œ¥n so that B2pŒ¥1 Œ¥nq n 1 and W1pŒ¥1 Œ¥nq n 1 soW1 m1 2B2 ii Œ± Œ¥1andŒ≤ Œ¥2 so that B2 W1 1 Proofs of these statements could be obtained directly or by way of Proposition 2 13 and Proposition 2 14 which we discuss in the next subsection This leads to an as yet open question If one constrains the structure ofGorŒ± Œ≤ can the bound in Corollary 2 11 be improved 2 3 Worked examples Paths and trees In this subsection we specialize to the case of the path graph Pnand derive explicit formulas for WpandBp so as to illustrate their connections to optimal transportation in the setting of the real line and to elucidate their differences when pƒÖ1 We also take a look at Bpon tree graphs and obtain a closed form solution for all 1ƒèpƒÉ 8 which generalizes the known formula for 1 Wasserstein distance on trees Proofs of Proposition 2 13 Proposition 2 14 and Proposition 2 15 are all included in Appendix A 1 LetPnbe the unweighted path graph with nnodes ordered from 1 to n and oriented edges E1of the formpi i 1qfor 1ƒèiƒèn 1 For Œ±PPpVpPnqq letpŒ± ≈ôn i 1Œ±piqŒ¥ibe the empirical measure on Rinduced by Œ±where with a slight abuse of notation Œ¥iis the Dirac measure on Ratx i Let FpŒ± be the cumulative distribution function of pŒ± and let F 1 pŒ±be its psuedo inverse or quantile function i e F 1 pŒ±ptq inftx FpŒ±pxqƒõtufortPr0 1s The proposition below is an adaptation of the classical inverse cdf result for WponRto the case of path graphs for more detail see e g 64 71 11 Œ±piq i1 2 11 2 20 3 Œ≤piq 01 43 4 a Illustration of two copies of P3with arrows indi cating orientation of the edges Example probability measures Œ± Œ≤are shown with node color opacity pro portional to mass KŒ±peq ep1 2q1 2 p2 3q1 KŒ≤peq 01 4 b Illustration of two copies of P3with arrows indicat ing orientation of the edges The edge cdfs KŒ±and KŒ≤are shown with edge color opacity proportional to value 0 1 2 300 250 50 751 FpŒ±pxq FpŒ≤pxq x c Plots of the cdfs FpŒ± FpŒ≤of the empirical mea surespŒ± pŒ≤onR From Proposition 2 14 we have BppŒ± Œ≤qp 0 5p 0 75p 0 0 25 0 5 0 75 10123 F 1 pŒ±ptqF 1 pŒ≤ptq t d Plots of the inverse cdfs F 1 pŒ±andF 1 pŒ≤of the empirical measure pŒ± pŒ≤onR From Proposition 2 13 we have WppŒ± Œ≤qp 1pp0 25q 2pp0 25q 1pp0 5q Figure 2 a b Illustrations of two measures Œ± Œ≤ and their edge cdfs KŒ± KŒ≤for the fixed path graph P3 c d Plots of the empirical cdfs FpŒ±andFpŒ≤ as well as their inverses Note that the shaded regions are reflections of each other and that for p 1 their common area is B1pŒ± Œ≤q W1pŒ± Œ≤q This also demonstrates the divergence of the metrics for pƒÖ1 Proposition 2 13 LetŒ± Œ≤PPpVpPnqqand1ƒèpƒÉ8 Then the p Wasserstein distance WppŒ± Œ≤qonPn is given by WppŒ± Œ≤qp ≈º1 0ÀáÀáÀáF 1 pŒ±ptq F 1 pŒ≤ptqÀáÀáÀáp dt 43 To characterize p Beckmann distance on Pn we introduce a bit more notation For Œ±PPpVqand an oriented edge e pi i 1qPE1 letKŒ±P‚ÑìpE1qbe the edge cdf i e KŒ±pi i 1q √ø jƒèiŒ±pjq 44 Note that naturally KŒ±pi i 1q FpŒ±piq Proposition 2 14 LetŒ± Œ≤PPpVpPnqqand1ƒèpƒÉ8 Then the p Beckmann distance BppŒ± Œ≤qonPnis given by BppŒ± Œ≤qp KŒ± KŒ≤ p p ≈º R FpŒ±ptq FpŒ≤ptq pdt 45 12 Proposition 2 13 and Proposition 2 14 provide an alternate albeit circuitous proof of Theorem 2 8 on paths by applying a change of variables This also demonstrates why we do not expect the two families of metrics to overlap for pƒÖ1 A worked example of Proposition 2 13 and Proposition 2 14 in action is illustrated in Fig 2 It is also intriguing to note that much in the same way that W1 and hence B1 is determined explicitly on trees a result which itself has been leveraged in other applications such as computer vision and natural language processing 79 53 90 80 31 45 Bpcan be determined in an similar manner for general pon trees Proposition 2 15 LetT pV E wqbe a weighted tree Œ± Œ≤PPpVq and fix 1ƒèpƒÉ8 For an oriented edge e pi jqPE1 define a generalized version of Eq 44 by KŒ±pe pi jqq √ø kPV pi eqŒ±pkq where V pi eqƒÇVis the set of nodes belonging to the subtree with root iobtained from Tby removing the edge e and similarly for KŒ≤ Then it holds BppŒ± Œ≤q KŒ± KŒ≤ w p 46 3 The 2 Beckmann problem Three perspectives This section is focused entirely on B2 The foundational premise of this section is that B2 while ostensibly a simple least squares optimization problem actually posesses a rich supply of connections and contexts to other notions that already exist in graph theory In Section 3 1 we view B2as an effective resistance metric between measures and connect it to the simple random walk on Gand optimal stopping times for measures In Section 3 2 we view B2as a negative Sobolev norm for functions on graphs and thereby obtain a Benamou Brenier type formula for B2 Lastly in Section 3 3 we view B2as a linearized variant of W2 prove a convex separation result and showcase its potential application in unsupervised learning problems for graph data 3 1 Effective resistance between measures In this subsection we consider B2from the perspective of effective resistance We begin with some sup plemental background on effective resistance which will be needed and then proceed with discussion of the main results 3 1 1 Background on effective resistance Definition 3 1 Leti jPVbe any two nodes The effective resistance between i j denoted rij is given by the formula rij pŒ¥i Œ¥jqTL pŒ¥i Œ¥jq 47 The effective resistance between the nodes of a graph has many different properties and representations many of which intersect with the simple random walk on G We give its definition and some of its properties below Definition 3 2 Thesimple random walk onGis the Markov chain pXtqtƒõ0on the state space of nodes V with transition probability matrix D 1A that is PrXt 1 j Xt is wij diifi j 0 otherwise 13 Recall that Xtwill admit a stationary distribution whenever Gis connected and will be ergodic whenever Gis also non bipartite The stationary distribution which we denote œÅ is always proportional to the degree at each node A useful tool is the vertex hitting time Ts j infttƒõs Xt ju sƒõ0 jPV 48 Also let volpGq ≈ô iPVdidenote the volume of the graph G Proposition 3 3 Leti jPVbe any two nodes Then the following observations hold i pi jq√û√ërijis a metric on V ii pi jq√û√ër1 2 ijis a metric on V iii rij inftfTLf Lf Œ¥i Œ¥ju iv rij L 1 2pui ujq 2 2 where uiis the i th orthonormal eigenvector of the graph Laplacian L v rij 1 volpGq ErT0 i X0 js ErT0 j X0 is i e rijis proportional to the commute time between i j For proofs of these results see e g 40 50 25 3 1 2 2 Beckmann as measure effective resistance We begin with the observation that B2pŒ± Œ≤q2can be realized as a type of effective resistance for measures We make this precise in the theorem below Theorem 3 4 LetŒ± Œ≤PPpVq Then B2pŒ± Œ≤q2 pŒ± Œ≤qTL pŒ± Œ≤q 49 Proof We can write B2pŒ± Œ≤q2 inft J 2 w 2 BJ Œ± Œ≤u 50 inft W1 2J 2 2 BJ Œ± Œ≤u 51 inft S 2 2 pBW 1 2qS Œ± Œ≤u 52 with S W1 2J Recall that for matrices X xit holds that X x argmint y 2 Xy xuwhen xbelongs to the column space of X 6 and thus we have B2pŒ± Œ≤q2 pBW 1 2q pŒ± Œ≤q 2 2 53 pŒ± Œ≤qTppBW 1 2q qTpBW 1 2q pŒ± Œ≤q 54 pŒ± Œ≤qTL pŒ± Œ≤q 55 since L BW BT One immediate consequence of Theorem 3 4 is a Brenier type theorem for graphs which we state as a corollary below Based on the discussion following Eq 12 in this theorem we opt to use the notation BT to achieve homophily with the continuous setting Corollary 3 5 LetŒ± Œ≤PPpVq The unique optimal edge flow J forB2pŒ± Œ≤qcan be written J fwhere fis a mean zero potential function of minimal ‚Ñì2norm satisfying the Poisson type equation Lf Œ± Œ≤ The optimal cost is f w 2 14 Proof It is enough to write pŒ± Œ≤qTL pŒ± Œ≤q pŒ± Œ≤qTL LL pŒ± Œ≤qandL BW BT The claim then follows from the proof of Theorem 3 4 Based on the Theorem 3 4 we introduce the following definition and notation of effective resistance for measures Definition 3 6 LetŒ± Œ≤PPpVq Then we define the measure effective resistance by rŒ±Œ≤ pŒ± Œ≤qTL pŒ± Œ≤q 56 In the subsequent sections we will explore the properties of rŒ±Œ≤in its own right with particular focus on its relationship to optimal stopping rules of the simple random walk 3 1 3 Measure effective resistance and optimal stopping rules We have already argued in Theorem 3 4 that measure effective resistance can be understood as a type of flow based optimal transportation distance between the measures The goal of this section is to essentially set that aside and think about rŒ±Œ≤in its own right A natural approach along these lines is to take Proposition 3 3 as a starting point Immediately rŒ±Œ≤has several analagous properties which we state below without proof Proposition 3 7 LetŒ± Œ≤PPpVq Then the following statements hold i pŒ± Œ≤q√û√ër1 2 Œ±Œ≤is a metric on PpVq ii rŒ±Œ≤ inf fTLf Lf Œ± Œ≤ iii rŒ±Œ≤ L 1 2pŒ± Œ≤q 2 2 To go deeper we pose the following question along the lines of Proposition 3 3 v To what extent can rŒ±Œ≤be understood as a generalized commute time between the measures Œ± Œ≤ To answer this question we need to use results that concern stopping rules for the simple random walk Definition 3 8 Astopping rule is a map Œì that associates to each finite path œâ pX0 X1 X kqon Ga number Œìpœâqinr0 1s We can think of Œì pœâqas the probability that we continue a random walk given that œâis the walk so far observed Alternatively Œì can be considered a random variable taking values in t0 1 2 uwhose distribution depends only on the steps pX0 X1 X Œìq The mean length ErŒìsis the expected duration of the walk If ErŒìsƒÉ8 then the walk stops almost surely in finite time so we define XŒìto be the position of the random walk at the stopping time Having defined stopping rules we can define the generalized hitting time between Œ± Œ≤ Definition 3 9 LetŒ± Œ≤PPpVq The access time HpŒ± Œ≤qis defined as HpŒ± Œ≤q inftErŒì X0 Œ±s ErŒìsƒÉ8 andXŒì Œ≤u 57 where for any random variable YonV we say Y Œ±ifPrY is Œ±piqforiPV In other words HpŒ± Œ≤qis the minimum mean length of walks that originate with distribution Œ±and terminate according to a stopping rule that achieves distribution Œ≤at stopping time If Œì achieves the inf in HpŒ± Œ≤q then Œì is said to be an optimal stopping rule Remark 3 10 It is not hard to see that the set of feasible stopping rules in Definition 3 9 is nonempty The so called na ƒ±ve stopping rule Œì ncan be obtained from the following construction at the beginning of the random walk sample j Œ≤ and stop the walk when XŒìn j It is readily verified that XŒìn Œ≤ and that ErŒìns √ø i jPVŒ±iŒ≤jHpi jq where for i jPV the hitting time Hpi jqis defined by Hpi jq HpŒ¥i Œ¥jq or the mean number of steps to reach jfrom i 15 Œ± Œ≤ a Initial distribution Œ± stopping distribution Œ≤ Œ± Œ≤ b Initial distribution Œ≤ stopping distribution Œ± Figure 3 Two illustrations of 1000 simulated simple random walks on the dodecahedral graph with given initial distribution illustrated with node opacity proportional to density and na ƒ±ve stopping rule according to the given stopping distribution illustrated with opposite node color and opacity proportional to density The edges are dashed only to indicate depth and edge opacity is proportional to the total number of times the simulated random walks landed on each edge The mean lengths of paths in a resp b correspond to HnpŒ± Œ≤q resp HnpŒ≤ Œ±qq Several examples of optimal stopping rules for general distributions are given in 52 and in particular it is shown thereby that any such Œ± Œ≤admit an optimal stopping rule provided Gis connected The access time HpŒ± Œ≤qhas several notable properties which we summarize in the Proposition below The proofs of these results can be found in 52 Proposition 3 11 LetŒ± Œ≤PPpVq Then the following facts hold for HpŒ± Œ≤q i When Œ≤is concentrated at a node jPV it holds HpŒ± jq ≈ô iPVŒ±iHpi jq ii HpŒ± Œ≤q max j≈ô iPVpŒ±i Œ≤iqHpi jq iii HpŒ± Œ≤qis convex in both of its arguments namely if Œ± Œ±1 Œ≤ Œ≤1PPpVqandcPr0 1s we have HpcŒ± p1 cqŒ±1 Œ≤qƒècHpŒ± Œ≤q p1 cqHpŒ±1 Œ≤q 58 HpŒ± cŒ≤ p1 cqŒ≤1qƒècHpŒ± Œ≤q p1 cqHpŒ± Œ≤1q 59 Beveridge 8 also established a connection between the entries of the psuedoinverse of the graph Lapla cian treated as a discrete Green s function and the access time between nodes and the stationary distribution of the random walk We reproduce that formula here as a proposition in a modified form to match our conventional choice of Laplacian matrix Theorem 3 12 Access Time Formula for the Discrete Green s Function Beveridge 8 LetœÅbe the stationary distribution of the simple random walk on V Then the i j th entry of L can be expressed as pL qij 1 volpGqpHpœÅ jq Hpi jqq 60 where volpGq ≈ô iPVdi ForŒ±PPpVq define HŒ±P‚ÑìpVqbyHŒ±piq HpŒ± iq We are ready to state our main result connecting rŒ±Œ≤to the access time 16 Theorem 3 13 Generalized Commute Time Formula LetŒ± Œ≤PPpVq Then rŒ±Œ≤ 1 volpGqpŒ± Œ≤qTpHŒ± HŒ≤q 61 Or in alternative expanded forms rŒ±Œ≤ 1 volpGq√ø iPVpŒ±i Œ≤iqpHpŒ± iq HpŒ≤ iqq 62 1 volpGq√ø i kPVpŒ±i Œ≤iqpŒ±k Œ≤kqHpi kq 63 We provide the proof in Appendix A 2 The reason we term Theorem 3 13 a generalized commute time formula is that in the case where Œ± Œ≤are Dirac measures this recovers the result that rij9pHpi jq Hpj iqq since we have rij 1 volpGqpŒ¥i Œ¥jqTpHŒ¥i HŒ¥jq 64 1 volpGqpHpi iq Hpi jq Hpj jq Hpj iq 65 1 volpGqpHpi jq Hpj iqq 66 Theorem 3 13 shows however that in the more general case of measures rŒ±Œ≤is not exactly the same as a measure commute time HpŒ± Œ≤q HpŒ≤ Œ±q Theorem 3 13 can be used to establish a relationship between the measure commute time and the measure resistance as we show below Corollary 3 14 Measure Commute Time Inequalities LetŒ± Œ≤PPpVq Then rŒ±Œ≤satisfies the following two inequalities rŒ±Œ≤ƒè2 volpGqmaxtHpŒ± Œ≤q HpŒ≤ Œ±qu 67 rŒ±Œ≤ƒè1 volpGqpHnpŒ± Œ≤q HnpŒ≤ Œ±qq 68 where HnpŒ± Œ≤q ErŒìns resp HnpŒ≤ Œ±q is the expected duration of the na ƒ±ve stopping rule described in Remark 3 10 with initial distribution Œ± resp Œ≤ and stopping node sampled from Œ≤ resp Œ± The proof of Corollary 3 14 is located in Appendix A 2 Note that the second half of the proof of Corollary 3 14 actually contains a related observation regarding B2andWr 1 which we state as another corollary below without proof Corollary 3 15 LetŒ± Œ≤PPpVq and let rbe the effective resistance metric on V Then we have B2pŒ± Œ≤qƒèWr 1pŒ± Œ≤q1 2 A final observation that one can glean from the random walk and access time setting is an interpretation of the transportation potential f L pŒ± Œ≤qmentioned in Corollary 3 5 Proposition 3 16 LetŒ± Œ≤PPpVq and let Œìbe any stopping rule such that ErŒìsƒÉ8 andXŒì Œ≤ Let fŒì be the degree weighted exit frequencies fŒìpiq E Œì 1√ø t 01 diŒ¥tXt iupiq X0 Œ±ff Then LfŒì Œ± Œ≤ In particular the transportation potential L pŒ± Œ≤qis given by the mean normalized function fŒì 1 nfT Œì1nfor any suitable stopping rule Œì 17 This proposition follows from applying the so called conservation equation for exit frequencies mentioned in e g 52 51 Another way of putting it is that the unique optimal flow for B2pŒ± Œ≤qcan be realized as the gradient of the degree normalized exit frequencies of anyfinite mean stopping rule which achieves a stopping distribution of Œ≤ having been intialized at Œ± since moreover as discussed in 51 any two degree normalized exit frequencies for different stopping times differ by a constant and thus have the same gradient 3 2 Sobolev norms and a graph Benamou Brenier formula The relationship between Sobolev spaces and optimal transportation have been explored at length in the continuous setting 84 63 37 77 The goal of this subsection is to present the 2 Beckmann problem as an optimization problem on a negative Sobolev space for functions defined on graphs In this section we follow the exposition laid out in 63 in the continuous setting by analogy to the discrete setting As it turns out there are at least two results that achieve strong homophily between the graph and continuous setting 3 2 1 Background from the continuous setting Recall that if f Rn√ëRis a function with a square integrable derivative fin the weak sense and ¬µis a Borel probability measure which is absolutely continuous with respect to the Lebesgue measure d x i e so that d ¬µ gdxfor a density function g we can define the Sobolev type seminorm 2 9H1p¬µqby f 2 9H1p¬µq ≈º Rn f 2 2d¬µ 69 The dot 9H1p¬µqserves to distinguish 2 9H1p¬µqfrom a true Sobolev norm which include a contribution from L2 We can then define the possibly infinite dual norm to f 2 9H1p¬µq denoted 9H 1p¬µqby the following for any d x absolutely continuous signed measure ŒΩ hdx hdx 9H 1p¬µq sup ≈º Rnf hd¬µ f 9H1p¬µqƒè1 70 This setup leads to two results among others in the continuous setting which are of interest in the graph setting as well Theorem 3 17 9H 1 Linearization of W2 63 If¬µis a Borel probability measure on Rnandd¬µis an infintesimally small perturbation of ¬µ then W2p¬µ ¬µ d¬µq d¬µ 9H 1p¬µq opd¬µq The second result which interests us is the Sobolev form of the Benamou Brenier formula which we state below Theorem 3 18 Benamou Brenier formula 9H 1form 63 Let¬µ ŒΩbe Borel probability measures on Rn Then it holds W2p¬µ ŒΩq inf ≈º1 0 d¬µt 9H 1p¬µtq ¬µ0 ¬µ ¬µ 1 ŒΩ 71 3 2 2 Graph sobolev norms Next we transition to defining the appropriate discrete analogues of 9H1p¬µqand 9H 1p¬µqon the graph G 18 Definition 3 19 Letf gP‚ÑìpVq We define the graph Sobolev seminorm 9H1pVqby the equation f 2 9H1pVq √ø pi jqPE1wij fpi jq 2 2 √ø pi jqPE1wij fpiq fpjq 2 2 72 We define the possibly infinite dual graph Sobolev norm 9H 1pVqby the supremum g 2 9H 1pVq sup fTg f 9H1pVqƒè1 73 It is useful to note that for mean zero functions 9H1pVqand 9H 1pVqwill be true norms in particular the former will be definite and the latter will be finite The first proposition in the section elucidates the relationship between graph Sobolev norms and the graph Laplacian matrix Proposition 3 20 Letf gP‚ÑìpVq Then the following hold i f 2 9H1pVq fTLf ii If 1Tg 0 then g 2 9H 1pVq gTL g Proof The first claim is straightforward The second requires a closer look We can work backwards and use the proof of Theorem 2 1 to obtain pgTL gq1 2 L 1 2g 2 74 inft f 2 L1 2f gu 75 sup fTg L1 2f 2ƒè1 76 where Lt 1 2 1 2u UŒõt 1 2 1 2uUTas in Eq 10 But L1 2f 2 2 fTLf f 2 9H1pVq so the claim follows 3 2 3 2 Beckmann as a negative Sobolev distance Since our definitions have been set up and explained properly we can derive the following result which follows directly from Proposition 3 20 and Theorem 3 4 We consider this a graph analogue of Theorem 3 17 for 2 Beckmann distance Theorem 3 21 LetŒ± Œ≤PPpVq Then B2pŒ± Œ≤q Œ± Œ≤ 9H 1pVq Thus in particular if Œ≤ Œ± dŒ±for a small perturbation dŒ± then B2pŒ± Œ± dŒ±q dŒ± 9H 1pVq Note that since the 9H 1norm depends on the operator L 1 2 the Beckmann distance B2pŒ± Œ± dŒ±q can be articulated in terms of the spectral coefficients of d Œ± to wit if d Œ± ≈ôn ‚Ñì 2c‚Ñìu‚Ñìis the spectral decomposition of a mean zero perturbation d Œ±P‚ÑìpVqin terms of the eigenvectors u‚ÑìofL then B2pŒ± Œ± dŒ±q2 L 1 2dŒ± 2 2 n√ø ‚Ñì 2c2 ‚Ñì Œª‚Ñì 77 Therefore in general one has B2pŒ± Œ± dŒ±qƒèŒª 1 2 2 dŒ± 2 but if the spectral properties of d Œ±are known this estimate may be sharpened The next result which leverages the Sobolev norm perspective on graph optimal transport can be con sidered a graph analogue of the Benamou Brenier formula Let ¬µtP‚ÑìpVqfor each tP r0 1s We say ¬µtPC1pr0 1sqif the map t√û√ë¬µt r0 1s√ë‚ÑìpVqis continuously differentiable as a map from r0 1stoRn Moreover when ¬µtadmits a derivative we write d ¬µt d ds¬µsÀáÀá s t 19 Theorem 3 22 Graph Benamou Brenier Formula LetŒ± Œ≤PPpVq Then we have B2pŒ± Œ≤q2 inf ≈º1 0 d¬µt 2 9H 1pVqdt ¬µtPC1pr0 1sq ¬µ0 Œ± ¬µ 1 Œ≤ 78 See Appendix A 2 for a proof which is based on the proof appearing in the lecture notes 35 Note that the infimum is achieved by a linear line segment connecting Œ±toŒ≤ which underlines the idea that B2 should be treated as a linearized variant of Wasserstein distance We explore the linearization angle and its implications on graph learning tasks in the next section 3 3 A linearized optimal transportation distance A third and final perspective on 2 Beckmann distance is from the world of clustering and classification of graph data We begin with a background discussion from the continuous setting and then explore convex separation properties for graph data as well as an application involving a handwritten digit dataset 3 3 1 Background from the continuous setting A typical classification scenario usually consists of some data txiuƒÇRnwhich one wishes to separate into classes or clusters either without prior knowledge of the data labels unsupervised or with some information about the class labels of the dataset semi supervised or supervised In many applications e g 20 93 13 the data xican often occur not as vectors in Rnbut as distributions ¬µionRn In such scenarios Wasserstein distance formulated in the Monge sense as W2p¬µ ŒΩq2 inf T T7¬µ ŒΩ≈º Rn Tpxq x 2 2d¬µpxq 79 where T7¬µis the push forward of ¬µunder the map Tand the inf is over all such Texhibiting desired regularity arises as a natural distance metric between probabilitiy measures on Rn However this metric can be slow to compute at scale and in the discrete setting can even suffer from being undefined One approach to resolving this issue involves the usage and study of Linearized Optimal Transport embeddings 58 88 54 43 21 which can be summarized as follows Given data measures t¬µiu fix a reference measure ¬µ and define Ti argmin T T7¬µ ¬µi≈ü Rn Tpxq x 2 2d¬µpxq Then the measures are featurized in the form of the maps Ti Rn√ëRnand 2 Wasserstein distance between the data measures is then approximated by Ti Tj L2 Theoretically the ability of Linearized Optimal Transport in approximating 2 Wasserstein distance can be captured by the following two results the informal statements of which we restate from 58 Theorem 3 23 Informal Statement of Theorem 4 4 from 58 IfP t¬µiuareœµ perturbations of shifts and scalings of ¬µ andQ tŒΩiuareœµ perturbations of shifts and scalings of ŒΩ andPandQhave small minimal distance depending on œµand satisfy a few technical assumptions PandQare linearly separable in LOT embedding space Theorem 3 24 Informal Statement of Theorem 4 1 from 58 If¬µ ŒΩ areœµ perturbations of shifts and scalings of one another then W2p¬µ ŒΩqƒèWLOT 2p¬µ ŒΩqƒèW2p¬µ ŒΩq CœÉœµ C1 œÉœµ1 2 80 In particular when œµ 0and¬µ ŒΩare shifts and scalings of each other LOT is an isometry It is also worth noting that in the situation where ¬µiis some shift or scaling of ¬µ i e ¬µi T7¬µfor an affine combination of scaling and translation TonRn then the LOT embedding of ¬µiwill be the map T itself i e Tis optimal in the Wasserstein cost 20 3 3 2 Linear separtation for 2 Beckmann distance To set up the problem suppose we have data sampled from Cclasses and each class is associated to a canonical distribution Œ±iPPpVqso that our data may be represented in the form D tT Œ±iui 1 C TPT That is each data point is of the form T Œ±iwhere Œ±iis the underlying distribution of the class and the map T PpVq√ëPpVqrepresents a perturbation of the class distribution among valid perturbations T to be determined For example consider a hypothetical dataset of images with resolution kÀÜ‚Ñì In this setting Gis the kÀÜ‚Ñìlattice graph and after normalization each image can be understood as a distribution on G In our data model we assume that each image may be expressed as a perturbation of a canonical distribution associated to the corresponding image class In the unsupervised setting we simply have Dand we wish to use some clustering technique e g k means or principal geodesic analysis associated to a metric on PpVq In the supervised setting we have data equipped with labels D1 tpT Œ±i yiqui 1 C TPTso that each label depends only on the class distribution and we seek to build some classifier ideally linear which facilitates predictive analysis We treat B2as a linearization of W2on graphs In this setting the featurization of each canonical class distribution is given by Œ±i√û√ëL 1 2Œ±iand similarly for the individual samples T Œ±i For a set of measures A tŒ±1 Œ± kuƒÇPpVq define the mutual support by MSpAq kƒç i 1supppŒ±iqƒÇV where supppŒ±iqis the subset of nodes in Von which Œ±ƒÖ0 Theorem 3 25 LetŒ±1 Œ±2PPpVqbe the canonical class distributions for a binary distributional dataset D tT Œ±iui 1 2 TPT Assume MSptŒ±1 Œ±2uq and let Tbe the set of affine perturbation maps defined by T Œ± Œ± dT where dTP‚ÑìpVqis vector which satisfies the three conditions i dTis mean zero or 1T ndT 0 ii supppdTqƒÇMSptŒ±1 Œ±2uq and iii dT 1ƒÉŒ¥ where Œ¥ min 1 3 Œ±1 Œ±2 2 min i 1 2 kPMSptŒ±1 Œ±2uqŒ±ipkq ƒÖ0 81 Then the sets A1 tT Œ±1uTPTandA2 tT Œ±2uTPT which belong to the metric space pPpVq B2q can be isometrically embedded into ‚Ñì2pVqunder L 1 2so that their images are linearly separable as convex sets in Rn See Appendix A 2 for the proof Note that it is not hard to extend Theorem 3 23 to the case of CƒÖ2 classes by simply requiring that the perturbations be smaller than the mutual distances between the canonical class distributions up to an appropriate constant so that in the data model there are no overlapping perturbations The main point of this result is that L 1 2as an embedding of measures is well behaved in the sense that as long as the original collections of measures were separable L 1 2will not imbue any pathology 21 x√û√ëx p1Txq Œ±√û√ëL 1 2Œ±R64 PpVq ‚Ñì2pVq 0 05 0 0 05 15 7 5 0 Figure 4 An illustration of the preprocessing pipeline for the digits data with an example from the class of handwritten zeros The first step is a mass normalization to convert the pixel values into a fixed sum distribution viewed on the nodes Vof the 8ÀÜ8 lattice graph The second step is an embedding Œ±√û√ëL 1 2Œ± such that ‚Ñì2distance in the target corresponds to 2 Beckmann distance in PpVq When computing W2 we omit the final step Figure 5 Using the digits dataset and for each pair of digit classes we computed the pairwise 2 Beckmann and 2 Wasserstein distances for each pair of samples originating from the respective digit classes with around 30 000 pairs of distances per pair of digit classes Within each tile of the grid we render a scatterplot of the distances over the overall linear regression between B2andW2for the experiment given by W2 8 446B2 22  a Similarity kernel between each image is given by exp t B2p q2u b Similarity kernel between each image is given by exp t W2p q2u RI ARI MI AMI Hom Com B20 940 0 685 1 782 0 783 0 774 0 797 W20 935 0 656 1 719 0 755 0 747 0 775 c Performance metrics Figure 6 a b Using the digits dataset we demonstrate the results of an unsupervised clustering algorithm with different choices of similarity kernel Each node corresponds to an image of a digit which we featurize as a distribution on the 8 ÀÜ8 square lattice graph We built a k 42 nearest neighbor graph on the nodes shown and then apply spectral clustering 87 to create predicted classes The text labels of the nodes correspond to the ground truth classes i e digit values The colors of the nodes on the left resp right are given by the ground truth classes resp predicted classes c We evaluate the performance of the unsupervised clustering alogrithm for each kernel We compare across several metrics including Rand index RI and adjusted Rand index ARI 39 mutual information MI and adjusted mutual information AMI 86 and homogeneity Hom and completeness Com 66 In all such cases other than MI a value of 1 0 corresponds to perfect clustering as compared to the ground truth Since the predictions depend on a random initialization in the k means step we simulated 100 runs of the algorithm and reported the best result for each kernel across the six metrics 23 3 3 3 Experimental results To further explore the implications of our results on learning with graph data we conducted two experiments which illuminate some of the potential applications and underlying properties of 2 Beckmann distance be tween measures Both examples make use of the Pen Based Recognition of Handwritten Digits dataset 3 accessed and processed via the Sklearn Python package 14 The dataset contains 1797 grayscale hand written digits of resolution 8 ÀÜ8 and pixel values ranging from 0 15 The reason for choosing this dataset is that the 2 Wasserstein kernel is computationally prohibitive at scale without approximation so rather than regularize the Wasserstein metric we opt for a lighter weight image dataset Specifically if W2is obtained via e g a Hungarian algorithm or a linear program the time complexity to obtain a kernel matrix for N samples each defined on a graph of nnodes is roughly Opn3N2q A Beckmann kernel matrix on the other hand runs Opn3 pn2 nqN2qsince only a single SVD calculation of L is required at the beginning and then subsequent entries of the kernel matrix are obtained through matrix vector multiplication and pairwise comparison The Wasserstein metric W2pŒ± Œ≤qwas directly evaluated for each pair of examples Œ± Œ≤by using the CVXPY convex optimization Python package 24 with initialization determined by the trivial coupling Œ≤Œ±T Fig 4 illustrates the pre processing pipeline for an example image of a handwritten zero The first experiment described in detail in Fig 5 is a comparative study of B2andW2in which we show that the two metrics are well correlated for the dataset in question The second experiment described in Fig 6 is a comparative study of B2andW2within the context of unsupervised learning on the digit dataset In this setup we show that the two kernels perform almost identically on the dataset with respect to several metrics with a slight edge toward B2 and thus there is some empirical evidence for drop in usage of B2 where W2 or perhaps Wpmore generally may present a computational bottleneck 4 Acknowledgements The authors would like to acknowledge Yusu Wang for her inspirational discussions early on in this project Stefan Steinerberger for his help placing the preliminary results within the context of the broader OT space Zachary Lubberts for his helpful discussions on applications of graph OT to discrete geometry and Caroline Moosm uller for her helpful feedback on linearized Wasserstein distance and Sobolev transport in the con tinuous setting SR wishes to acknowledge Evangelos Nikitopoulos as well as financial support from the Halƒ±cƒ±o glu Data Science Institute through their Graduate Prize Fellowship AC was funded by NSF DMS 2012266 and a gift from Intel ZW was supported by NSF CCF 2217058 References 1 Tara Abrishami Nestor Guillen Parker Rule Zachary Schutzman Justin Solomon Thomas Weighill and Si Wu Geometry of graph partitions via optimal transport SIAM Journal on Scientific Computing 42 5 2020 2 Morteza Alamgir and Ulrike Luxburg Phase transition in the family of p resistances Advances in neural information processing systems 24 2011 3 E Alpaydin and Fevzi Alimoglu Pen Based Recognition of Handwritten Digits UCI Machine Learning Repository 1998 4 Frank Bauer J urgen Jost and Shiping Liu Ollivier ricci curvature and the spectrum of the normalized graph laplace operator arXiv preprint arXiv 1105 3803 2011 5 Martin Beckmann A continuous model of transportation Econometrica Journal of the Econometric Society 1952 24  6 Adi Ben Israel and A Charnes Contributions to the theory of generalized inverses Journal of the Society for Industrial and Applied Mathematics 11 3 1963 7 Jean David Benamou and Yann Brenier A computational fluid mechanics solution to the monge kantorovich mass transfer problem Numerische Mathematik 84 3 375 393 2000 8 Andrew Beveridge A hitting time formula for the discrete green s function Combinatorics Probability and Computing 25 3 2016 9 Andrew Beveridge and L aszl o Lov asz Exit frequency matrices for finite markov chains Combinatorics Probability and Computing 19 4 2010 10 Robi Bhattacharjee Alexander Cloninger Yoav Freund and Andreas Oslandsbotn Effective resistance in metric spaces arXiv preprint arXiv 2306 15649 2023 11 J er emie Bigot Statistical data analysis in the wasserstein space ESAIM Proceedings and Surveys 68 2020 12 Mitchell Black Zhengchao Wan Amir Nayyeri and Yusu Wang Understanding oversquashing in gnns through the lens of effective resistance In International Conference on Machine Learning pages 2528 2547 PMLR 2023 13 Robert V Bruggner Bernd Bodenmiller David L Dill Robert J Tibshirani and Garry P Nolan Au tomated identification of stratifying signatures in cellular subpopulations Proceedings of the National Academy of Sciences 111 26 2014 14 Lars Buitinck Gilles Louppe Mathieu Blondel Fabian Pedregosa Andreas Mueller Olivier Grisel Vlad Niculae Peter Prettenhofer Alexandre Gramfort Jaques Grobler Robert Layton Jake VanderPlas Arnaud Joly Brian Holt and Ga el Varoquaux API design for machine learning software experiences from the scikit learn project In ECML PKDD Workshop Languages for Data Mining and Machine Learning 2013 15 Sarah Cannon Moon Duchin Dana Randall and Parker Rule Spanning tree methods for sampling graph partitions arXiv preprint arXiv 2210 01401 2022 16 Benson Chen Gary B ecigneul Octavian Eugen Ganea Regina Barzilay and Tommi Jaakkola Optimal transport graph neural networks arXiv preprint arXiv 2006 04804 2020 17 Samantha Chen Sunhyuk Lim Facundo M emoli Zhengchao Wan and Yusu Wang Weisfeiler lehman meets gromov wasserstein In International Conference on Machine Learning pages 3371 3416 PMLR 2022 18 Fan Chung Wenbo Zhao and Mark Kempton Ranking and sparsifying a connection graph Internet Mathematics 10 1 2 2014 19 Alex Cloninger Gal Mishne Andreas Oslandsbotn Sawyer Jack Robertson Zhengchao Wan and Yusu Wang Random walks conductance and resistance for the connection graph laplacian arXiv preprint arXiv 2308 09690 2023 20 Alexander Cloninger Brita Roy Carley Riley and Harlan M Krumholz People mover s distance Class level geometry using fast pairwise data adaptive transportation costs Applied and Computational Harmonic Analysis 47 1 2019 21 Alexander Cloninger Keaton Hamm Varun Khurana and Caroline Moosm uller Linearized wasserstein dimensionality reduction with approximation guarantees arXiv preprint arXiv 2302 07373 2023 25  22 Karel Devriendt and Renaud Lambiotte Discrete curvature on graphs from the effective resistance Journal of Physics Complexity 3 2 2022 23 Karel Devriendt Andrea Ottolini and Stefan Steinerberger Graph curvature via resistance distance Discrete Applied Mathematics 348 2024 24 Steven Diamond and Stephen Boyd CVXPY A Python embedded modeling language for convex optimization Journal of Machine Learning Research 2016 25 Peter G Doyle and J Laurie Snell Random walks and electric networks volume 22 American Mathe matical Soc 1984 26 Abderrahim Elmoataz Olivier Lezoray and S ebastien Bougleux Nonlocal discrete regularization on weighted graphs a framework for image and manifold processing IEEE transactions on Image Pro cessing 17 7 2008 27 Abderrahim Elmoataz Matthieu Toutain and Daniel Tenbrinck On the p laplacian and infinity laplacian on graphs with applications in image and data processing SIAM Journal on Imaging Sciences 8 4 2015 28 Montacer Essid and Justin Solomon Quadratically regularized optimal transport on graphs SIAM Journal on Scientific Computing 40 4 2018 29 Lawrence C Evans Partial differential equations and monge kantorovich mass transfer Current devel opments in mathematics 1997 1 1997 30 Lawrence C Evans and Wilfrid Gangbo Differential equations methods for the Monge Kantorovich mass transfer problem American Mathematical Soc 1999 31 Zhongxi Fang Jianming Huang Xun Su and Hiroyuki Kasai Wasserstein graph distance based on l1 approximated tree edit distance between weisfeiler lehman subtrees In Proceedings of the AAAI Conference on Artificial Intelligence volume 37 2023 32 Alfred Galichon Optimal transport methods in economics Princeton University Press 2018 33 Wilfrid Gangbo and Robert J McCann The geometry of optimal transportation Acta Mathematica 177 1996 34 Jun Ge and Fengming Dong Spanning trees in complete bipartite graphs and resistance distance in nearly complete bipartite graphs Discrete Applied Mathematics 283 2020 35 Augusto Gerolin Benamou brenier s approach for ott In Optimal Transport and Applications Summer School Lake Arrowhead 2013 36 Severino V Gervacio Resistance distance in complete n partite graphs Discrete Applied Mathematics 203 2016 37 Philip Greengard Jeremy G Hoskins Nicholas F Marshall and Amit Singer On a linearization of quadratic wasserstein distance arXiv preprint arXiv 2201 13386 2022 38 Steven Haker Lei Zhu Allen Tannenbaum and Sigurd Angenent Optimal mass transport for registra tion and warping International Journal of computer vision 60 2004 39 Lawrence Hubert and Phipps Arabie Comparing partitions Journal of classification 2 1985 40 Palle Jorgensen and Erin PJ Pearse Operator theory and analysis of infinite networks volume 7 World Scientific 2023 26  41 Leonid V Kantorovich On the translocation of masses In Dokl Akad Nauk USSR NS volume 37 1942 42 Nguyen Lu Dang Khoa and Sanjay Chawla Large scale spectral clustering using resistance distance and spielman teng solvers In Discovery Science 15th International Conference DS 2012 Lyon France October 29 31 2012 Proceedings 15 Springer 2012 43 Varun Khurana Harish Kannan Alexander Cloninger and Caroline Moosm uller Supervised learning of sheared distributions using linearized optimal transport Sampling Theory Signal Processing and Data Analysis 21 1 2023 44 Matt Kusner Yu Sun Nicholas Kolkin and Kilian Weinberger From word embeddings to document distances In International conference on machine learning PMLR 2015 45 Tam Le Makoto Yamada Kenji Fukumizu and Marco Cuturi Tree sliced variants of wasserstein distances Advances in neural information processing systems 32 2019 46 Tam Le Truyen Nguyen Dinh Phung and Viet Anh Nguyen Sobolev transport A scalable metric for probability measures with graph metrics In International Conference on Artificial Intelligence and Statistics pages 9844 9868 PMLR 2022 47 Tam Le Truyen Nguyen and Kenji Fukumizu Scalable unbalanced sobolev transport for measures on a graph In International Conference on Artificial Intelligence and Statistics PMLR 2023 48 Tam Le Truyen Nguyen and Kenji Fukumizu Generalized sobolev transport for probability measures on a graph arXiv preprint arXiv 2402 04516 2024 49 Haibin Ling and Kazunori Okada An efficient earth mover s distance algorithm for robust histogram comparison IEEE transactions on pattern analysis and machine intelligence 29 5 2007 50 L aszl o Lov asz Random walks on graphs Combinatorics Paul erdos is eighty 2 1 46 1993 51 L aszl o Lov asz and Peter Winkler Efficient stopping rules for markov chains In Proceedings of the twenty seventh annual ACM symposium on Theory of computing 1995 52 L aszl o Lov asz and Peter Winkler Mixing of random walks and other diffusions on a graph London Mathematical Society Lecture Note Series 1995 53 Maxime Mathey Prevot and Alain Valette Wasserstein distance and metric trees L Enseignement Math ematique 69 3 2023 54 Quentin M erigot Alex Delalande and Frederic Chazal Quantitative stability of optimal transport maps and linearization of the 2 wasserstein space In International Conference on Artificial Intelligence and Statistics PMLR 2020 55 Stacy Miller The problem of redistricting the use of centroidal voronoi diagrams to build unbiased congressional districts Senior project Whitman College 2007 56 Gaspard Monge M emoire sur la th eorie des d eblais et des remblais Mem Math Phys Acad Royale Sci 1781 57 LUIGI Montrucchio and Giovanni Pistone Kantorovich distance on a weighted graph arXiv preprint arXiv 1905 07547 1420 2019 58 Caroline Moosm uller and Alexander Cloninger Linear optimal transport embedding Provable wasser stein classification for certain rigid transformations and perturbations arXiv preprint arXiv 2008 09165 2020 27  59 Florentin M unch and Rados law K Wojciechowski Ollivier ricci curvature for general graph laplacians heat equation laplacian comparison non explosion and diameter bounds Advances in Mathematics 356 2019 60 Canh Hao Nguyen and Hiroshi Mamitsuka New resistance distances with global information on large graphs In Artificial intelligence and statistics PMLR 2016 61 Victor M Panaretos and Yoav Zemel Statistical aspects of wasserstein distances Annual review of statistics and its application 6 2019 62 Gabriel Peyr e Marco Cuturi et al Computational optimal transport Center for Research in Economics and Statistics Working Papers 1 2017 86 2017 63 R emi Peyre Comparison between w2 distance and h 1 norm and localization of wasserstein distance ESAIM Control Optimisation and Calculus of Variations 24 4 2018 64 Aaditya Ramdas Nicol as Garc ƒ±a Trillos and Marco Cuturi On wasserstein two sample testing and related families of nonparametric tests Entropy 19 2 2017 65 Sawyer Robertson Dhruv Kohli Gal Mishne and Alexander Cloninger On a generalization of wasser stein distance and the beckmann problem to connection graphs arXiv preprint arXiv 2312 10295 2023 66 Andrew Rosenberg and Julia Hirschberg V measure A conditional entropy based external cluster evaluation measure In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning EMNLP CoNLL 2007 67 Ernest K Ryu Yongxin Chen Wuchen Li and Stanley Osher Vector and matrix optimal mass transport theory algorithm and applications SIAM Journal on Scientific Computing 40 5 2018 68 Shota Saito and Mark Herbster Multi class graph clustering via approximated effective p resistance InInternational Conference on Machine Learning PMLR 2023 69 Tim Salimans Han Zhang Alec Radford and Dimitris Metaxas Improving gans using optimal transport arXiv preprint arXiv 1803 05573 2018 70 Areejit Samal RP Sreejith Jiao Gu Shiping Liu Emil Saucan and J urgen Jost Comparative analysis of two discretizations of ricci curvature for complex networks Scientific reports 8 1 2018 71 Filippo Santambrogio Optimal transport for applied mathematicians Birk auser NY 55 58 63 2015 72 Geoffrey Schiebinger Jian Shu Marcin Tabaka Brian Cleary Vidya Subramanian Aryeh Solomon Joshua Gould Siyan Liu Stacie Lin Peter Berube et al Optimal transport analysis of single cell gene expression identifies developmental trajectories in reprogramming Cell 176 4 2019 73 Maurice Sion On general minimax theorems Pacific J Math 8 4 1958 74 Justin Solomon Optimal transport on discrete domains AMS Short Course on Discrete Differential Geometry 2018 75 Justin Solomon Raif Rustamov Leonidas Guibas and Adrian Butscher Earth mover s distances on discrete surfaces ACM Transactions on Graphics ToG 33 4 2014 76 Daniel A Spielman and Nikhil Srivastava Graph sparsification by effective resistances In Proceedings of the fortieth annual ACM symposium on Theory of computing 2008 77 Stefan Steinerberger A wasserstein inequality and minimal green energy on compact manifolds Journal of Functional Analysis 281 5 2021 28  78 Tomohiro Sugiyama and Kazuhiro Sato Kron reduction and effective resistance of directed graphs SIAM Journal on Matrix Analysis and Applications 44 1 2023 79 Yuki Takezawa Ryoma Sato and Makoto Yamada Supervised tree wasserstein distance In Interna tional Conference on Machine Learning PMLR 2021 80 Yuki Takezawa Ryoma Sato Zornitsa Kozareva Sujith Ravi and Makoto Yamada Fixed support tree sliced wasserstein barycenter IEICE Technical Report IEICE Tech Rep 122 325 2022 81 MINH TANG AVANTI ATHREYA DANIEL L SUSSMAN VINCE LYZINSKI and CAREY E PRIEBE A nonparametric two sample hypothesis testing problem for random graphs Bernoulli 2017 82 Prasad Tetali Random walks and the effective resistance of networks Journal of Theoretical Probability 4 1991 83 Yu Tian Zachary Lubberts and Melanie Weber Curvature based clustering on graphs arXiv preprint arXiv 2307 10155 2023 84 C edric Villani Topics in optimal transportation volume 58 American Mathematical Soc 2021 85 C edric Villani et al Optimal transport old and new volume 338 Springer 2009 86 NX Vinh J Epps and J Bailey Information theoretic measures for clusterings comparison Variants Properties Normalization and Correction for Chance 18 2009 87 Ulrike Von Luxburg A tutorial on spectral clustering Statistics and computing 17 2007 88 Wei Wang Dejan SlepÀá cev Saurav Basu John A Ozolek and Gustavo K Rohde A linear optimal transportation framework for quantifying and visualizing variations in sets of images International journal of computer vision 101 2013 89 Nik Weaver Lipschitz algebras World Scientific 2018 90 Makoto Yamada Yuki Takezawa Ryoma Sato Han Bao Zornitsa Kozareva and Sujith Ravi Approx imating 1 wasserstein distance with trees Transactions on Machine Learning Research 2022 91 Luzh Ye and Weigen Yan Resistance between two vertices of almost complete bipartite graphs Discrete Applied Mathematics 257 2019 92 Ze Ye Tengfei Ma Chien Chun Ni Kin Sum Liu Jie Gao and Chao Chen Ricci gnn Defending against structural attacks through a geometric approach 2021 URL https openreview net forum id qoQkWNEhS 93 Yin Zhang Rong Jin and Zhi Hua Zhou Understanding bag of words model a statistical framework International journal of machine learning and cybernetics 1 2010 A Proofs This appendix contains deferred proofs from the remainder of the paper 29 A 1 Proofs from section 2 Proof of Theorem 2 1 We can derive the Lagrangian dual in a fairly straightforward manner We rewrite the constraint BJ Œ± Œ≤using the expresion sup œÜP‚ÑìpVqœÜTpŒ± Œ≤q œÜTpBJq 0 if BJ Œ± Œ≤ 8 otherwise 82 Thus the Beckmann problem may be rewritten as the following inf BJ Œ± Œ≤ J w p inf J J w p sup œÜœÜTpŒ± Œ≤q œÜTpBJq 83 sup œÜ œÜTpŒ± Œ≤q inf J J w p œÜTpBJq 84 where we exchange the sup and inf using e g Sion s minimax theorem 73 or Slater s condition Now we have that using the Legendre transform of p inf J Jpeq w p œÜTpBJq inf J J w p JTpBTœÜq 85 0 if BTœÜ w1 q qƒè1 8 otherwise 86 since the dual norm to w pis w1 q q Therefore the Lagrangian dual becomes sup œÜTpŒ± Œ≤q œÜP‚ÑìpVq BTœÜ w1 q qƒè1 87 The special case of p 1 and q 8 follows from the same proof with the only change being that the dual norm to w 1is w 1 8 which can be shown directly or through a limiting argument Proof of Proposition 2 5 Leti jPVbe fixed distinct vertices We begin by noting simply that by inspec tion rBpœÄijIPijq œÄijpŒ¥i Œ¥jq and thus that rB √ø i jPVœÄijIPij √ø i jPVœÄijpŒ¥i Œ¥jq 88 √ø iPVŒ¥ipœÄ1qi √ø jPVŒ¥jp1TœÄqj 89 Œ± Œ≤ 90 Since JœÄƒõ0 by construction JœÄis a feasible edge flow for BppŒ± Œ≤q The estimate on JœÄ w pfollows immediately from the triangle inequality Proof of Theorem 2 7 Suppose œÄPŒ†pŒ± Œ≤qis an optimal coupling for Wdp ppŒ± Œ≤q For each i j let PijPPpi jqbe a choice of path which is minimal in the sense of dp and let JœÄ JœÄ pPijqbe the feasible 30 flow as in Proposition 2 5 Then by H older s inequality BppŒ± Œ≤qƒè JœÄ w p 91 ƒè√ø i jPVœÄij IPij w p 92 ƒè√ø i jPVœÄijdppi jq 93 ƒè 1VÀÜV q √ø i jPVœÄp ijdppi jqp 1 p 94 ƒè 1VÀÜV q √ø i jPVœÄijdppi jqp 1 p n2 qWdp ppŒ± Œ≤q 95 since œÄijƒè1 and pƒÖ1 Proof of Proposition 2 13 The proof of this result is straightforward and comes in two parts First we observe that WppŒ± Œ≤q WpppŒ± pŒ≤qwhere WpppŒ± pŒ≤qp inf ≈º RÀÜR x y pdœÄpx yq œÄPŒ†ppŒ± pŒ≤q 96 In this case Œ† ppŒ± pŒ≤qis the set of all Borel probability measures on RÀÜRwith respective marginals Œ± Œ≤ Equality here holds because any such œÄPŒ†ppŒ± pŒ≤q since it has discrete marginals must itself be discrete and supported on the Cartesian product t1 nuÀÜt1 nu i e it can be written as a matrix Thus there is a one to one correspondence between the feasible couplings on VÀÜVandRÀÜR and the cost of any such pair of corresponding couplings is identical since shortest path distance on VpPnqis i j The second part of the proof is classical and it suffices to observe that the p Wasserstein distance between probability measures on Ris simply the p norm distance between their inverse cdfs For a proof we recommend e g 64 Proof of Proposition 2 14 We note that the signed incidence matrix B which is of shape nÀÜn 1 must have no kernel for the rank of Bagrees with the rank of BBT L which is n 1 for any connected graph on nnodes Therefore for any1ƒèpƒÉ8 BppŒ± Œ≤qis the p norm of the unique flow Jsuch that BJ Œ± Œ≤and which does not depend on p We therefore need only show that BpKŒ± KŒ≤q Œ± Œ≤ To wit if i 1 note that BKŒ±p1q KŒ±p1 2q Œ±p1q and if 1ƒÉiƒÉn BKŒ±piq KŒ±pi i 1q KŒ±pi 1 iq Œ±piq and lastly if i n BKŒ±pnq KŒ±pn 1 nq Œ±pnq 1 Therefore with a simple cancellation it holds BpKŒ± KŒ≤q Œ± Œ≤for all iPV The first equality in the proposition therefore follows and the second follows immediately upon inspection Proof of Proposition 2 15 Once again the proof of this result is a matter of rigid feasibility there can only really be one feasible flow JP‚ÑìpE1qsatisfying BJ Œ± Œ≤owing to e g rank considerations for Laplacians on trees Thus it is enough only to establish that BpKŒ± KŒ≤q Œ± Œ≤ Let iPVbe fixed and suppose i hasNincoming oriented edges and Moutgoing oriented edges At the root of each incoming oriented edge is a subtree I‚Ñì and at the head of each outgoing oriented edge is a subtree O‚Ñì This setup is illustrated in 31 I1 I2 INO1 O2 OMi Figure 7 A sketch of a vertex neighborhood in an oriented tree Fig 7 For each 1 ƒè‚ÑìƒèN letŒ±pI‚Ñìqbe the total mass of Œ±on that component and similarly for Œ±pO‚Ñìq We split up the argument into a few cases depending on M N Assume for a moment that Mƒõ2 and Nƒõ1 then by inspection we have pBKŒ±qpiq p Œ±pI1q Œ±pI2q Œ±pINqq 97 Œ±piq N√ø ‚Ñì 1Œ±pI‚Ñìq √ø ‚Ñì 1Œ±pO‚Ñìq Œ±piq N√ø ‚Ñì 1Œ±pI‚Ñìq √ø ‚Ñì MŒ±pO‚Ñìq 98 MŒ±piq pM 1qN√ø ‚Ñì 1Œ±pI‚Ñìq pM 1qM√ø ‚Ñì 1Œ±pO‚Ñìq 99 Œ±piq M 1 100 IfM 1 and Nƒõ1 thenpBKŒ±qpiq Œ±piq IfM 0 and Nƒõ1 thenpBKŒ±qpiq Œ±piq 1 If N 0 thenpBKŒ±qpiq Œ±piq M 1 as before since the only difference is the lack of contribution of the I‚Ñì s Therefore BKŒ± Œ± cwhere cPRnis a vector which does not depend on Œ± and thus it holds that BpKŒ± KŒ≤q Œ± Œ≤ The claim follows A 2 Proofs from section 3 Proof of Theorem 3 13 We start by considering the pointwise value of the vector L pŒ± Œ≤q To this end a useful observation is that for any coupling œÄPŒ†pŒ± Œ≤q we have Œ± Œ≤ √ø i jPVœÄijpŒ¥i Œ¥jq 32 With this in hand alongside Theorem 3 12 we can calculate L pŒ± Œ≤qi √ø jPV pL qij√ø k ‚ÑìPVœÄk‚ÑìpŒ¥kpjq Œ¥‚Ñìpjqq 101 1 volpGq√ø j k ‚ÑìPVpHpœÅ jq Hpi jqqœÄk‚ÑìpŒ¥kpjq Œ¥‚Ñìpjqq 102 1 volpGq√ø k ‚ÑìPVœÄk‚ÑìpHpœÅ kq Hpi kq HpœÅ ‚Ñìq Hpi ‚Ñìqq 103 1 volpGq√ø kŒ±kpHpœÅ kq Hpi kqq √ø ‚ÑìŒ≤‚ÑìpHpœÅ ‚Ñìq Hpi ‚Ñìqq 104 1 volpGq√ø kpŒ±k Œ≤kqpHpœÅ kq Hpi kqq 105 c 1 volpGq√ø kpŒ±k Œ≤kqHpi kq 106 where by introducing the placeholder cfor some cPR we are separating the first piece of the summand which does not depend on iand which will be eliminated in the subsequent calculation Then multiplying by pŒ± Œ≤qT and using the fact that pŒ± Œ≤qT1n 0 we get pŒ± Œ≤qTL pŒ± Œ≤q 1 volpGq√ø i kPVpŒ±i Œ≤iqpŒ±k Œ≤kqHpi kq 107 1 volpGq√ø kPVpŒ±k Œ≤kq√ø iPVpŒ±iHpi kq Œ≤iHpi kqq 108 1 volpGq√ø kPVpŒ±k Œ≤kqpHpŒ± kq HpŒ≤ kqq 109 using Proposition 3 11 i in the final line The claim follows Proof of Corollary 3 14 From the proof of Theorem 3 13 and H older s inequality we have rŒ±Œ≤ 1 volpGq√ø i kPVpŒ±i Œ≤iqpŒ±k Œ≤kqHpi kq 110 1 volpGq√ø kpŒ≤k Œ±kq√ø ipŒ±i Œ≤iqHpi kq 111 ƒè1 volpGq Œ± Œ≤ 1 √ø ipŒ±i Œ≤iqHpi kq 8 112 ƒè2 volpGqmaxtHpŒ± Œ≤q HpŒ≤ Œ±qu 113 where in the final line we make use of Proposition 3 11 ii The second inequality in the Corollary follows from the observation in the proof of Theorem 3 13 that if œÄPŒ†pŒ± Œ≤qis any coupling Œ± Œ≤ ≈ô i jPVœÄijpŒ¥i Œ¥jq Then since x√û√ë L 1 2x 2 2is convex in x pŒ± Œ≤qTL pŒ± Œ≤q L 1 2pŒ± Œ≤q 2 2 114 √ø i jœÄijL 1 2pŒ¥i Œ¥jq 2 2 115 ƒè√ø i jœÄijrij 116 33 Thus in the special case where œÄ Œ±Œ≤T rŒ±Œ≤ƒè1 volpGq√ø i jPVŒ±iŒ≤jpHpi jq Hpj iqq 117 1 volpGqpHnpŒ± Œ≤q HnpŒ≤ Œ±qq 118 Proof of Theorem 3 22 Let¬µtPC1pr0 1sqsuch that ¬µ0 Œ±and¬µ1 Œ≤ Then we estimate using Proposition 3 20 and the fact that L L LL ≈º1 0 d¬µt 2 9H 1pVqdt ≈º1 0pd¬µtqTL d¬µtdt 119 ≈º1 0pL d¬µtqTLpL d¬µtqdt 120 ≈º1 0√ø pi jqPE1wijÀáÀáL d¬µtpiq L d¬µtpjqÀáÀá2dt 121 √ø pi jqPE1wij≈º1 0ÀáÀáL d¬µtpiq L d¬µtpjqÀáÀá2dt 122 ƒõ√ø pi jqPE1wijÀáÀáÀáÀá≈º1 0pL d¬µtpiq L d¬µtpjqqdtÀáÀáÀáÀá2 123 where the final inequality follows from Jensen s inequality Then focusing on the inner term and letting e pi jqPE1we evaluate ≈º1 0pL d¬µtpiq L d¬µtpjqqdt BTÀÜ≈º1 0L d¬µtdt peq 124 BTL ÀÜ≈º1 0d¬µtdt peq 125 BTL pŒ≤ Œ±qpeq 126 127 Hence √ø pi jqPE1wijÀáÀáÀáÀá≈º1 0pL d¬µtpiq L d¬µtpjqqdtÀáÀáÀáÀá2 √ø pi jqPE1wijÀáÀáBTL pŒ≤ Œ±qpi jqÀáÀá2 128 pL pŒ≤ Œ±qqTLpL pŒ≤ Œ±qq 129 pŒ± Œ≤qTL pŒ± Œ≤q 130 and therefore inf ≈º1 0 d¬µt 2 9H 1pVqdt ¬µtPC1pr0 1sq ¬µ0 Œ± ¬µ 1 Œ≤ ƒõpŒ± Œ≤qTL pŒ± Œ≤q B2pŒ± Œ≤q2 131 For the reverse inequality it is sufficient to consider the special curve ¬µt p1 tqŒ± tŒ≤ which satisfies≈ü1 0 d¬µt 2 9H 1pVqdt pŒ± Œ≤qTL pŒ± Œ≤q B2pŒ± Œ≤q2 34 Proof of Theorem 3 25 We observe first that the collection of vectors tdTuTPTƒÇ‚ÑìpVqis convex for the first two conditions i and ii are equivalent to the intersection of a finite number of linear constraints and iii is stable under convex combinations Since L 1 2is a linear map the image L 1 2Aiwill be convex for i 1 2 Separately we have for S TPT T Œ±1 SŒ±2 2 pŒ±1 Œ±2q pdT dSq 2 132 ƒõ Œ±1 Œ±2 2 dT dS 2 133 ƒõ Œ±1 Œ±2 2 2Œ¥ƒÖ0 134 Thus min S TPT T Œ±1 SŒ±2 2ƒÖ0 135 and hence A1XA2 Note moreover that L 1 2 and more generally any power of L will act nonsingularly on mean zero vectors since its kernel is exactly the constant functions Therefore since T Œ±1 SŒ±2is mean zero for any T S and since A1andA2are disjoint the sets L 1 2A1andL 1 2A2are disjoint as well and thus they are linearly separable 35 
