{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as btfs\n",
    "import re \n",
    "from datetime import datetime,timedelta\n",
    "import PyPDF2\n",
    "import warnings\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras_preprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trunc\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Import everything from /api/ into keras.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# Import * ignores names start with \"_\".\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Add everything in /api/ to the module search path.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/api/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/api/activations/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deserialize\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialize\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/activations/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exponential\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gelu\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/activations/activations.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/backend/__init__.py:9\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# When using the torch backend,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# torch needs to be imported first, otherwise it will segfault\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# upon import.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasTensor\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m any_symbolic_tensors\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/backend/common/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasVariable\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/backend/common/dtypes.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[1;32m      7\u001b[0m BOOL_TYPES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbool\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m      8\u001b[0m INT_TYPES \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint16\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/backend/common/variables.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstateless_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_stateless_scope\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstateless_scope\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mKerasVariable\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/utils/__init__.py:12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_interactive_logging\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_interactive_logging_enabled\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_visualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_to_dot\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_visualization\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_model\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumerical_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalize\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/utils/model_visualization.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/tree/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flatten\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_nested\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/tree/tree_api.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dmtree\u001b[38;5;241m.\u001b[39mavailable:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/keras/src/tree/optree_impl.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrackable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_structures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[1;32m     19\u001b[0m     optree\u001b[38;5;241m.\u001b[39mregister_pytree_node(\n\u001b[1;32m     20\u001b[0m         ListWrapper,\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: (x, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m metadata, children: ListWrapper(\u001b[38;5;28mlist\u001b[39m(children)),\n\u001b[1;32m     23\u001b[0m         namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_nested\u001b[39m(structure):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from math import trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-22\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-21\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-20\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n",
      "2024-04-19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/ntd6ngxn5klgfg7ylpwj8n_h0000gn/T/ipykernel_98957/1224453495.py:23: Warning: not date, link ommited: 'to AJ'\n",
      "  warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n"
     ]
    }
   ],
   "source": [
    "last_page = 1\n",
    "links = []\n",
    "date = datetime.strptime('2024-04-23', '%Y-%m-%d').date()\n",
    "for i in range(0,last_page,200):\n",
    "        html = requests.get('https://arxiv.org/search/?searchtype=all&query=MACHINE+LEARNING&\\\n",
    "                            abstracts=show&size=200&order=-announced_date_first&start='+str(i)).text\n",
    "        soup = btfs(html,'html.parser')\n",
    "        \n",
    "        for link in soup.find_all('li', class_='arxiv-result'):\n",
    "        \n",
    "                \n",
    "                try:\n",
    "                        date_string = link.text.split('Submitted ')[-1].split(';')[0].strip()\n",
    "                        date_obj = datetime.strptime(date_string, '%d %B, %Y').date()\n",
    "                        print(date_obj)\n",
    "                        if (date - timedelta(days=7)) <= date_obj <= date:  \n",
    "                \n",
    "                                pdf = link.find('a', string='pdf')['href']\n",
    "                                links.append(pdf)\n",
    "                                \n",
    "                except ValueError:\n",
    "                        \n",
    "                        warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
    "\n",
    "         \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/2404.14408',\n",
       " 'https://arxiv.org/pdf/2404.14402',\n",
       " 'https://arxiv.org/pdf/2404.14397',\n",
       " 'https://arxiv.org/pdf/2404.14395',\n",
       " 'https://arxiv.org/pdf/2404.14389',\n",
       " 'https://arxiv.org/pdf/2404.14388',\n",
       " 'https://arxiv.org/pdf/2404.14378',\n",
       " 'https://arxiv.org/pdf/2404.14367',\n",
       " 'https://arxiv.org/pdf/2404.14366',\n",
       " 'https://arxiv.org/pdf/2404.14358',\n",
       " 'https://arxiv.org/pdf/2404.14354',\n",
       " 'https://arxiv.org/pdf/2404.14347',\n",
       " 'https://arxiv.org/pdf/2404.14332',\n",
       " 'https://arxiv.org/pdf/2404.14328',\n",
       " 'https://arxiv.org/pdf/2404.14326',\n",
       " 'https://arxiv.org/pdf/2404.14322',\n",
       " 'https://arxiv.org/pdf/2404.14319',\n",
       " 'https://arxiv.org/pdf/2404.14312',\n",
       " 'https://arxiv.org/pdf/2404.14286',\n",
       " 'https://arxiv.org/pdf/2404.14276',\n",
       " 'https://arxiv.org/pdf/2404.14271',\n",
       " 'https://arxiv.org/pdf/2404.14270',\n",
       " 'https://arxiv.org/pdf/2404.14265',\n",
       " 'https://arxiv.org/pdf/2404.14258',\n",
       " 'https://arxiv.org/pdf/2404.14246',\n",
       " 'https://arxiv.org/pdf/2404.14244',\n",
       " 'https://arxiv.org/pdf/2404.14243',\n",
       " 'https://arxiv.org/pdf/2404.14240',\n",
       " 'https://arxiv.org/pdf/2404.14236',\n",
       " 'https://arxiv.org/pdf/2404.14233',\n",
       " 'https://arxiv.org/pdf/2404.14222',\n",
       " 'https://arxiv.org/pdf/2404.14212',\n",
       " 'https://arxiv.org/pdf/2404.14202',\n",
       " 'https://arxiv.org/pdf/2404.14197',\n",
       " 'https://arxiv.org/pdf/2404.14188',\n",
       " 'https://arxiv.org/pdf/2404.14164',\n",
       " 'https://arxiv.org/pdf/2404.14161',\n",
       " 'https://arxiv.org/pdf/2404.14146',\n",
       " 'https://arxiv.org/pdf/2404.14122',\n",
       " 'https://arxiv.org/pdf/2404.14121',\n",
       " 'https://arxiv.org/pdf/2404.14107',\n",
       " 'https://arxiv.org/pdf/2404.14099',\n",
       " 'https://arxiv.org/pdf/2404.14080',\n",
       " 'https://arxiv.org/pdf/2404.14076',\n",
       " 'https://arxiv.org/pdf/2404.14073',\n",
       " 'https://arxiv.org/pdf/2404.14068',\n",
       " 'https://arxiv.org/pdf/2404.14064',\n",
       " 'https://arxiv.org/pdf/2404.14063',\n",
       " 'https://arxiv.org/pdf/2404.14062',\n",
       " 'https://arxiv.org/pdf/2404.14061',\n",
       " 'https://arxiv.org/pdf/2404.14052',\n",
       " 'https://arxiv.org/pdf/2404.14047',\n",
       " 'https://arxiv.org/pdf/2404.14042',\n",
       " 'https://arxiv.org/pdf/2404.14033',\n",
       " 'https://arxiv.org/pdf/2404.14027',\n",
       " 'https://arxiv.org/pdf/2404.14017',\n",
       " 'https://arxiv.org/pdf/2404.14016',\n",
       " 'https://arxiv.org/pdf/2404.14006',\n",
       " 'https://arxiv.org/pdf/2404.13993',\n",
       " 'https://arxiv.org/pdf/2404.13990',\n",
       " 'https://arxiv.org/pdf/2404.13964',\n",
       " 'https://arxiv.org/pdf/2404.13954',\n",
       " 'https://arxiv.org/pdf/2404.13946',\n",
       " 'https://arxiv.org/pdf/2404.13941',\n",
       " 'https://arxiv.org/pdf/2404.13910',\n",
       " 'https://arxiv.org/pdf/2404.13904',\n",
       " 'https://arxiv.org/pdf/2404.13895',\n",
       " 'https://arxiv.org/pdf/2404.13891',\n",
       " 'https://arxiv.org/pdf/2404.13885',\n",
       " 'https://arxiv.org/pdf/2404.13879',\n",
       " 'https://arxiv.org/pdf/2404.13860',\n",
       " 'https://arxiv.org/pdf/2404.13858',\n",
       " 'https://arxiv.org/pdf/2404.13853',\n",
       " 'https://arxiv.org/pdf/2404.13846',\n",
       " 'https://arxiv.org/pdf/2404.13844',\n",
       " 'https://arxiv.org/pdf/2404.13841',\n",
       " 'https://arxiv.org/pdf/2404.13831',\n",
       " 'https://arxiv.org/pdf/2404.13815',\n",
       " 'https://arxiv.org/pdf/2404.13808',\n",
       " 'https://arxiv.org/pdf/2404.13804',\n",
       " 'https://arxiv.org/pdf/2404.13798',\n",
       " 'https://arxiv.org/pdf/2404.13786',\n",
       " 'https://arxiv.org/pdf/2404.13785',\n",
       " 'https://arxiv.org/pdf/2404.13779',\n",
       " 'https://arxiv.org/pdf/2404.13777',\n",
       " 'https://arxiv.org/pdf/2404.13770',\n",
       " 'https://arxiv.org/pdf/2404.13752',\n",
       " 'https://arxiv.org/pdf/2404.13745',\n",
       " 'https://arxiv.org/pdf/2404.13736',\n",
       " 'https://arxiv.org/pdf/2404.13733',\n",
       " 'https://arxiv.org/pdf/2404.13731',\n",
       " 'https://arxiv.org/pdf/2404.13715',\n",
       " 'https://arxiv.org/pdf/2404.13706',\n",
       " 'https://arxiv.org/pdf/2404.13704',\n",
       " 'https://arxiv.org/pdf/2404.13702',\n",
       " 'https://arxiv.org/pdf/2404.13701',\n",
       " 'https://arxiv.org/pdf/2404.13698',\n",
       " 'https://arxiv.org/pdf/2404.13694',\n",
       " 'https://arxiv.org/pdf/2404.13690',\n",
       " 'https://arxiv.org/pdf/2404.13682',\n",
       " 'https://arxiv.org/pdf/2404.13678',\n",
       " 'https://arxiv.org/pdf/2404.13673',\n",
       " 'https://arxiv.org/pdf/2404.13671',\n",
       " 'https://arxiv.org/pdf/2404.13669',\n",
       " 'https://arxiv.org/pdf/2404.13663',\n",
       " 'https://arxiv.org/pdf/2404.13655',\n",
       " 'https://arxiv.org/pdf/2404.13652',\n",
       " 'https://arxiv.org/pdf/2404.13649',\n",
       " 'https://arxiv.org/pdf/2404.13648',\n",
       " 'https://arxiv.org/pdf/2404.13647',\n",
       " 'https://arxiv.org/pdf/2404.13646',\n",
       " 'https://arxiv.org/pdf/2404.13634',\n",
       " 'https://arxiv.org/pdf/2404.13631',\n",
       " 'https://arxiv.org/pdf/2404.13628',\n",
       " 'https://arxiv.org/pdf/2404.13621',\n",
       " 'https://arxiv.org/pdf/2404.13613',\n",
       " 'https://arxiv.org/pdf/2404.13604',\n",
       " 'https://arxiv.org/pdf/2404.13591',\n",
       " 'https://arxiv.org/pdf/2404.13588',\n",
       " 'https://arxiv.org/pdf/2404.13584',\n",
       " 'https://arxiv.org/pdf/2404.13576',\n",
       " 'https://arxiv.org/pdf/2404.13571',\n",
       " 'https://arxiv.org/pdf/2404.13557',\n",
       " 'https://arxiv.org/pdf/2404.13530',\n",
       " 'https://arxiv.org/pdf/2404.13528',\n",
       " 'https://arxiv.org/pdf/2404.13522',\n",
       " 'https://arxiv.org/pdf/2404.13521',\n",
       " 'https://arxiv.org/pdf/2404.13518',\n",
       " 'https://arxiv.org/pdf/2404.13515',\n",
       " 'https://arxiv.org/pdf/2404.13507',\n",
       " 'https://arxiv.org/pdf/2404.13506',\n",
       " 'https://arxiv.org/pdf/2404.13504',\n",
       " 'https://arxiv.org/pdf/2404.13503',\n",
       " 'https://arxiv.org/pdf/2404.13500',\n",
       " 'https://arxiv.org/pdf/2404.13491',\n",
       " 'https://arxiv.org/pdf/2404.13478',\n",
       " 'https://arxiv.org/pdf/2404.13476',\n",
       " 'https://arxiv.org/pdf/2404.13475',\n",
       " 'https://arxiv.org/pdf/2404.13474',\n",
       " 'https://arxiv.org/pdf/2404.13465',\n",
       " 'https://arxiv.org/pdf/2404.13456',\n",
       " 'https://arxiv.org/pdf/2404.13454',\n",
       " 'https://arxiv.org/pdf/2404.13449',\n",
       " 'https://arxiv.org/pdf/2404.13441',\n",
       " 'https://arxiv.org/pdf/2404.13430',\n",
       " 'https://arxiv.org/pdf/2404.13423',\n",
       " 'https://arxiv.org/pdf/2404.13421',\n",
       " 'https://arxiv.org/pdf/2404.13404',\n",
       " 'https://arxiv.org/pdf/2404.13402',\n",
       " 'https://arxiv.org/pdf/2404.13401',\n",
       " 'https://arxiv.org/pdf/2404.13393',\n",
       " 'https://arxiv.org/pdf/2404.13391',\n",
       " 'https://arxiv.org/pdf/2404.13388',\n",
       " 'https://arxiv.org/pdf/2404.13386',\n",
       " 'https://arxiv.org/pdf/2404.13381',\n",
       " 'https://arxiv.org/pdf/2404.13368',\n",
       " 'https://arxiv.org/pdf/2404.13364',\n",
       " 'https://arxiv.org/pdf/2404.13362',\n",
       " 'https://arxiv.org/pdf/2404.13356',\n",
       " 'https://arxiv.org/pdf/2404.13349',\n",
       " 'https://arxiv.org/pdf/2404.13348',\n",
       " 'https://arxiv.org/pdf/2404.13347',\n",
       " 'https://arxiv.org/pdf/2404.13344',\n",
       " 'https://arxiv.org/pdf/2404.13343',\n",
       " 'https://arxiv.org/pdf/2404.13342',\n",
       " 'https://arxiv.org/pdf/2404.13334',\n",
       " 'https://arxiv.org/pdf/2404.13328',\n",
       " 'https://arxiv.org/pdf/2404.13327',\n",
       " 'https://arxiv.org/pdf/2404.13322',\n",
       " 'https://arxiv.org/pdf/2404.13318',\n",
       " 'https://arxiv.org/pdf/2404.13316',\n",
       " 'https://arxiv.org/pdf/2404.13309',\n",
       " 'https://arxiv.org/pdf/2404.13300',\n",
       " 'https://arxiv.org/pdf/2404.13278',\n",
       " 'https://arxiv.org/pdf/2404.13273',\n",
       " 'https://arxiv.org/pdf/2404.13270',\n",
       " 'https://arxiv.org/pdf/2404.13268',\n",
       " 'https://arxiv.org/pdf/2404.13267',\n",
       " 'https://arxiv.org/pdf/2404.13265',\n",
       " 'https://arxiv.org/pdf/2404.13260',\n",
       " 'https://arxiv.org/pdf/2404.13257',\n",
       " 'https://arxiv.org/pdf/2404.13252',\n",
       " 'https://arxiv.org/pdf/2404.13244',\n",
       " 'https://arxiv.org/pdf/2404.13240',\n",
       " 'https://arxiv.org/pdf/2404.13238',\n",
       " 'https://arxiv.org/pdf/2404.13235',\n",
       " 'https://arxiv.org/pdf/2404.13227',\n",
       " 'https://arxiv.org/pdf/2404.13224',\n",
       " 'https://arxiv.org/pdf/2404.13220',\n",
       " 'https://arxiv.org/pdf/2404.13218',\n",
       " 'https://arxiv.org/pdf/2404.13215',\n",
       " 'https://arxiv.org/pdf/2404.13208',\n",
       " 'https://arxiv.org/pdf/2404.13207',\n",
       " 'https://arxiv.org/pdf/2404.13198',\n",
       " 'https://arxiv.org/pdf/2404.13194',\n",
       " 'https://arxiv.org/pdf/2404.13182',\n",
       " 'https://arxiv.org/pdf/2404.13161',\n",
       " 'https://arxiv.org/pdf/2404.13159',\n",
       " 'https://arxiv.org/pdf/2404.13150']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store the extracted text\n",
    "pdf_text = np.empty([0,2])\n",
    "# pdf_text = []\n",
    "count = 0\n",
    "\n",
    "\n",
    "# Loop through each PDF link\n",
    "\n",
    "for link in links[:5]:\n",
    "    \n",
    "\n",
    "        pdf_string = \"\"\n",
    "        \n",
    "        # Send a GET request to the PDF link\n",
    "        response = requests.get(link)\n",
    "\n",
    "        # Create a BytesIO object from the response content\n",
    "        pdf_file = BytesIO(response.content)\n",
    "\n",
    "        # Create a PyPDF2 PdfFileReader object from the BytesIO object\n",
    "        pdf = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        # Extract the text from each page of the PDF\n",
    "        for page in range(len(pdf.pages)):\n",
    "            \n",
    "            text = pdf.pages[page].extract_text()\n",
    "            text = re.sub(r'\\W+', ' ', text)\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "            \n",
    "            if page == 0:\n",
    "\n",
    "        # Extract the text between 'abstract' and 'introduction'\n",
    "                abstract_text = re.search(r'A?bstract(.*?)I?ntroduction', text, re.DOTALL | re.IGNORECASE).group(1)\n",
    "\n",
    "                \n",
    "                text = text.replace(abstract_text, \"\")\n",
    "            pdf_string += text + \" \"\n",
    "        \n",
    "        abstract_paper = np.array([abstract_text,pdf_string])\n",
    "        \n",
    "        pdf_text = np.vstack((pdf_text, abstract_paper))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Tokenization is widely used in large language models because it significantly improves performance However tokenization imposes several disadvantages such as performance biases increased adversarial vulnerability decreased character level modeling performance and increased modeling complexity To address these disadvantages without sacrificing performance we propose SpaceByte a novel byte level decoder architecture that closes the performance gap between byte level and subword autoregressive language modeling SpaceByte consists of a byte level Transformer model but with extra larger transformer blocks inserted in the middle of the layers We find that performance is significantly improved by applying these larger blocks only after certain bytes such as space characters which typically denote word boundaries Our experiments show that for a fixed training and inference compute budget SpaceByte outperforms other byte level architectures and roughly matches the performance of tokenized Transformer architectures 1 ',\n",
       "       ' We connect adversarial training for binary classiﬁcation t o a geometric evolution equa tion for the decision boundary Relying on a perspective tha t recasts adversarial training as a regularization problem we introduce a modiﬁed trainin g scheme that constitutes a minimizing movements scheme for a nonlocal perimeter funct ional We prove that the scheme is monotone and consistent as the adversarial budget vanishes and the perime ter localizes and as a consequence we rigorously show that t he scheme approximates a weighted mean curvature ﬂow This highlights that the eﬃcac y of adversarial training may be due to locally minimizing the length of the decision bo undary In our analysis we introduce a variety of tools for working with the subdiﬀeren tial of a supremal type nonlocal total variation and its regularity properties Keywords mean curvature ﬂow adversarial training adversarial mac hine learning min imizing movements monotone and consistent schemes AMS subject classiﬁcations 28A75 35D40 49J45 53E10 68T05 1 ',\n",
       "       ' Warning this paper contains and discusses content that may be offensive or upsetting Large language models LLMs and small language models SLMs are being adopted at remarkable speed although their safety still remains a serious concern With the advent of multilingual S LLMs the question now becomes a matter of scale can we expand multilingual safety evaluations of these models with the same velocity at which they are deployed To this end we introduce RTP LX a human transcreated and human annotated corpus of toxic prompts and outputs in 28 languages RTP LX follows participatory design practices and a portion of the corpus is especially designed to detect culturally specific toxic language We evaluate seven S LLMs on their ability to detect toxic content in a culturally sensitive multilingual scenario We find that although they typically score acceptably in terms of accuracy they have low agreement with human judges when judging holisti cally the toxicity of a prompt and have difficulty discerning harm in context dependent scenarios particularly with subtle yet harmful content e g microagressions bias We release of this dataset to contribute to further reduce harmful uses of these models and improve their safe deployment 1 ',\n",
       "       ' In this paper we present PARAMANU GANITA a 208 million parameter novel Auto Regressive AR decoder based language model on mathe matics The model is pretrained from scratch at context size of 4096 on our curated mixed mathematical corpus We evaluate our model on both perplexity metric and GSM8k mathe matical benchmark Paramanu Ganita despite being 35 times smaller than 7B LLMs out performed generalist LLMs such as LLaMa 1 7B by 28 4 points LLaMa 2 7B by 27 6 points Falcon 7B by 32 6 points PaLM 8B by 35 3 points and math specialised LLMs such as Minerva 8B by 23 2 points and LLEMMA 7B by 3 0 points in GSM8k test accuracy metric respectively Paramanu Ganita also outperformed giant LLMs like PaLM 62B by 6 4 points Falcon 40B by 19 8 points LLaMa 1 33B by 3 8 points and Vicuna 13B by 11 8 points respectively The large significant margin improvement in performance of our math model over the ex isting LLMs signifies that reasoning capabili ties of language model are just not restricted to LLMs with humongous number of parameters Paramanu Ganita took 146 hours of A100 train ing whereas math specialised LLM LLEMMA 7B was trained for 23 000 A100 hours of train ing equivalent Thus our approach of pre training powerful domain specialised language models from scratch for domain adaptation is much more cost effective than performing con tinual training of LLMs for domain adaptation Hence we conclude that for strong mathemati cal reasoning abilities of language model we do not need giant LLMs and immense comput ing power to our end In the end we want to point out that we have only trained Paramanu Ganita only on a part of our entire mathematical corpus and yet to explore the full potential of our model 1 ',\n",
       "       ' Federated Learning FL offers a distributed frame work to train a global control model across multiple base stations without compromising the privacy of their local network data This makes it ideal for applications like wireless traffic prediction WTP which plays a crucial role in optimizing network resources enabling proactive traffic flow management and enhancing the reliability of downstream communication aided applications such as IoT devices autonomous vehicles and industrial automation systems Despite its promise the security aspects of FL based distributed wireless systems partic ularly in regression based WTP problems remain inadequately investigated In this paper we introduce a novel fake traffic injection FTI attack designed to undermine the FL based WTP system by injecting fabricated traffic distributions with minimal knowledge We further propose a defense mechanism termed global local inconsistency detection GLID which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension Extensive experimental evaluations performed on real world wireless traffic datasets demonstrate that both our attack and defense strategies significantly outperform existing baselines Index Terms Poisoning attacks wireless traffic prediction federated learning injection attack I I '],\n",
       "      dtype='<U75756')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class webscraper:\n",
    "    \"\"\"\n",
    "    This class creates a list of full texts from research papers using the keyword 'Machine Learning' \n",
    "    in the computer science section of arxiv.com\n",
    "\n",
    "    Attributes:\n",
    "\n",
    "    date (date): The date you want to look back from. Format = \"YYYY-MM-DD\"\n",
    "    last_page (int): each page goes up by 200, specify the final page to scrape from then add 1 ie 1 = page 1, 201 = page 2, 401 = page 3...\n",
    "    num_days (int): number of days you want to collect articles for. Default 7\n",
    "    link (str): Link to arxiv page\n",
    "\n",
    "    Methods:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, last_page, num_days = 7, date=datetime.now().date(), link = 'https://arxiv.org/search/?searchtype=all&query=MACHINE+LEARNING&\\\n",
    "                                                                abstracts=show&size=200&order=-announced_date_first&start='):\n",
    "        \n",
    "        self.date = date\n",
    "        self.last_page = last_page\n",
    "        self.num_days = num_days\n",
    "        self.link = link\n",
    "        # self.__pdf_list = None\n",
    "\n",
    "    # @property\n",
    "    # def get_pdf_list(self):\n",
    "    #     return self.__pdf_list\n",
    "    \n",
    "\n",
    "    # def set_pdf_list(self, pdf_list):\n",
    "    #     # if pdf_list != self.get_pdf_list or is None:\n",
    "    #     #     self.__pdf_list.append(pdf_list)\n",
    "    #     # else:\n",
    "    #     #       print(\"list already updated\")\n",
    "    #     self.__pdf_list = pdf_list\n",
    "\n",
    "   \n",
    "    def create_pdf_list(self):\n",
    "        \n",
    "        links = np.empty(0)\n",
    "\n",
    "        \n",
    "        for i in range(0,self.last_page,200):\n",
    "            html = requests.get('https://arxiv.org/search/?searchtype=all&query=MACHINE+LEARNING&\\\n",
    "                                abstracts=show&size=200&order=-announced_date_first&start='+str(i)).text\n",
    "            soup = btfs(html,'html.parser')\n",
    "\n",
    "            for link in soup.find_all('li', class_='arxiv-result'):\n",
    "                    \n",
    "                    try:\n",
    "                            date_string = link.text.split('Submitted ')[-1].split(';')[0].strip()\n",
    "                            date_obj = datetime.strptime(date_string, '%d %B, %Y').date()\n",
    "                            if (self.date - timedelta(days=7)) <= date_obj <= self.date:     \n",
    "                                    pdf = link.find('a', string='pdf')['href']\n",
    "                                    links = np.append(links,pdf)\n",
    "                                    \n",
    "                    except (ValueError,TypeError):\n",
    "                        warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
    "                                    \n",
    "        return links\n",
    "\n",
    "    \n",
    "    def save_papers(self):\n",
    "         \n",
    "        links = self.create_pdf_list()\n",
    "\n",
    "        # empty df to store the extracted text\n",
    "        pdf_text = pd.DataFrame(columns=['abstract', 'full_text'])\n",
    "        \n",
    "        for link in links:\n",
    "            \n",
    "                pdf_string = \"\"\n",
    "                \n",
    "                try:\n",
    "                    # Send a GET request to the PDF link\n",
    "                    response = requests.get(link)\n",
    "\n",
    "                    # Create a BytesIO object from the response content\n",
    "                    pdf_file = BytesIO(response.content)\n",
    "\n",
    "                    # Create a PyPDF2 PdfFileReader object from the BytesIO object\n",
    "                    pdf = PyPDF2.PdfReader(pdf_file)\n",
    "                    \n",
    "                    # Extract the text from each page of the PDF\n",
    "                    for page in range(len(pdf.pages)):\n",
    "                        \n",
    "                        text = pdf.pages[page].extract_text()\n",
    "                        text = re.sub(r'\\W+', ' ', text)\n",
    "                        text = text.replace(\"\\n\", \" \")\n",
    "                        \n",
    "                        if page == 0:\n",
    "\n",
    "                    # Extract the text between 'abstract' and 'introduction'\n",
    "                            abstract_text = re.search(r'A?bstract(.*?)I?ntroduction', text, re.DOTALL | re.IGNORECASE).group(1)\n",
    "\n",
    "                            \n",
    "                            text = text.replace(abstract_text, \"\")\n",
    "                        pdf_string += text + \" \"\n",
    "                    \n",
    "                    # Create a new row in the DataFrame\n",
    "                    new_row = pd.DataFrame({'summary': [abstract_text], 'text': [pdf_string]})\n",
    "            \n",
    "                    # Append the new row to the DataFrame\n",
    "                    pdf_text = pd.concat([pdf_text, new_row], ignore_index=True)\n",
    "                except:\n",
    "                      pass\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        pdf_text.to_csv('full_text1.csv', index=False)\n",
    "\n",
    "         \n",
    "         \n",
    "         \n",
    "    \n",
    "                \n",
    "\n",
    "                                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/96/ntd6ngxn5klgfg7ylpwj8n_h0000gn/T/ipykernel_1929/2553492512.py:62: Warning: not date, link ommited: 'to IEEE GlobeCom 2024'\n",
      "  warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
      "/var/folders/96/ntd6ngxn5klgfg7ylpwj8n_h0000gn/T/ipykernel_1929/2553492512.py:62: Warning: not date, link ommited: 'to IEEE TDSC'\n",
      "  warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
      "/var/folders/96/ntd6ngxn5klgfg7ylpwj8n_h0000gn/T/ipykernel_1929/2553492512.py:62: Warning: not date, link ommited: 'to Data-Centric Engineering Journal and it is under review'\n",
      "  warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
      "/var/folders/96/ntd6ngxn5klgfg7ylpwj8n_h0000gn/T/ipykernel_1929/2553492512.py:62: Warning: not date, link ommited: '24 April, 2024'\n",
      "  warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
      "/var/folders/96/ntd6ngxn5klgfg7ylpwj8n_h0000gn/T/ipykernel_1929/2553492512.py:62: Warning: not date, link ommited: 'to IEEE Transactions on Communications'\n",
      "  warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
      "/var/folders/96/ntd6ngxn5klgfg7ylpwj8n_h0000gn/T/ipykernel_1929/2553492512.py:62: Warning: not date, link ommited: 'to AJ'\n",
      "  warnings.warn(f\"not date, link ommited: '{link.text.split('Submitted ')[-1].split(';')[0].strip()}'\", Warning)\n",
      "/Users/cex/opt/anaconda3/envs/datascience/lib/python3.10/site-packages/PyPDF2/_cmap.py:142: PdfReadWarning: Advanced encoding /GBK-EUC-H not implemented yet\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scraper = webscraper(last_page=401, date=datetime.now().date())\n",
    "scraper.save_papers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199,)\n"
     ]
    }
   ],
   "source": [
    "print(scraper.get_pdf_list.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
